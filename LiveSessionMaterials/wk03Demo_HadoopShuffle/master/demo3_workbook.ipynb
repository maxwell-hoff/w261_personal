{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo 3 - Hadoop Shuffle & TOS\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2019`__\n",
    "\n",
    "Designing MapReduce algorithms involves two kinds of planning. First we have to figure out which parts of a calculation can be performed in parallel and which can't. Then we have to figure out how to put those pieces together so that the right information ends up in the right place at the right time. That's what the Hadoop shuffle is all about. Today we'll talk about a few techniques to optimize your Hadoop jobs. By the end of this demo you should be able to:  \n",
    "* ... __identify__ what makes the Hadoop Shuffle potentially costly.\n",
    "* ... __define__ local aggregation & combiners.\n",
    "* ... __implement__ partial, unordered, and total order sort.\n",
    "* ... __create__ custom counters for your Hadoop Jobs.\n",
    "* ... __describe__ the order inversion pattern & when to use it (ie: relative frequencies).\n",
    "\n",
    "**Note**: Hadoop Streaming syntax is very particular. Make sure to test your python scripts before passing them to the Hadoop job and pay careful attention to the order in which Hadoop job parameters are specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop 2.6.0-cdh5.16.2\n",
      "Subversion http://github.com/cloudera/hadoop -r 4f94d60caa4cbb9af0709a2fd96dc3861af9cf20\n",
      "Compiled by jenkins on 2019-06-03T10:43Z\n",
      "Compiled with protoc 2.5.0\n",
      "From source with checksum 79b9b24a29c6358b53597c3b49575e37\n",
      "This command was run using /usr/lib/hadoop/hadoop-common-2.6.0-cdh5.16.2.jar\n"
     ]
    }
   ],
   "source": [
    "!hadoop version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Hadoop Streaming Docs\n",
    "https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/HadoopStreaming.html   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports & magic commands\n",
    "import sys\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/root/demo3': File exists\n"
     ]
    }
   ],
   "source": [
    "# globals\n",
    "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\"\n",
    "HOME_DIR = \"/media/notebooks\" # this is where docker mounts your repo, ADJUST AS NEEDED\n",
    "DEMO_DIR = HOME_DIR + \"/LiveSessionMaterials/wk03Demo_HadoopShuffle\"\n",
    "HDFS_DIR = \"/user/root/demo3\"\n",
    "!hdfs dfs -mkdir {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# <--- SOLUTION --->\n",
    "# ... for instructors ...\n",
    "HOME_DIR = \"/media/notebooks/Instructors\"\n",
    "DEMO_DIR = HOME_DIR + \"/LiveSessionMaterials/wk03Demo_HadoopShuffle/master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store notebook environment path\n",
    "from os import environ\n",
    "PATH  = environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`REMINDER:`__ If you are running this notebook from the course Docker container you can track your Hadoop Jobs using the UI at: http://localhost:19888/jobhistory/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "In this notebook, we'll continue working with the  _Alice in Wonderland_ text file from HW1 and the test file we created for debugging. Run the following cell to confirm that you have access to these files and save their location to a global variable to use in your Hadoop Streaming jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# make a data subfolder - RUN THIS CELL AS IS\n",
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  170k  100  170k    0     0   174k      0 --:--:-- --:--:-- --:--:--  174k\n"
     ]
    }
   ],
   "source": [
    "# (Re)Download Alice Full text from Project Gutenberg - RUN THIS CELL AS IS \n",
    "# NOTE: feel free to replace 'curl' with 'wget' or equivalent command of your choice.\n",
    "!curl \"http://www.gutenberg.org/files/11/11-0.txt\" -o data/alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/alice_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/alice_test.txt\n",
    "This is a small test file. This file is for a test.\n",
    "This small test file has two small lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the paths - RUN THIS CELL AS IS (if Option 1 failed)\n",
    "ALICE_TXT = PWD + \"/data/alice.txt\"\n",
    "TEST_TXT = PWD + \"/data/alice_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### alice.txt #########\n",
      "﻿The Project Gutenberg EBook of Alice’s Adventures in Wonderland, by Lewis Carroll\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and most\n",
      "other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever.  You may copy it, give it away or re-use it under the terms of\n",
      "the Project Gutenberg License included with this eBook or online at\n",
      "######### alice_test.txt #########\n",
      "This is a small test file. This file is for a test.\n",
      "This small test file has two small lines.\n"
     ]
    }
   ],
   "source": [
    "# confirm the files are there - RUN THIS CELL AS IS\n",
    "!echo \"######### alice.txt #########\"\n",
    "!head -n 6 {ALICE_TXT}\n",
    "!echo \"######### alice_test.txt #########\"\n",
    "!cat {TEST_TXT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the file into HDFS for easy access\n",
    "\n",
    "See the docs for the difference between `-put` and `-copyFromLocal`:     \n",
    "\n",
    "https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/FileSystemShell.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/root/demo3/alice_test.txt': File exists\n",
      "put: `/user/root/demo3/alice.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# load the input files into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -put {TEST_TXT} {ALICE_TXT} {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Overview: Hadoop Shuffle\n",
    "\n",
    "The week 3 reading from Chapter 3 of _Data Intensive Text Processing with Map Reduce_ by Lin and Dyer discusses 5 key techniques for controlling execution and managing the flow of data in MapReduce:\n",
    "1. using complex data structures to communicate partial results\n",
    "2. user-specified initialization/termination code before/after each map/reduce task\n",
    "3. preserving state across multiple keys\n",
    "4. controling the sort order of intermediate keys\n",
    "5. specifying the partitioning of the key space\n",
    "\n",
    "### Our goal in employing these techniques is to minimize the amount we have to move the data. This involves keeping track of what data is stored where at each stage in our job (_DITP figure 2.5 and 2.6_):\n",
    "*Recall that a Hadoop cluster stores our data on datanotes and ships the programmer's map and reduce code to those nodes to perform those transformations in place.*\n",
    "\n",
    "![HDFSdiagram](DITP_fig2-5,6.png)\n",
    "\n",
    "\n",
    "### Though it isn't always possible to do so, ideally we'd like to design an implementation that retrieves the result in a single MapReduce job (_DITP figure 2.4_):\n",
    "![MRdiagram](DITP_fig2-4.png)\n",
    "\n",
    "### Shuffle & Sort Detail (_Hadoop, The Definitve Guide, by Tom White; fig 7-4_):   \n",
    "![CircularBuffer-DFG](HDG_fig7-4-annotated.png)\n",
    "\n",
    "\n",
    "> __DISCUSSION QUESTIONS:__  \n",
    "> * What work does your Hadoop cluster have to do at the shuffle stage? \n",
    "> * What determines the time complexity of this work? \n",
    "> * Compare the Lyn & Dyer diagram with the om White diagram. How might the Lyn & Dyer diagram be misleading?\n",
    "> * What is a combiner, how does it impact the shuffle? 'where' does the combining happen?\n",
    "> * What is local aggregation? Is a combiner the only way to do it?\n",
    "> * What is a partitioner and how does it impact the shuffle? How/where can we specify custom paritioning behavior? If we don't specify a partitioner, will Hadoop still partition the data? How?\n",
    "> * Can you think of an example of a task that can't be accomplished in a single MapReduce job? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *_A note on the Lin and Dyer reading (and other readings you may encounter):_\n",
    "\n",
    ">In MapReduce, the programmer defines a mapper and a reducer with the following signatures:   \n",
    "map: (k1; v1) -> [(k2; v2)]   \n",
    "reduce: (k2; [v2]) -> [(k3; v3)]   \n",
    "where [..] denotes a list\n",
    "\n",
    "_It is important to note that the MapReduce framework ensures that the keys are ordered, so we know that if a key is different from the previous one, we have moved into a new key group. In contrast to the Java API, where you are provided an iterator over each key group (as per above Lin and Dyer version), in Streaming you have to find key group boundaries in your program. (Page 39, Hadoop - The Definitive Guide)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS__  \n",
    "* What work does your Hadoop cluster have to do at the shuffle stage? What determines the time complexity of this work?\n",
    "> Short digression: there is something slightly deceptive about this kind of MapReduce diagram -- the arrows make it look like data is flowing from step to step down the chart. Of course that is not the case physically. The shuffle stage is where the data gets moved. It doesn't linearly follow the 'paritioning' but includes all the work done by the framework after the mapper emits the data and before the reducers start. The work involved is costly in two ways: first in order to perform the sort Hadoop has to look at which keys are present on each data node & make a bunch of comparisions to sort these keys and plan where records with those keys are going to end up, then the data has to be transfered. The complexity is a function of the number of records output by our mappers (NOTE: this is different than the number of records we started with.)\n",
    "\n",
    "* Compare the Lyn & Dyer diagram with the om White diagram. How might the Lyn & Dyer diagram be misleading?\n",
    "> In the Lyn & Dyer diagram combiners appear to only be executed at the map phase.\n",
    "\n",
    "* What is a combiner, how does it impact the shuffle? 'where' does the combining happen?\n",
    "> A combiner is an aggregation script specfied by the programmer that will take mapper output records with the same key and turn them into a single, combined, record. Usually this is desireable because fewer records = a faster shuffle. However its important to note that Hadoop uses the combiners strategically -- sometimes records are combined before leaving the mapper node, sometimes after arriving at a reducer node (this is related to the 'circular buffer' that you may remember discussed in the async). There are two key takeaways here 1) that Hadoop doesn't guarantee that it will perform the combining on any or all records and 2) Hadoop makes smart choice about when to hold data in memory/combine/spill to disk... saving a lot of work for the programmer.\n",
    "\n",
    "* What is local aggregation? Is a combiner the only way to do it?\n",
    "> Local aggregation is basically what we described a minute ago: reducing the number of records that need to be transfered over the network in the shuffle state _OR_ between two sequential MapReduce jobs. Combiners are _NOT_ the only way to perform local aggregation, we could also use an in memory form of aggregation eg. a python dicitonary. Of course there are risks to this approach if your mappers are emitting more records than the ones they read in. (_NOTE:_ local aggregation is one example of the 2nd technique that Lin & Dyer list: \"preserving state across multiple keys\").\n",
    "\n",
    "\n",
    "* What is a partitioner and how does it impact the shuffle? How/where can we specify custom partitioning behavior? If we don't specify a partitioner, will Hadoop still partition the data? How?\n",
    "> The 'partitioner' parameter in our Hadoop jobs (we saw this last week in live session) simply tells Hadoop how to determine which keys end up together on the same reducer node. Custom paritioning is just a matter of manipulating the key format/data structure -- this would happen in a  mapper script there is no 'paritioner' script. Anytime the programmer specifies more than one reducer then Hadoop will perform partitioning... if we don't tell it how to partittion explicitly then Hadoop will make its own decision about how to split up the key space.\n",
    "\n",
    "* Can you think of an example of a task that can't be accomplished in a single MapReduce job? Explain.\n",
    "> Students should be able to come up with sorted word count (challenge question from the end of live session in week2) --> can't sort until after reducing so you need a second job. We'll look at an example below of calculating relative frequencies... it would be good to guide them to consider the key challenge in that task: you can't divide until you have a total but what if you don't want to hold all of the counts in memory? (we'll teach order inversion below, no need to mention that here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Relative Frequencies\n",
    "In last week's live session and HW1 we used Word Count as a cannonical example of an embarassingly parallel task. At first glance, computing relative frequencies (`word count / total count`) seems like it would be just as easy to implement -- after all it's just word count with an extra division at the end. However this task actually presents a small design challenge that is perfect to illustrate a few of the techniques that Lin & Dyer talk about.\n",
    "\n",
    "__DISCUSSION:__\n",
    "> * Talk through what the MapReduce job would look like? What is the challenge here?\n",
    "> * Is it possible to compute relative frequencies in a single MapReduce job? \n",
    "> * Is it possible to compute relative frequencies in a single MapReduce job with multiple reducers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS (before exercise 1)__  \n",
    "* Talk through what the MapReduce job would look like? What is the challenge here?  \n",
    "> The map would work the same as in word count, then the reducer would aggregate counts and also add up a total word count so that it can divide each word's count by the total. The challenge is that we won't know the total until after reducing. In word count we count in parallel and then add at the end. To compute frequencies we can still count in parallel but then we need to add both individual words and the total number of words before we can divide. \n",
    "\n",
    "* Is it possible to compute relative frequencies in a single MapReduce job?  \n",
    "> Yes, we could hold all the word counts in memory and then divide at the end... but that's not scalable. \n",
    "\n",
    "* Is it possible to compute relative frequencies in a single MapReduce job with multiple reducers?\n",
    "> No, if we use multiple reducers we'd have no way to get the total-- we'd just have partial totals on each reducer node. We'd need a second job to get the full total & perform the division.\n",
    "\n",
    ".... OK _actually_ there IS a scalable a way to do this. Its called the order inversion pattern. Lets take a look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Tasks:\n",
    "\n",
    "* __a) read provided code:__ We've provided a naive interpretation to get you started. Take a look at __`Frequencies/mapper.py`__, __`Frequencies/combiner.py`__  and __`Frequencies/reducer.py`__.  \n",
    "\n",
    "* __b) discuss:__ How does it resolve the challenge of computing the total? Uncomment line 22 in the mapper and lines 26-27 in the reducer to take advantage of this 'solution', then run the unit tests below, to confirm that you understand what's going on in each of these scripts. Despite solving the problem of computing the total in a single map-reduce job, what is wrong with this approach? \n",
    "\n",
    "* __c) fix the problem:__ To fix the problem, all we need to do is make a small change to the key used to emit the total counts (you need to do this in both the mapper and reducer). Think about Hadoop's default sorting. What keys arrive first? What key could you assign to the total that would be guaranteed to arrive first? Once you are satisfied that your solution works, run the provided Hadoop job on the test file & then the full `alice` text. (note it has a single reducer for now).  \n",
    "\n",
    "* __d) discuss:__  Now, re-run the job with 4 reducers, what happens to the results? What would we have to do to fix the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - make sure scripts are executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x Frequencies/mapper.py\n",
    "!chmod a+x Frequencies/combiner.py\n",
    "!chmod a+x Frequencies/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\t1\n",
      "!total\t1\n",
      "foo\t1\n",
      "!total\t1\n",
      "quux\t1\n",
      "!total\t1\n",
      "labs\t1\n",
      "!total\t1\n",
      "foo\t1\n",
      "!total\t1\n",
      "bar\t1\n",
      "!total\t1\n",
      "quux\t1\n",
      "!total\t1\n"
     ]
    }
   ],
   "source": [
    "# part b - unit test mapper script\n",
    "!echo \"foo foo quux labs foo bar quux\" | Frequencies/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!total\t7\n",
      "bar\t1\n",
      "foo\t3\n",
      "labs\t1\n",
      "quux\t2\n"
     ]
    }
   ],
   "source": [
    "# part b - unit test map-combine (sort mimics shuffle) (RUN THIS CELL AS IS)\n",
    "!echo \"foo foo quux labs foo bar quux\" | Frequencies/mapper.py | sort -k1,1 | Frequencies/combiner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!total\t1.0\n",
      "bar\t0.14285714285714285\n",
      "foo\t0.42857142857142855\n",
      "labs\t0.14285714285714285\n",
      "quux\t0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "# part b - unit test map-combine-reduce (sort mimics shuffle) (RUN THIS CELL AS IS)\n",
    "!echo \"foo foo quux labs foo bar quux\" | Frequencies/mapper.py | sort -k1,1 | Frequencies/combiner.py | Frequencies/reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/demo3/frequencies-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# parts c - clear the output directory (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/frequencies-output\n",
    "# NOTE: this directory won't exist unless you are re-running a job, that's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob711175312799087886.jar tmpDir=null\n",
      "21/01/18 11:00:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "21/01/18 11:00:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "21/01/18 11:00:50 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "21/01/18 11:00:51 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "21/01/18 11:00:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1610967571082_0001\n",
      "21/01/18 11:00:51 INFO impl.YarnClientImpl: Submitted application application_1610967571082_0001\n",
      "21/01/18 11:00:51 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1610967571082_0001/\n",
      "21/01/18 11:00:51 INFO mapreduce.Job: Running job: job_1610967571082_0001\n",
      "21/01/18 11:00:58 INFO mapreduce.Job: Job job_1610967571082_0001 running in uber mode : false\n",
      "21/01/18 11:00:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "21/01/18 11:01:03 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "21/01/18 11:01:04 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "21/01/18 11:01:08 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "21/01/18 11:01:08 INFO mapreduce.Job: Job job_1610967571082_0001 completed successfully\n",
      "21/01/18 11:01:09 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=150\n",
      "\t\tFILE: Number of bytes written=450944\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=369\n",
      "\t\tHDFS: Number of bytes written=103\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5768\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2873\n",
      "\t\tTotal time spent by all map tasks (ms)=5768\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2873\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5768\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2873\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5906432\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2941952\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2\n",
      "\t\tMap output records=40\n",
      "\t\tMap output bytes=311\n",
      "\t\tMap output materialized bytes=156\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=40\n",
      "\t\tCombine output records=16\n",
      "\t\tReduce input groups=11\n",
      "\t\tReduce shuffle bytes=156\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=11\n",
      "\t\tSpilled Records=32\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=76\n",
      "\t\tCPU time spent (ms)=1820\n",
      "\t\tPhysical memory (bytes) snapshot=801386496\n",
      "\t\tVirtual memory (bytes) snapshot=4143108096\n",
      "\t\tTotal committed heap usage (bytes)=883949568\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=141\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=103\n",
      "21/01/18 11:01:09 INFO streaming.StreamJob: Output directory: /user/root/demo3/frequencies-output\n"
     ]
    }
   ],
   "source": [
    "# parts c - Hadoop streaming job (RUN THIS CELL AS IS FIRST, then make your modification)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files Frequencies/reducer.py,Frequencies/mapper.py,Frequencies/combiner.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -combiner combiner.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/alice_test.txt \\\n",
    "  -output {HDFS_DIR}/frequencies-output \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!total\t1.0\n",
      "a\t0.1\n",
      "file\t0.15\n",
      "for\t0.05\n",
      "has\t0.05\n",
      "is\t0.1\n",
      "lines\t0.05\n",
      "small\t0.15\n",
      "test\t0.15\n",
      "this\t0.15\n"
     ]
    }
   ],
   "source": [
    "# take a look at the first few results (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/frequencies-output/part-00000 | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/demo3/frequencies-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob8811832056850768385.jar tmpDir=null\n",
      "21/01/18 11:01:15 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "21/01/18 11:01:15 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "21/01/18 11:01:16 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "21/01/18 11:01:16 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "21/01/18 11:01:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1610967571082_0002\n",
      "21/01/18 11:01:16 INFO impl.YarnClientImpl: Submitted application application_1610967571082_0002\n",
      "21/01/18 11:01:16 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1610967571082_0002/\n",
      "21/01/18 11:01:16 INFO mapreduce.Job: Running job: job_1610967571082_0002\n",
      "21/01/18 11:01:21 INFO mapreduce.Job: Job job_1610967571082_0002 running in uber mode : false\n",
      "21/01/18 11:01:21 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "21/01/18 11:01:27 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "21/01/18 11:01:28 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "21/01/18 11:01:33 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "21/01/18 11:01:33 INFO mapreduce.Job: Job job_1610967571082_0002 completed successfully\n",
      "21/01/18 11:01:34 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=613758\n",
      "\t\tFILE: Number of bytes written=1677074\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=179007\n",
      "\t\tHDFS: Number of bytes written=88988\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7150\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3303\n",
      "\t\tTotal time spent by all map tasks (ms)=7150\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3303\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7150\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3303\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=7321600\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3382272\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3774\n",
      "\t\tMap output records=61236\n",
      "\t\tMap output bytes=491280\n",
      "\t\tMap output materialized bytes=613764\n",
      "\t\tInput split bytes=218\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3022\n",
      "\t\tReduce shuffle bytes=613764\n",
      "\t\tReduce input records=61236\n",
      "\t\tReduce output records=3022\n",
      "\t\tSpilled Records=122472\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=70\n",
      "\t\tCPU time spent (ms)=3910\n",
      "\t\tPhysical memory (bytes) snapshot=800686080\n",
      "\t\tVirtual memory (bytes) snapshot=4134084608\n",
      "\t\tTotal committed heap usage (bytes)=886046720\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=178789\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=88988\n",
      "21/01/18 11:01:34 INFO streaming.StreamJob: Output directory: /user/root/demo3/frequencies-output\n",
      "!total\t1.0\n",
      "a\t0.022699065908942453\n",
      "abide\t6.532105297537396e-05\n",
      "able\t3.266052648768698e-05\n",
      "about\t0.003331373701744072\n",
      "above\t9.798157946306095e-05\n",
      "absence\t3.266052648768698e-05\n",
      "absurd\t6.532105297537396e-05\n",
      "accept\t3.266052648768698e-05\n",
      "acceptance\t3.266052648768698e-05\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# <--- SOLUTION --->\n",
    "# parts c - Hadoop streaming job (RUN THIS CELL AS IS FIRST, then make your modification)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/frequencies-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files Frequencies/reducer.py,Frequencies/mapper.py,Frequencies/combiner.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/alice.txt \\\n",
    "  -output {HDFS_DIR}/frequencies-output \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 1\n",
    "!hdfs dfs -cat {HDFS_DIR}/frequencies-output/part-00000 | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected Results:__\n",
    "\n",
    "<table>\n",
    "<th>part c (test)</th>\n",
    "<th>part c (full)</th>\n",
    "<tr>\n",
    "<td><pre>\n",
    "!total\t1.0\n",
    "a\t0.1\n",
    "file\t0.15\n",
    "for\t0.05\n",
    "has\t0.05\n",
    "is\t0.1\n",
    "lines\t0.05\n",
    "small\t0.15\n",
    "test\t0.15\n",
    "this\t0.15\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "!total\t1.0\n",
    "a\t0.022728759238668322\n",
    "abide\t6.540650140623977e-05\n",
    "able\t3.270325070311989e-05\n",
    "about\t0.003335731571718229\n",
    "above\t9.810975210935967e-05\n",
    "absence\t3.270325070311989e-05\n",
    "absurd\t6.540650140623977e-05\n",
    "accept\t3.270325070311989e-05\n",
    "acceptance\t3.270325070311989e-05\n",
    "</pre></td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "__DISCUSSION:__  \n",
    "> *  Was the original implementation scalable? How does it resolve the challenge of computing the total? \n",
    "> * Despite solving that problem, what is wrong with this approach?  \n",
    "> * Did you come up with a new key that is guaranteed to arrive first? how?\n",
    "> * What happened when you went from 1 reducer to 4, why? (HINT: Use the UI to see the error logs for the failed reduce tasks)\n",
    "> * What do we need to do with the total counts when we move up to 4 reducers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS (after exercise 1)__  \n",
    "*  Was the original implementation scalable? How does it resolve the challenge of computing the total? \n",
    "  > Yes, instead of holding records in memory it emits a second record allowing us to count the totals without\n",
    "  relying on the word counts.\n",
    "  \n",
    "* Despite solving that problem, what is wrong with this approach?  \n",
    "  > The Hadoop Shuffle ensures that the key \"Total\" arrives in alphabetical order... after most of our word counts have already been added up. We need the total to arrive first.\n",
    "\n",
    "* Did you come up with a new key that is guaranteed to arrive first? how?\n",
    "  > *Total, !Total etc\n",
    "  \n",
    "* What happened when you went from 1 reducer to 4, why?\n",
    "  > The Hadoop job fails due to a zero division error (ask them if they used the UI to see the error logs for the failed reduce tasks). This happens because the total key only gets sent to one of the 4 reducers. \n",
    "  \n",
    "* What do we need to do with the total counts when we move up to 4 reducers?\n",
    "  > We need a way to make the total available to each reducer node (partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Custom Partitioning\n",
    "\n",
    "Last week in Breakout 4 you learned out to implement a secondary sort -- that is, you learned how to tell Hadoop to order key-value pairs within each partition based on the value. However you also saw the limitation of this simple secondary sort: namely that sorting within a partition is not very useful if you need to use multiple reducers because, for example, the top word could end up in any one of the partitions and the next highest might end up in a totally different partition. Of course post processing your partially sorted partition files (eg. using mergesort) might solve this problem, but if your data is too large to fit on a single machine that is not a viable solution.\n",
    "\n",
    "Luckily, Hadoop provides a way to partition the data across reducers in a user-defined way.  This is done telling Hadoop to use all or part of the composite key as a partitioning key. All lines with the same partition key are guaranteed to be processed by the same reducer. This is similar to the sort key, but allows for control at a higher level. This \"custom partitioning\" will both solve our sorting troubles from last week and solve the problem you saw when using multiple reducers in Exercise 1 this week. To use custom partitioning, there are 2 more parameters we need to add to our Hadoop Streaming Command:\n",
    "\n",
    " __`-D mapreduce.partition.keypartitioner.options=\"-k1,1\"`__: tells Hadoop that we want to use the first key field as a partition key. Note: just like the `keycomparator`, `keypartitioner` must be used in conjuction with `stream.num.map.output.key.fields`.\n",
    "\n",
    "__`-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner`__: tells Hadoop that we want to partition the data in a custom way. Note about partitioner: this line **MUST** appear after all of the `-D` options or it will be ignored.\n",
    "\n",
    "> __DISCUSSION QUESTIONS (before exercise 2):__\n",
    "* Quick review: What is a composite key? \n",
    "* Quick review: Practically speaking, what is a 'partition'? How is this concept related to the HDFS output of a job?\n",
    "* What is the difference between the partition key & the 'sort' key?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS (before exercise 2)__\n",
    "* Quick review: What is a composite key?  \n",
    "> We created a composite key in Breakout 4 -- that was when we told Hadoop to treat the first two fields together as the key. We did this above for sorting purposes. Another way to create a composite key would have been to simply make a string joinin the two fields with a comma or dash (anything other than the Hadoop field delimiter).\n",
    "\n",
    "* Quick review: Practically speaking, what is a 'partition'? How is this concept related to the HDFS output of a job?\n",
    "> Partition = data split / guaranteed co-location of records for processing in reduce tasks. Number of partitions = number of reduce tasks = number of files in the HDFS output directory.\n",
    "\n",
    "* What is the difference between the partition key & the 'sort' key?\n",
    "> Normally Hadoop's only guarantee is that records with the same key get processed on the same reducer node. However, if we wanted to process multiple keys together on a given reducer, we could use a partition key which determines which reducer note handles a record... but still allows us to sort on something more granular (eg. another field)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Tasks:\n",
    "\n",
    "* __a) read provided code:__ In __`PartitionSort/mapper.py`__ we've provided a function that will assign a custom partition key (just a letter) to each word. We're going to use this mapper to sort our sample file with 3 partitions. Read this script. \n",
    "\n",
    "* __b) discus:__ How does the mapper decide which partition to assign each record? When you print out the results in what order do you expect to see the records?\n",
    "\n",
    "* __c) Hadoop job:__ Add the required parameters to complete the Hadoop Streaming code below. Your job should partition based on the newly added first key field, and sort alphabetically by word. Run your job. [__`Hint:`__ Don't forget to specify the number of fields!]\n",
    "\n",
    "* __d) discuss:__ Examine the output from your job. Compare it to the partitioning function we used. Are the words sorted alphabetically? What is suprising about the partition key that ended up in `part-00000`?\n",
    "\n",
    "* __e) code:__ If time permits, modify your job so that it sorts the words by _count_ instead (still using 3 partitions). To do this you will need to change the partition function in __`PartitionSort/mapper.py`__ so that it partitions based on count instead of the first letter of the word. Use 4 and 8 as your cut-points. You will also need to modify one of the Hadoop parameters (which one? why?). Run your job. Are you able to get a total sort? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing PartitionSort/sample.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile PartitionSort/sample.txt\n",
    "foo\t5\n",
    "quux\t9\n",
    "labs\t100\n",
    "bar\t5\n",
    "qi\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample file into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -copyFromLocal PartitionSort/sample.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - complete your work above then RUN THIS CELL AS IS\n",
    "!chmod a+x PartitionSort/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/demo3/psort-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# parts a - clear output directory (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/psort-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - Hadoop streaming command - ADD SORT and PARTITION PARAMETERS HERE\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  -files PartitionSort/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "\n",
    "  -input {HDFS_DIR}/sample.txt \\\n",
    "  -output {HDFS_DIR}/psort-output \\\n",
    "  -cmdenv PATH={PATH}\\\n",
    "  -numReduceTasks 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/demo3/psort-output': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob729108224399404002.jar tmpDir=null\n",
      "21/01/18 11:02:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "21/01/18 11:02:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "21/01/18 11:02:55 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "21/01/18 11:02:55 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "21/01/18 11:02:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1610967571082_0003\n",
      "21/01/18 11:02:55 INFO impl.YarnClientImpl: Submitted application application_1610967571082_0003\n",
      "21/01/18 11:02:55 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1610967571082_0003/\n",
      "21/01/18 11:02:55 INFO mapreduce.Job: Running job: job_1610967571082_0003\n",
      "21/01/18 11:03:01 INFO mapreduce.Job: Job job_1610967571082_0003 running in uber mode : false\n",
      "21/01/18 11:03:01 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "21/01/18 11:03:06 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "21/01/18 11:03:07 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "21/01/18 11:03:12 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "21/01/18 11:03:13 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "21/01/18 11:03:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "21/01/18 11:03:15 INFO mapreduce.Job: Job job_1610967571082_0003 completed successfully\n",
      "21/01/18 11:03:15 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=76\n",
      "\t\tFILE: Number of bytes written=750501\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=270\n",
      "\t\tHDFS: Number of bytes written=48\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5454\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9802\n",
      "\t\tTotal time spent by all map tasks (ms)=5454\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9802\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5454\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=9802\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5584896\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10037248\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=5\n",
      "\t\tMap output bytes=48\n",
      "\t\tMap output materialized bytes=94\n",
      "\t\tInput split bytes=220\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce shuffle bytes=94\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=5\n",
      "\t\tSpilled Records=10\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=124\n",
      "\t\tCPU time spent (ms)=3700\n",
      "\t\tPhysical memory (bytes) snapshot=1173082112\n",
      "\t\tVirtual memory (bytes) snapshot=6876905472\n",
      "\t\tTotal committed heap usage (bytes)=1268252672\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=48\n",
      "21/01/18 11:03:15 INFO streaming.StreamJob: Output directory: /user/root/demo3/psort-output\n"
     ]
    }
   ],
   "source": [
    "# <--- SOLUTION --->\n",
    "# part a - Hadoop streaming command\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/psort-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k3,3nr\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\"  \\\n",
    "  -files PartitionSort/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/sample.txt \\\n",
    "  -output {HDFS_DIR}/psort-output \\\n",
    "  -cmdenv PATH={PATH}\\\n",
    "  -numReduceTasks 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - Save results locally (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/psort-output/part-0000* > PartitionSort/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\tlabs\t100\t\n",
      "B\tquux\t9\t\n",
      "C\tbar\t5\t\n",
      "C\tfoo\t5\t\n",
      "A\tqi\t1\t\n"
     ]
    }
   ],
   "source": [
    "# part a - view results (RUN THIS CELL AS IS)\n",
    "!head PartitionSort/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\tlabs\t100\t\n",
      "B\tquux\t9\t\n"
     ]
    }
   ],
   "source": [
    "# part a - look at first partition (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/psort-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\tbar\t5\t\n",
      "C\tfoo\t5\t\n"
     ]
    }
   ],
   "source": [
    "# part a - look at second partition (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/psort-output/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tqi\t1\t\n"
     ]
    }
   ],
   "source": [
    "# part a - look at third partition (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/psort-output/part-00002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected Output:__\n",
    "\n",
    "<table>\n",
    "<th>part c</th>\n",
    "<th>part e</th>\n",
    "<tr>\n",
    "<td><pre>\n",
    "B\tlabs\t100\n",
    "C\tqi\t1\n",
    "C\tquux\t9\n",
    "A\tbar\t5\n",
    "A\tfoo\t5\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "B\tlabs\t100\n",
    "B\tquux\t9\n",
    "C\tfoo\t5\n",
    "C\tbar\t5\n",
    "A\tqi\t1\n",
    "</pre></td>\n",
    "</tr></table>\n",
    "\n",
    "> __DISCUSSION QUESTIONS (exercise 2 debrief):__\n",
    "* In the provided implementation how did we assign records to partitions?\n",
    "* In part c, why didn't this partitioning result in alphabetically sorted words?\n",
    "* Given what you saw in part c, how did you 'trick' Hadoop into doing a full sort (by count) in part e?\n",
    "* If you didn't achive the full sort in `e` why not? On a larger data set what postprocessing would you have to do in this kind of scenario? Is this postprocessing non-trivial?\n",
    "* In addition to changing the partition function what other Hadoop parameter did you have to change for part `e`?\n",
    "* In the real world you wouldn't want your partition key as part of your output. What would we do to avoid this?\n",
    "* Does anyone need any additional clarification about any of the Hadoop Streaming?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS (exercise 2 debrief)__\n",
    "\n",
    "* In the provided implementation how did we assign records to partitions?\n",
    " > according to the first letter in the word... a-h when in partition 'A', j-o in partition 'B' and p=z in partition 'C'\n",
    "* In part c, why didn't this partitioning result in alphabetically sorted words?\n",
    " > We assigned the top(alphabetical) words to partition A, but when Hadoop concatenated the files, it put partition A last. This is not by chance. Hadoop uses a hash function to assign our human readible partition keys to an ordered list of partitions. The ordering that is logical to us, may or may not get preserved in the process. However, its worth noting that this hash function is consistent. i.e. we just say Hadoop order B-C-A, that means it will always order those three keys that way. We can take advantage of this fact to perform a Total Order Sort -- i.e. a sort with multiple paritions that doestn' require any post-processing. Implementing this will be one of the most important question on HW2.\n",
    " \n",
    "* Given what you saw in part c, how did you 'trick' Hadoop into doing a full sort (by count) in part e?\n",
    "> Since we knew partition B was going to go first, we put the highest counts there, followed by the 5-9 count words in 'C' and the less than 5 count words in 'A'.\n",
    "\n",
    "* If you didn't achive the full sort in `e` why not? On a larger data set what postprocessing would you have to do in this kind of scenario? Is this postprocessing non-trivial?\n",
    "> Not knowing about the hash function you may have simply assigned the top counts to A.. in that case we still have the counts segemented by top/middle/bottom we'd just need to figure out in what order the partitions should be concatenated. This is a lot less work than the merge-sort we needed for postprocessing our secondary sort in break out 4... but with a really large corpus this could still be in-ideal because it requires checking and comparing each partition.\n",
    "\n",
    "* In addition to changing the partition function what other Hadoop parameter did you have to change for part `e`?\n",
    "> the `keycomparator` -- so that it reverse numerical sorts on the 3rd field within each partition\n",
    "\n",
    "* Does anyone need any additional clarification about any of the Hadoop Streaming parameters -- you will need to be fluent in these for HW2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Relative Frequencies Revisited\n",
    "### (+ intro to counters)\n",
    "\n",
    "Now that you know how to use custom partition keys, let's use this techinique to fix the problem from part d in Exercise 1.\n",
    "\n",
    "First a brief digression... A common challenge when implementing custom partitioning at scale is load balancing: _how can we ensure that each reducer node is doing approximately the same amount of work?_ Hadoop's option to define custom counters can be a useful way to monitor this kind of detail. Just like the built in  counters that you learned about last week, custom counters are a slight departure from statelessness. Normally we wouldn't want to share a mutable variable across multiple nodes, however in this case the framework manages so that you can increment them from any node without causing a race condition. \n",
    "\n",
    "To use a custom counter you'll just write an appropriately formatted line to the standard error stream. For example the following line would increment a counter called 'counter-name' in group 'MyWordCounters' by 10:\n",
    " > `sys.stderr.write(\"reporter:counter:MyWordCounters,counter-name,10\\n\")`\n",
    " \n",
    "This line can be added to your mapper, combiner or reducer scripts and wraped in `if` clauses to increment only under certain conditions. If a counter with that name/group doesn't exsit yet the framework will create one. The values of your custom counters will be printed to the console in their respective groups just like the built in Job Tracking counters. Counters can only be incremented using integers (ie, floats are not supported).\n",
    "\n",
    "Ok, armed with this new tool let's return to the relative frequencies task.\n",
    "\n",
    "__DISCUSSION:__\n",
    "> * Remember the problem we encountered at the end of Exercise 1? How would a custom partition key help us make sure the total counts get sent to each reducer node?\n",
    "> * What does this do to the number of records being shuffled? Is there a more and a less efficient way to implement it from the perspective of amount of data shuffled?\n",
    "> * Where would you implement the custom partition key?\n",
    "> * How would you partition the records? (i.e what criteria would you use to assign keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS (before exercise 2)__  \n",
    "\n",
    "* How would a custom partition key help us make sure the total counts get sent to each reducer node?\n",
    "> If each partition has a specfic key then we can explicitly send the totals to each one.\n",
    "\n",
    "* What does this do to the number of records being shuffled?\n",
    "> We will be emitting  alot more records - x4 for four reducers. But if we are using local aggregation this needn't increase the number of records being shuffled unduly. This would be a great situaion in which to use an in-mapper aggregator.\n",
    "\n",
    "* Where would you implement the custom partition key?\n",
    "> In the first mapper.\n",
    "\n",
    "* How would you partition the records? (i.e what criteria would you use to assign keys)\n",
    "> It doesn't really matter since we don't actually care where the records end up... however it would be smart to try and balance out the amount of work done on each reducer so in this case we could use the alphabet or some other EDA to write a partition function based on the words. Since we're going to use 4 reducers maybe you'd do something like a-g, h-n, o-t,u-z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Tasks:\n",
    "\n",
    "## UPDATE Jan 23, 2021 - Combiner has been removed from this exercise. Combiners do not play nice with secondary sort. https://issues.apache.org/jira/browse/MAPREDUCE-3310\n",
    "\n",
    "* __a) add a custom partitioner:__  In __`MultiPart/mapper.py`__, and __`MultiPart/reducer.py`__ we've copied the base code from the frequencies job. Complete the code in this mapper so that it adds a custom partition key to each record and emits the total subcounts _once for each partition_. Assume we'll be using 4 partitions. Then make any required adjustments to your reducer and combiner to accomodate the new record format (your final output should not include the partition key, though you may want it there initially for debugging purposes).\n",
    "\n",
    "* __b) discuss:__ Keep in mind that each partition still needs the total counts to arrive before the individual word counts. However since you've added a custom partition key to each record, the order inversion trick of adding a special character isn't going to work with Hadoop's default sorting behavior any more. Why not? What will you need to specify in your Hadoop job to make sure that the totals still arrive first?\n",
    "\n",
    "* __c) unit test & run your job:__ Write a few unit tests to debug your scripts. When you have them working as expected run the Hadoop job. [__`NOTE`__ We've provided a few tests to get you started but you'll need to add a unix sort to mimic the sorting you discussed in part 'b'].\n",
    "\n",
    "* __d) custom counters:__ Add custom counter(s) to your reducer code so that you can count how many records are processed by each reduce task. [__`TIP`__ use the partition key as the counter name and python3 string formatting to do this efficiently].\n",
    "\n",
    "* __e) discuss:__ Rerun your Hadoop job and take a look at the custom counter values. What do they tell you? Are these counts the same as the number of keys in the result of each partition? Try changing the partitioning criteria in your mapper... what partitioning results in a really uneven split? what partitioning results in the most even split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - make sure scripts are executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x MultiPart/mapper.py\n",
    "!chmod a+x MultiPart/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tfoo\t1\n",
      "A\tfoo\t1\n",
      "C\tquux\t1\n",
      "B\tlabs\t1\n",
      "A\tfoo\t1\n",
      "A\tbar\t1\n",
      "C\tquux\t1\n",
      "A\t!total\t7\n",
      "B\t!total\t7\n",
      "C\t!total\t7\n",
      "D\t!total\t7\n"
     ]
    }
   ],
   "source": [
    "# part c - unit test mapper script\n",
    "!echo \"foo foo quux labs foo bar quux\" | MultiPart/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:MyCounters,A,1\n",
      "reporter:counter:MyCounters,A,1\n",
      "reporter:counter:MyCounters,A,1\n",
      "bar\t0.14285714285714285\n",
      "reporter:counter:MyCounters,A,1\n",
      "reporter:counter:MyCounters,A,1\n",
      "reporter:counter:MyCounters,B,1\n",
      "foo\t0.42857142857142855\n",
      "reporter:counter:MyCounters,B,1\n",
      "reporter:counter:MyCounters,C,1\n",
      "labs\t0.14285714285714285\n",
      "reporter:counter:MyCounters,C,1\n",
      "reporter:counter:MyCounters,C,1\n",
      "reporter:counter:MyCounters,D,1\n",
      "quux\t0.2857142857142857\n",
      "!total\t1.0\n"
     ]
    }
   ],
   "source": [
    "# part c - unit test map-combine-reduce (ADJUST SORT AS NEEDED)\n",
    "!echo \"foo foo quux labs foo bar quux\" | MultiPart/mapper.py | sort -k1,1 | MultiPart/reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/demo3/multipart-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# parts c - clear the output directory (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/multipart-output\n",
    "# NOTE: this directory won't exist unless you are re-running a job, that's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - Hadoop streaming command - FILL IN HERE (don't forget to specify 4 reducers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/demo3/multipart-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob3024830843428666992.jar tmpDir=null\n",
      "21/01/23 17:46:11 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "21/01/23 17:46:11 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "21/01/23 17:46:12 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "21/01/23 17:46:12 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "21/01/23 17:46:12 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1610967571082_0011\n",
      "21/01/23 17:46:13 INFO impl.YarnClientImpl: Submitted application application_1610967571082_0011\n",
      "21/01/23 17:46:13 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1610967571082_0011/\n",
      "21/01/23 17:46:13 INFO mapreduce.Job: Running job: job_1610967571082_0011\n",
      "21/01/23 17:46:22 INFO mapreduce.Job: Job job_1610967571082_0011 running in uber mode : false\n",
      "21/01/23 17:46:22 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "21/01/23 17:46:28 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "21/01/23 17:46:29 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "21/01/23 17:46:36 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "21/01/23 17:46:38 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "21/01/23 17:46:39 INFO mapreduce.Job: Job job_1610967571082_0011 completed successfully\n",
      "21/01/23 17:46:39 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=368976\n",
      "\t\tFILE: Number of bytes written=1642736\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=179007\n",
      "\t\tHDFS: Number of bytes written=88977\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8114\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=19396\n",
      "\t\tTotal time spent by all map tasks (ms)=8114\n",
      "\t\tTotal time spent by all reduce tasks (ms)=19396\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8114\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=19396\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8308736\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=19861504\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3774\n",
      "\t\tMap output records=30626\n",
      "\t\tMap output bytes=307700\n",
      "\t\tMap output materialized bytes=369000\n",
      "\t\tInput split bytes=218\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3029\n",
      "\t\tReduce shuffle bytes=369000\n",
      "\t\tReduce input records=30626\n",
      "\t\tReduce output records=3021\n",
      "\t\tSpilled Records=61252\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=376\n",
      "\t\tCPU time spent (ms)=9690\n",
      "\t\tPhysical memory (bytes) snapshot=1425997824\n",
      "\t\tVirtual memory (bytes) snapshot=8308097024\n",
      "\t\tTotal committed heap usage (bytes)=1563426816\n",
      "\tMyCounters\n",
      "\t\tA=8411\n",
      "\t\tB=6833\n",
      "\t\tC=6836\n",
      "\t\tD=8546\n",
      "\t\tmapper=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=178789\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=88977\n",
      "21/01/23 17:46:39 INFO streaming.StreamJob: Output directory: /user/root/demo3/multipart-output\n"
     ]
    }
   ],
   "source": [
    "# <--- SOLUTION --->\n",
    "# https://issues.apache.org/jira/browse/MAPREDUCE-3310 combiners don't play nice with secondary sort!\n",
    "# part c - Hadoop streaming command\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/multipart-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\"  \\\n",
    "  -files MultiPart/mapper.py,MultiPart/combiner.py,MultiPart/reducer.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/alice.txt \\\n",
    "  -output {HDFS_DIR}/multipart-output \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PARTITION1 ==========\n",
      "t\t0.007119994774315762\n",
      "table\t0.0005878894767783657\n",
      "tail\t0.00029394473838918284\n",
      "tails\t9.798157946306095e-05\n",
      "take\t0.0007185315827291136\n",
      "cat: Unable to write to output stream.\n",
      "========== PARTITION2 ==========\n",
      "a\t0.022699065908942453\n",
      "abide\t6.532105297537396e-05\n",
      "able\t3.266052648768698e-05\n",
      "about\t0.003331373701744072\n",
      "above\t9.798157946306095e-05\n",
      "========== PARTITION3 ==========\n",
      "gained\t3.266052648768698e-05\n",
      "gallons\t3.266052648768698e-05\n",
      "game\t0.0004245868443399308\n",
      "games\t3.266052648768698e-05\n",
      "garden\t0.0005225684238029917\n",
      "========== PARTITION4 ==========\n",
      "name\t0.0003592657913645568\n",
      "named\t3.266052648768698e-05\n",
      "names\t6.532105297537396e-05\n",
      "narrow\t6.532105297537396e-05\n",
      "nasty\t3.266052648768698e-05\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "#  part c - take a look at a few records from each partition (RUN THIS CELL AS IS)\n",
    "for p in range(4):\n",
    "    print('='*10,f'PARTITION{p+1}','='*10)\n",
    "    !hdfs dfs -cat {HDFS_DIR}/multipart-output/part-0000{p} | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected Results:__  \n",
    "__`NOTE:`__ exact partition splits depend on your custom function:\n",
    "\n",
    "Test file:\n",
    "<table>\n",
    "<th>partition 1</th>\n",
    "<th>partition 2</th>\n",
    "<th>partition 3</th>\n",
    "<th>partition 4</th>\n",
    "<tr>\n",
    "<td><pre>\n",
    "test\t0.15\n",
    "this\t0.15\n",
    "two\t0.05\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "a\t0.1\n",
    "file\t0.15\n",
    "for\t0.05\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "has\t0.05\n",
    "is\t0.1\n",
    "lines\t0.05\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "small\t0.15\n",
    "</pre></td>\n",
    "</tr></table>\n",
    "\n",
    "Full Alice Text:\n",
    "<table>\n",
    "<tr>\n",
    "<th>partition 1</th>\n",
    "<th>partition 2</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><pre>\n",
    "t\t0.007119994774315762\n",
    "table\t0.0005878894767783657\n",
    "tail\t0.00029394473838918284\n",
    "tails\t9.798157946306095e-05\n",
    "take\t0.0007185315827291136\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "a\t0.022699065908942453\n",
    "abide\t6.532105297537396e-05\n",
    "able\t3.266052648768698e-05\n",
    "about\t0.003331373701744072\n",
    "above\t9.798157946306095e-05\n",
    "</pre></td>\n",
    "</tr>\n",
    "    <tr>\n",
    "<th>partition 3</th>\n",
    "<th>partition 4</th>\n",
    "</tr>\n",
    "    <tr><td><pre>\n",
    "gained\t3.266052648768698e-05\n",
    "gallons\t3.266052648768698e-05\n",
    "game\t0.0004245868443399308\n",
    "games\t3.266052648768698e-05\n",
    "garden\t0.0005225684238029917\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "name\t0.0003592657913645568\n",
    "named\t3.266052648768698e-05\n",
    "names\t6.532105297537396e-05\n",
    "narrow\t6.532105297537396e-05\n",
    "nasty\t3.266052648768698e-05\n",
    "</pre></td>\n",
    "</tr></table>\n",
    "\n",
    ">__DISCUSSION__\n",
    "> * After adding the custom partition key what else did you have to do so that the order inversion pattern would still work?\n",
    "> * In part `d` what did you see from your custom counters? Do these numbers match the number of keys in the result of each partition? Why/why not?\n",
    "> * What other partitioning cuts did you try? What partitioning results in a really uneven split? what partitioning results in the most even split?\n",
    "> * Do custom counters solve the load balancing challenge?\n",
    "> * If we wanted to design a subsequent job to sort these results from highest to lowest frequency what partitioning strategy would you explore? Any particular challenge there? (__`HINT:`__ you may wish to consider the kinds of numbers you saw in the results from Exercise 1, how python stores floats, and what you know about word frequencies in natural language)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS (after exercise 3)__\n",
    "* After adding the custom partition key what else did you have to do so that the order inversion pattern would still work?\n",
    "> We need to add a secondary sort.\n",
    "\n",
    "* In part `d` what did you see from your custom counters? Do these numbers match the number of keys in the result of each partition? Why/why not?\n",
    "> Results will vary for counts depending on the partition students choose & the ammount of combining. We saw: \n",
    "```\n",
    "A=1391\n",
    "B=845\n",
    "C=1170\n",
    "D=633 \n",
    "```\n",
    "\n",
    " >Note that these are not the same as the number of resulting lines in each partition. Some words eg. 'the' will occur many more times than others, so even if we divide the alphabet evenly that may not be an even split of word's processed.\n",
    "Use this opportunity to reinforce the fact that Hadoop did not put partition 'A' first & remind students about the hash function. They'll implement a Total Order Sort in HW2 where they reverse engineer that hash function to pre-order the partition keys.\n",
    "\n",
    "* What other partitioning cuts did you try? What partitioning results in a really uneven split? what partitioning results in the most even split?\n",
    "> Results will vary. Give students an opportunity to share & then discuss their reasoning.\n",
    "\n",
    "* Do custom counters solve the load balancing challenge?\n",
    "> No, they can help us determine whether we have a load balancing problem in the first place, but solving that problem if it exists requires some outside understading of our data distribution. That's usually one of the key goals behind our EDA. With really large datasets we'll also often do this EDA on a random sample instead of the full data -- we'll talk more about that in week 5.\n",
    "\n",
    "* If we wanted to design a subsequent job to sort these results from highest to lowest frequency what partitioning strategy would you explore? Any particular challenge there? (__`HINT:`__ you may wish to consider the kinds of numbers you saw in the results from Exercise 1, how python stores floats, and what you know about word frequencies in natural language).\n",
    "> We'd need to partition based on the relative frequencies but that is a challenge because the numbers are very very small and we're going to run into floating point errors. Another challenge is that this is going to be a inverse power law distribution --- lots of words which occur very few times & whose relative frequencies are clustered around 0. We could try a log-transformation to help us partition... but in practice we'd want to use EDA on specific words to design a partition strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Unordered Total Sort vs Total Order Sort\n",
    "\n",
    "In HW1 we emphasized that sorting can be a useful preprocessing tool because sorted input can sometimes facilitate a more efficient algorithm design. However sorting is also often desirable in its own right. For example suppose you wanted to get the top and bottom 100 words according to their relative frequencies? The best way to get these results would be to do a Total Order Sort -- that is to sort the entire data set from highest to lowest so that you can simply read the first 100 records in the first partition and last 100 records in the last partition.\n",
    "\n",
    "So far we've tried two different ways of sorting using the Hadoop framework and neither quite achieved this result to satisfaction. First, in last week's breakout we performed a secondary sort using the `keycomparator` and accompanying Hadoop parameters. When we used a single reducer that strategy _did_ successfully result in a sort from top to bottom, but in a class all about scaling up for large data solutions that require a single reducer won't get us very far. Unfortunately when we tried using those same three sorting parameters and with multiple reducers we ended up with results sorted within each partition but not across partitions. This is what we call that a _partial sort_ because records are only ordered relative to the other keys that happen to be processed by the same reducer. Importantly a partial sort is of no use at all for us if we wanted to find the top 100 and bottom 100 from our relative frequencies file.\n",
    "\n",
    "Then in Exercise 2 of this notebook we learned how to specify a custom partition key so that we can explicitly control the partitioning. By combining the use of a custom partition key and the secondary sort from last week we were able to ensure that our results were not only sorted within their paritions but also that the paritioning grouped records with similar value ranges together. In theory this should have solved our 'total sort with multiple reducers' challenge but in practice something odd happened: the records with the top value didn't reliably end up in the first partition nor did the records with the lowest values necessarily end up in the last. The partition files themselves were out of order, making this an _unordered total sort_.\n",
    "\n",
    "The reason that the partitions appear out of order has to do with the hash function that Hadoop applies to your custom partition keys. That hash function results in an ordering that may not match the human readible string or integer numbering you as a programmer might have intended. Luckily, this hash function is both fixed and known -- so for example, if you are using partition keys `A`, `B`, and `C` your partitions will always end up ordered `B` - `C` -`A` (as you saw in Exercise 2). That means if we know in advance how many partitions (i.e. reducers) we plan to use, we can reverse engineer the hash function to figure out how Hadoop will order those keys and plan accordingly (for example in our 3 partition case by specifying that the top values should get the partition key `B` not `A`). When your data is sorted not only within each partition but the partitions are also in the right order, you've achieved a _total order sort_. \n",
    "\n",
    "Let's give it a try! Your job in this exercise is to write a Hadoop Job that will perform a total order sort on the output of Exercise 1 (relative frequencies) with any number of partitions.\n",
    "\n",
    "__Note__: Part III.D.4 in the [Total Sort Notebook](https://github.com/UCB-w261/main/blob/master/HelpfulResources/TotalSortGuide/_total-sort-guide-hadoop-streaming.ipynb) has a much more comprehensive explanation that you may wish to reference here.\n",
    "\n",
    "\n",
    "### Exercise 4 Tasks\n",
    "* __a) discuss:__ In the `%%writefile` cell below we create a partition file (__`partitions.txt`__) which contains the cut points we'll use to partition the data. Based on your reading of this file, how many partitions will we use? Think about the kinds of values that we got in Exercise 1 ... can you see any potential problems with these cutpoints?\n",
    "\n",
    "\n",
    "* __b) discuss:__ Read through __`TotalOrderSort/mapper.py`__ and __`TotalOrderSort/reducer.py`__. Which of these scripts makes use of our partition file? What does the mapper do? What does the reducer do? \n",
    "\n",
    "* __c) code:__ Run the provided code below to apply this mapper and reducer. Note how the Hadoop job reads directly from the output directory we created in Exercise 1 (you will need to be those results were computed inorder to run this job). Use the provided code to view the output and confirm that the current implementation performs an unordered sort. What adjustments might you want to make to the partition file? (go ahead and modify it as desired). Then modify the Hadoop job so that it uses `/bin/cat` instead of `reducer.py`... and re-run the job this will allow you to see the order in which the partition keys were sorted. \n",
    "\n",
    "* __d) discuss:__ Read through __`TotalOrderSort/TOS_mapper.py`__. Pay particular attention to the helper function `makeKeyHash()` -- what does this function do? how is it used? how is this mapper different that the original one? \n",
    "\n",
    "* __e) code + discussion:__ Replace the original mapper with this new mapper(_don't forget to add it to the `-files` line_) and rerun the job (still with `/bin/cat` as reducer). How did the parition ordering change? What happpens if you change the partition file so that it has one extra number? Does your job work? Re-place the true reducer. Et voila, you have achieved total order sort!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS__\n",
    "> __a)__ 5 partitions. Most relative frequencies will be very low decimals so most of the data is going to end up in the last few partitions. We can fix this by performing EDA on the input file and determining a way to balance the load across all 5 reducers. In this case we might use cutpoints more like: 0.5,0.3,0.2,0.1,0.\n",
    "\n",
    "> __b)__ The mapper reads the partition file and uses those cut points to assign letters for partition keys (eg. A, B, C, etc). The reducer simply removes these partition keys.\n",
    "\n",
    "> __d)__ `makeKeyHash` allows us to sort the parition keys in the order that Hadoop will, this different ordering of partition keys is the only difference between this mapper and the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting TotalOrderSort/partitions.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile TotalOrderSort/partitions.txt\n",
    "0.01,0.005,0.003,0.002,0.001,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - make sure files are executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x TotalOrderSort/mapper.py\n",
    "!chmod a+x TotalOrderSort/reducer.py\n",
    "!chmod a+x TotalOrderSort/TOS_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/demo3/tos-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.0.jar] /tmp/streamjob3305256276489392893.jar tmpDir=null\n",
      "18/09/19 07:41:23 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/09/19 07:41:23 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/09/19 07:41:24 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/09/19 07:41:24 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/09/19 07:41:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1537278954088_0020\n",
      "18/09/19 07:41:25 INFO impl.YarnClientImpl: Submitted application application_1537278954088_0020\n",
      "18/09/19 07:41:25 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1537278954088_0020/\n",
      "18/09/19 07:41:25 INFO mapreduce.Job: Running job: job_1537278954088_0020\n",
      "18/09/19 07:41:33 INFO mapreduce.Job: Job job_1537278954088_0020 running in uber mode : false\n",
      "18/09/19 07:41:33 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/09/19 07:41:44 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/09/19 07:41:45 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/09/19 07:41:58 INFO mapreduce.Job:  map 100% reduce 17%\n",
      "18/09/19 07:42:01 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "18/09/19 07:42:04 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "18/09/19 07:42:05 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "18/09/19 07:42:06 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/09/19 07:42:07 INFO mapreduce.Job: Job job_1537278954088_0020 completed successfully\n",
      "18/09/19 07:42:07 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=103005\n",
      "\t\tFILE: Number of bytes written=1412020\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=92273\n",
      "\t\tHDFS: Number of bytes written=96949\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=12\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=6\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14063\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=87305\n",
      "\t\tTotal time spent by all map tasks (ms)=14063\n",
      "\t\tTotal time spent by all reduce tasks (ms)=87305\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14063\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=87305\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=14400512\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=89400320\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3010\n",
      "\t\tMap output records=3010\n",
      "\t\tMap output bytes=96949\n",
      "\t\tMap output materialized bytes=103041\n",
      "\t\tInput split bytes=258\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3010\n",
      "\t\tReduce shuffle bytes=103041\n",
      "\t\tReduce input records=3010\n",
      "\t\tReduce output records=3010\n",
      "\t\tSpilled Records=6020\n",
      "\t\tShuffled Maps =12\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=12\n",
      "\t\tGC time elapsed (ms)=787\n",
      "\t\tCPU time spent (ms)=14480\n",
      "\t\tPhysical memory (bytes) snapshot=1715482624\n",
      "\t\tVirtual memory (bytes) snapshot=10984353792\n",
      "\t\tTotal committed heap usage (bytes)=1302331392\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92015\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=96949\n",
      "18/09/19 07:42:07 INFO streaming.StreamJob: Output directory: /user/root/demo3/tos-output\n"
     ]
    }
   ],
   "source": [
    "# part c - hadoop job (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/tos-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k3,3nr\" \\\n",
    "  -files TotalOrderSort/mapper.py,TotalOrderSort/reducer.py,TotalOrderSort/partitions.txt \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/frequencies-output \\\n",
    "  -output {HDFS_DIR}/tos-output \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/demo3/tos-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.0.jar] /tmp/streamjob3874368387786140947.jar tmpDir=null\n",
      "18/09/19 07:50:12 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/09/19 07:50:13 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/09/19 07:50:14 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/09/19 07:50:14 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/09/19 07:50:14 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1537278954088_0022\n",
      "18/09/19 07:50:14 INFO impl.YarnClientImpl: Submitted application application_1537278954088_0022\n",
      "18/09/19 07:50:14 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1537278954088_0022/\n",
      "18/09/19 07:50:14 INFO mapreduce.Job: Running job: job_1537278954088_0022\n",
      "18/09/19 07:50:23 INFO mapreduce.Job: Job job_1537278954088_0022 running in uber mode : false\n",
      "18/09/19 07:50:23 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/09/19 07:50:32 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/09/19 07:50:33 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/09/19 07:50:44 INFO mapreduce.Job:  map 100% reduce 17%\n",
      "18/09/19 07:50:48 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "18/09/19 07:50:51 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "18/09/19 07:50:52 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "18/09/19 07:50:53 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/09/19 07:50:53 INFO mapreduce.Job: Job job_1537278954088_0022 completed successfully\n",
      "18/09/19 07:50:53 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=103005\n",
      "\t\tFILE: Number of bytes written=1412116\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=92273\n",
      "\t\tHDFS: Number of bytes written=87919\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=12\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=6\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11627\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=74728\n",
      "\t\tTotal time spent by all map tasks (ms)=11627\n",
      "\t\tTotal time spent by all reduce tasks (ms)=74728\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11627\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=74728\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11906048\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=76521472\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3010\n",
      "\t\tMap output records=3010\n",
      "\t\tMap output bytes=96949\n",
      "\t\tMap output materialized bytes=103041\n",
      "\t\tInput split bytes=258\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3010\n",
      "\t\tReduce shuffle bytes=103041\n",
      "\t\tReduce input records=3010\n",
      "\t\tReduce output records=3010\n",
      "\t\tSpilled Records=6020\n",
      "\t\tShuffled Maps =12\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=12\n",
      "\t\tGC time elapsed (ms)=539\n",
      "\t\tCPU time spent (ms)=13280\n",
      "\t\tPhysical memory (bytes) snapshot=1678360576\n",
      "\t\tVirtual memory (bytes) snapshot=10929672192\n",
      "\t\tTotal committed heap usage (bytes)=1217396736\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92015\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=87919\n",
      "18/09/19 07:50:53 INFO streaming.StreamJob: Output directory: /user/root/demo3/tos-output\n"
     ]
    }
   ],
   "source": [
    "# <--- SOLUTION --->\n",
    "# part c - hadoop job (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/tos-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k3,3nr\" \\\n",
    "  -files TotalOrderSort/TOS_mapper.py,TotalOrderSort/reducer.py,TotalOrderSort/partitions.txt \\\n",
    "  -mapper TOS_mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/frequencies-output \\\n",
    "  -output {HDFS_DIR}/tos-output \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============== PART-00000===============\n",
      "Number of lines processed by this reducer: 15\n",
      "\n",
      "!total  1.0\n",
      "the     0.059757420372744306\n",
      "and     0.03089767610031884\n",
      "to      0.026591723367189297\n",
      "a       0.0226802090523617\n",
      "of      0.020740886829043816\n",
      "it      0.020050619597015415\n",
      "she     0.018177037110081187\n",
      "i       0.01791407816454656\n",
      "you     0.015810406600269535\n",
      "\n",
      "============== PART-00001===============\n",
      "Number of lines processed by this reducer: 16\n",
      "\n",
      "as    0.009006343884561023\n",
      "her   0.00815172731157348\n",
      "with  0.00749432994773691\n",
      "at    0.007461460079545081\n",
      "s     0.007198501134010452\n",
      "t     0.0071656312658186245\n",
      "on    0.006705453111133025\n",
      "all   0.0065739736383657104\n",
      "this  0.005949446142720968\n",
      "for   0.005883706406337311\n",
      "\n",
      "============== PART-00002===============\n",
      "Number of lines processed by this reducer: 18\n",
      "\n",
      "so      0.004996219965157939\n",
      "very    0.0047661308878151395\n",
      "what    0.004667521283239654\n",
      "is      0.004437432205896854\n",
      "he      0.004207343128554054\n",
      "little  0.004207343128554054\n",
      "out     0.003878644446635769\n",
      "if      0.003812904710252112\n",
      "one     0.0034842060283338263\n",
      "up      0.0033855964237583407\n",
      "\n",
      "============== PART-00003===============\n",
      "Number of lines processed by this reducer: 33\n",
      "\n",
      "them     0.0028925484008809127\n",
      "know     0.0028925484008809127\n",
      "project  0.002859678532689084\n",
      "were     0.0027939387963054267\n",
      "like     0.0027939387963054267\n",
      "have     0.0027939387963054267\n",
      "herself  0.0027281990599217695\n",
      "again    0.0027281990599217695\n",
      "went     0.0027281990599217695\n",
      "would    0.0027281990599217695\n",
      "\n",
      "============== PART-00004===============\n",
      "Number of lines processed by this reducer: 75\n",
      "\n",
      "now     0.0019721920915097132\n",
      "turtle  0.0019393222233178844\n",
      "way     0.001906452355126056\n",
      "my      0.001906452355126056\n",
      "began   0.001906452355126056\n",
      "tm      0.0018735824869342275\n",
      "ll      0.0018735824869342275\n",
      "which   0.0018407126187423989\n",
      "mock    0.0018407126187423989\n",
      "hatter  0.0018407126187423989\n",
      "\n",
      "============== PART-00005===============\n",
      "Number of lines processed by this reducer: 2853\n",
      "\n",
      "succeeded  9.860960457548565e-05\n",
      "moving     9.860960457548565e-05\n",
      "venture    9.860960457548565e-05\n",
      "mark       9.860960457548565e-05\n",
      "picked     9.860960457548565e-05\n",
      "pie        9.860960457548565e-05\n",
      "pieces     9.860960457548565e-05\n",
      "stopped    9.860960457548565e-05\n",
      "yesterday  9.860960457548565e-05\n",
      "pity       9.860960457548565e-05\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# part c - print top words in each class (RUN THIS CELL AS IS)\n",
    "for idx in range(6):\n",
    "    print(f\"\\n============== PART-0000{idx}===============\")\n",
    "    numLines = !hdfs dfs -cat {HDFS_DIR}/tos-output/part-0000{idx} | wc -l\n",
    "    print(f\"Number of lines processed by this reducer: {numLines[0]}\\n\")\n",
    "    !hdfs dfs -cat {HDFS_DIR}/tos-output/part-0000{idx} | head | column -t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "827px",
    "left": "0px",
    "right": "1125px",
    "top": "107px",
    "width": "428px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
