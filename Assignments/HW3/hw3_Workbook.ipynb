{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c052a4ea-9f23-4194-afc4-ef6472f60a3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## HW 3 - Synonym Detection In Spark\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Summer 2022`__\n",
    "\n",
    "In the last homework assignment you performed Naive Bayes to classify documents as 'ham' or 'spam.' In doing so, we relied on the implicit assumption that the list of words in a document can tell us something about the nature of that document's content. We'll rely on a similar intuition this week: the idea that, if we analyze a large enough corpus of text, the list of words that appear in small window before or after a vocabulary term can tell us something about that term's meaning. This is similar to the intuition behind the word2vec algorithm.\n",
    "\n",
    "This will be your first assignment working in Spark. You'll perform Synonym Detection by repurposing an algorithm commonly used in Natural Language Processing to perform document similarity analysis. In doing so you'll also become familiar with important datatypes for efficiently processing sparse vectors and a number of set similarity metrics (e.g. Cosine, Jaccard, Dice). By the end of this homework you should be able to:  \n",
    "* ... __define__ the terms `one-hot encoding`, `co-occurrence matrix`, `stripe`, `inverted index`, `postings`, and `basis vocabulary` in the context of both synonym detection and document similarity analysis.\n",
    "* ... __explain__ the reasoning behind using a word stripe to compare word meanings.\n",
    "* ... __identify__ what makes set-similarity calculations computationally challenging.\n",
    "* ... __implement__ stateless algorithms in Spark to build stripes, inverted index and compute similarity metrics.\n",
    "* ... __identify__ when it makes sense to take a stripe approach and when to use pairs\n",
    "* ... __apply__ appropriate metrics to assess the performance of your synonym detection algorithm. \n",
    "\n",
    "__RECOMMENDED READING FOR HW3__:\t\n",
    "Your reading assignment for weeks 4 and 5 were fairly heavy and you may have glossed over the papers on dimension independent similarity metrics by [Zadeh et al](http://stanford.edu/~rezab/papers/disco.pdf) and pairwise document similarity by [Elsayed et al](https://terpconnect.umd.edu/~oard/pdf/acl08elsayed2.pdf). If you haven't already, this would be a good time to review those readings, especially when it comes to the similarity formulas -- they are directly relevant to this assignment.\n",
    "\n",
    "DITP Chapter 4 - Inverted Indexing for Text Retrieval. While this text is specific to Hadoop, the Map/Reduce concepts still apply.\n",
    "\n",
    "__Please refer to the Canvas submission form for  homework submission instructions and additional resources.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "516b0fa3-8a8e-4279-a9a9-d0e2610edca0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Notebook Set-Up\n",
    "Before starting your homework run review the following  and run the code cells  to confirm your setup.\n",
    "\n",
    "When you create a DataProc cluster, HDFS is used as the default filesystem. In this course, we override this behavior by setting the defaultFS as a Cloud Storage bucket. The benefits of using Cloud Storage buckets are that your job results get to persist beyond the lifetime of the cluster (and btw latency on these cloud buckets is super low).\n",
    "\n",
    "\n",
    "In this HW, you use a cloud bucket (and folders on them), known as DATA_BUCKET, as input and output for the Spark Apps that you will develop as part of your submission.  \n",
    "The datasets for this homework are preloaded into your `private Data Bucket` on Google Cloud. Recall that you created a private data bucket during the first week of semester. You may have called it w261-<your initials>. Jimi's bucket name is `w261-jgs`. To facilitate ease of access, we have set up location variables for the course-related data buckets. Your private data bucket  can be accessed via: \n",
    "\n",
    "* `os.getenv('DATA_BUCKET','')` \n",
    "    \n",
    "The following image highlights the  Data bucket (1)  and the dataproc staging bucket (2):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.dropbox.com/s/joo5jcf51g6mtp9/DataBuckets_on_w261_PaaS.png?raw=true' style='width:80%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DATA_BUCKET will be used for hosting the input data for this assignment and also to store output from your HW Assignment apps.\n",
    "Associated with each DataProc cluster is a persistant storage bucket that we refer to as the DataProc Staging Bucket. You will be using this staging bucket to store notebooks and other files associated with your HW assignments, and  live sessions. The location of the staging bucket  is made available via `os.getenv(\"STAGING_BUCKET\")`. Since this bucket is persistent, we will no longer need to snapshot your student workspaces. \n",
    "    \n",
    "For more background on Dataproc staging  buckets please see:\n",
    "* https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up global variables for cloud buckets (both Data, Staging) and for Zone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persoanl Data bucket:  gs://hw_data_bucket\n",
      "     145 B  2022-06-15T03:25:30Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/sample_docs.txt\n",
      "     494 B  2022-06-15T03:25:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/systems_test.txt\n",
      "                                 gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/\n",
      "TOTAL: 2 objects, 639 bytes (639 B)\n"
     ]
    }
   ],
   "source": [
    "# Access your personal data bucket and see whats there\n",
    "# (RUN THIS CELL AS IS)\n",
    "import os\n",
    "DATA_BUCKET = os.getenv('DATA_BUCKET','')[:-1] # our private storage bucket location\n",
    "HW3_FOLDER = f\"{DATA_BUCKET}/main/Assignments/HW3/docker/student\"\n",
    "print(f\"Persoanl Data bucket:  {DATA_BUCKET}\")\n",
    "!gsutil ls -lh  {HW3_FOLDER}  #lets have a look the HW3 folder on our private storage bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"STAGING_BUCKET\"] =  \"gs://dataproc-staging-us-central1-913378501339-ychjbide/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGING_BUCKET location: gs://dataproc-staging-us-central1-275377091776-oiu4zcrv/\n",
      "       0 B  2022-05-07T20:15:35Z  gs://dataproc-staging-us-central1-275377091776-oiu4zcrv/notebooks/jupyter/\n",
      "                                 gs://dataproc-staging-us-central1-275377091776-oiu4zcrv/notebooks/jupyter/snapshots/\n",
      "                                 gs://dataproc-staging-us-central1-275377091776-oiu4zcrv/notebooks/jupyter/student-workspace/\n",
      "                                 gs://dataproc-staging-us-central1-275377091776-oiu4zcrv/notebooks/jupyter/ucb-w261-master/\n",
      "TOTAL: 1 objects, 0 bytes (0 B)\n",
      "STAGING_BUCKET: gs://dataproc-staging-us-central1-275377091776-oiu4zcrv/\n",
      "ZONE: us-central1\n"
     ]
    }
   ],
   "source": [
    "# Access staging bucket and see whats there\n",
    "# (RUN THIS CELL AS IS)\n",
    "import os\n",
    "print(f\"STAGING_BUCKET location: {os.getenv('STAGING_BUCKET')}\")\n",
    "!gsutil ls -lh {os.getenv(\"STAGING_BUCKET\")}notebooks/jupyter\n",
    "# GC zone\n",
    "print(f'STAGING_BUCKET: {os.getenv(\"STAGING_BUCKET\")}')\n",
    "ZONE = \"-\".join(os.getenv(\"STAGING_BUCKET\").split(\"-\")[2:4])\n",
    "print(f\"ZONE: {ZONE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark on the DataProc cluster\n",
    "\n",
    "Note: If you get an error with launching Spark, please restart your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6f5abc60-dcd4-468a-b0f2-250552a36386",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# (RUN THIS CELL AS IS)\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6f5abc60-dcd4-468a-b0f2-250552a36386",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "#PWD = !pwd\n",
    "#PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6f5abc60-dcd4-468a-b0f2-250552a36386",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1b509b16-0aab-4d33-9b4a-6d11eadb2524;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.1.3 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.1.3/spark-avro_2.12-3.1.3.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.1.3!spark-avro_2.12.jar (23ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (13ms)\n",
      ":: resolution report :: resolve 1469ms :: artifacts dl 38ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.spark#spark-avro_2.12;3.1.3 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1b509b16-0aab-4d33-9b4a-6d11eadb2524\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (173kB/7ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/16 00:42:59 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/06/16 00:42:59 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/06/16 00:42:59 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/06/16 00:43:00 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "# start Spark Session (RUN THIS CELL AS IS)\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw3_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7909b057-ba61-490a-95e7-e60c7cb06c04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://w261-m.us-central1-b.c.w261-student-349520.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>hw3_notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f564fc9ceb0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get Spark Session info (RUN THIS CELL AS IS)\n",
    "spark #NOTE the Spark UI link provided below as output from this command does not work. See next section on how to get that working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: Setup Spark UI (jobs and stages)\n",
    "\n",
    "Web UI (aka Application UI or webUI or Spark UI) is the web interface of a running Spark application to monitor and inspect Spark job executions in a web browser. The following is a screenshot of the Spark UI. Feel free to skip this section.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.dropbox.com/s/rithkjbboymr0ey/SparkUI_screenshot.png?raw=true' style='width:80%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL:  to view the Spark UI (jobs and stages) \n",
    "\n",
    "Getting access to Spark UI, you will need to create a tunnel from DataProc via the CloudShell. Please follow all three steps depicted below (starting with running the following code cell). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy the following command (and swap out the ZONE for your cluster zone) to cloud shell and run as shown in the screenshot below. \n",
      "And click on the PREVIEW ON PORT 8080 menu option .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gcloud compute ssh w261-m --zone us-central1-b --ssh-flag \"-L 8080:localhost:4040\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional  to view the Spark UI (jobs and stages) \n",
    "\n",
    "ui_port = spark._repr_html_().split(\".internal:\")[-1].split('\"')[0]\n",
    "print(\"Copy the following command (and swap out the ZONE for your cluster zone) to cloud shell and run as shown in the screenshot below. \\nAnd click on the PREVIEW ON PORT 8080 menu option .\")\n",
    "f'gcloud compute ssh w261-m --zone {ZONE}-b --ssh-flag \"-L 8080:localhost:{ui_port}\"' \n",
    "# expect 'gcloud compute ssh w261-m --zone us-central1-b --ssh-flag \"-L 8080:localhost:37649\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='https://www.dropbox.com/s/tlb4uiakj2bx7qg/Three_steps_launch_SparkUI.png?raw=true' style='width:100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5cf6e6b5-5993-4e10-9da2-47429244cfda",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 1: Spark Basics.\n",
    "In your readings and live session demos for weeks 4 and 5 you got a crash course in working with Spark. We also talked about how Spark RDDs fit into the broader picture of distributed algorithm design. The questions below cover key points from these discussions. Answer each one very briefly - 2 to 3 sentences.\n",
    "\n",
    "### Q1 Tasks:\n",
    "\n",
    "* __a) short response:__ What is Spark? How  does it relate to Hadoop MapReduce?\n",
    "\n",
    "* __b) short response:__ In what ways does Spark follow the principles of statelessness (a.k.a. functional programming)? List at least one way in which it allows the programmer to depart from this principle. \n",
    "\n",
    "* __c) short response:__ In the context of Spark what is a 'DAG' and how does it relate to the difference between an 'action' and a 'transformation'? Why is it useful to pay attention to the DAG that underlies your Spark implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d8876d5d-05fe-47a5-b61b-a251ee4ebc6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q1 Student Answers:\n",
    "> __a)__ Type your answer here!\n",
    "\n",
    "> __b)__ Type your answer here!\n",
    "\n",
    "> __c)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c2542400-9725-4996-94c2-313025d682ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 2: Similarity Metrics\n",
    "See Canvas for Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [8 8 6 2 8 7 2 1 5 4]\n",
      "B: [4 5 7 3 6 4 3 7 6 1]\n",
      "A.B: 243\n",
      "A.B: 243\n",
      "cosine_sim(A, B): 2.11e+02\n"
     ]
    }
   ],
   "source": [
    "# part a\n",
    "import numpy as np\n",
    "# The points below have been selected to demonstrate the case for Cosine similarity\n",
    "# Cosine similarity\n",
    "np.random.seed(2)\n",
    "A = np.random.randint(low=0, high=10, size=10, dtype=int)\n",
    "B = np.random.randint(low=0, high=10, size=10, dtype=int)\n",
    "print(f\"A: {A}\")\n",
    "print(f\"B: {B}\")\n",
    "print(f\"A.B: {np.dot(A, B)}\") \n",
    "print(f\"A.B: {A @ B}\") \n",
    "#Cosine similarity\n",
    "cosine_sim = A @ B / np.linalg.norm(A) * np.linalg.norm(B)\n",
    "print(f\"cosine_sim(A, B): {cosine_sim:.3}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [0 1 1 0 0 1 0 1 0 1]\n",
      "B: [0 1 1 1 1 1 1 1 0 0]\n",
      "A intersection B: 4\n",
      "jaccard_sim(A, B): 0.5\n"
     ]
    }
   ],
   "source": [
    "# part b\n",
    "import numpy as np\n",
    "# The points below have been selected to demonstrate the case for Jaccard similarity\n",
    "#Jaccard similarity\n",
    "np.random.seed(2)\n",
    "A = np.random.randint(low=0, high=2, size=10, dtype=int)\n",
    "B = np.random.randint(low=0, high=2, size=10, dtype=int)\n",
    "print(f\"A: {A}\")\n",
    "print(f\"B: {B}\")\n",
    "print(f\"A intersection B: {np.count_nonzero(np.logical_and(A, B))}\")\n",
    "jaccard_sim = np.count_nonzero(np.logical_and(A,B)) / np.count_nonzero(np.logical_or(A,B))  # hint: np.logical_or\n",
    "print(f\"jaccard_sim(A, B): {jaccard_sim:.3}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [0 1 1 0 0 1 0 1 0 1]\n",
      "B: [0 1 1 1 1 1 1 1 0 0]\n",
      "A intersection B: 4\n",
      "overlap_sim(A, B): 0.4\n"
     ]
    }
   ],
   "source": [
    "# part c\n",
    "import numpy as np\n",
    "# The points below have been selected to demonstrate the case for Overlap similarity\n",
    "# Overlap similarity\n",
    "np.random.seed(2)\n",
    "A = np.random.randint(low=0, high=2, size=10, dtype=int)\n",
    "B = np.random.randint(low=0, high=2, size=10, dtype=int)\n",
    "print(f\"A: {A}\")\n",
    "print(f\"B: {B}\")\n",
    "print(f\"A intersection B: {np.count_nonzero(np.logical_and(A, B))}\")\n",
    "overlap_sim = np.count_nonzero(np.logical_and(A, B)) / min(len(A), len(B))  # hint: min length\n",
    "print(f\"overlap_sim(A, B): {overlap_sim:.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [0 1 1 0 0 1 0 1 0 1]\n",
      "B: [0 1 1 1 1 1 1 1 0 0]\n",
      "A intersection B: 4\n",
      "dice_sim(A, B): 0.4\n"
     ]
    }
   ],
   "source": [
    "# part d\n",
    "import numpy as np\n",
    "# The points below have been selected to demonstrate the case for Dice similarity\n",
    "# Dice similarity\n",
    "np.random.seed(2)\n",
    "A = np.random.randint(low=0, high=2, size=10, dtype=int)\n",
    "B = np.random.randint(low=0, high=2, size=10, dtype=int)\n",
    "print(f\"A: {A}\")\n",
    "print(f\"B: {B}\")\n",
    "print(f\"A intersection B: {np.count_nonzero(np.logical_and(A, B))}\")\n",
    "dice_sim = (2 * np.count_nonzero(np.logical_and(A, B))) / (len(A) + len(B)) # hint: total non zero elements between vectors\n",
    "print(f\"dice_sim(A, B): {dice_sim:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9b5fd240-e9d2-44a2-8fa8-9c6adfa5df05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 3: Synonym Detection Strategy\n",
    "\n",
    "In the Synonym Detection task we want to compare the meaning of words, not documents. For clarity, lets call the words whose meaning we want to compare `terms`. If only we had a 'meaning document' for each `term` then we could easily use the document similarity strategy from Question 2 to figure out which `terms` have similar meaning (i.e. are 'synonyms'). Of course in order for that to work we'd have to reasonably believe that the words in these 'meaning documents' really do reflect the meaning of the `term`. For a good analysis we'd also need these 'meaning documents' to be fairly long -- the one or two sentence dictionary definition of a term isn't going to provide enough signal to distinguish between thousands and thousands of `term` meanings.\n",
    "\n",
    "This is where the idea of co-occurrence comes in. Just like DocSim makes the assumption that words in a document tell us about the document's meaning, we're going to assume that the set of words that 'co-occur' within a small window around our term can tell us some thing about the meaning of that `term`. Remember that we're going to make this 'co-words' list (a.k.a. 'stripe') by looking at a large body of text. This stripe is our 'meaning document' in that it reflects all the kinds of situations in which our `term` gets used in real language. So another way to phrase our assumption is: we think `terms` that get used to complete lots of the same phrases probably have related meanings. This may seem like an odd assumption but computational linguists have found that it works surprisingly well in practice. Let's look at a toy example to build your intuition for why and how.\n",
    "\n",
    "Consider the opening line of Charles Dickens' _A Tale of Two Cities_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b7504e66-ef09-472a-909b-bea5b1f5e393",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "corpus = \"\"\"It was the best of times, it was the worst of times, \n",
    "it was the age of wisdom it was the age of foolishness\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7676fba9-0e45-4f63-876d-89132d7b25c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are a total of 10 unique words in this short 'corpus':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "520d6d4c-336a-4df2-b518-a1e48c00bc98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foolishness', 'wisdom', 'it', 'age', 'the', 'best', 'was', 'of', 'worst', 'times']\n"
     ]
    }
   ],
   "source": [
    "words = list(set(re.findall(r'\\w+', corpus.lower())))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b095aa02-95a8-4ac1-9a1c-b7b6c985916a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "But of these 10 words, 4 are so common that they probably don't tell us very much about meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fec9f761-d5db-4c48-90cc-85303e7e3335",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stopwords = [\"it\", \"the\", \"was\", \"of\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d5a85035-6a79-476a-807b-cbcbdb71579e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "So we'll ignore these 'stop words' and we're left with a 6 word vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "117ecf80-a449-4cc5-9fa6-c5f5128a4daa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'best', 'foolishness', 'times', 'wisdom', 'worst']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted([w for w in words if w not in stopwords])\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b833d4bf-fe53-4ca1-bf82-733995aed493",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Your goal in the tasks below is to assess which of these six words are most related to each other in meaning -- based solely on this short two line body of text.\n",
    "\n",
    "### Q3 Tasks:\n",
    "\n",
    "* __a) multiple choice:__ Given this six word vocabulary, how many 'pairs' of words do we want to compare? \n",
    "\n",
    "* __b) multiple choice:__ More generally for a n-word vocabulary how many pairwise comparisons are there to make?\n",
    "\n",
    "* __c) code:__ In the space provided below, create a 'stripe' for each `term` in the vocabulary. This stripe should be the list of all other vocabulary words that occur within a __5 word window__ (two words on either side) of the `term`'s position in the original text (In this exercise, use ['it', 'was', 'the','of'] as stopwords, just ignore them from your 5 word vectors).\n",
    "\n",
    "* __d) multiple choice:__ Run the provided code to turn your stripes into a 1-hot encoded co-occurrence matrix. For our 6 word vocabulary how many entries are in this matrix? How many entries are zeroes? \n",
    "\n",
    "* __e) code:__ Complete the provided code to loop over all pairs and compute their cosine similarity. Please do not modify the existing code, just add your own in the spot marked.\n",
    "\n",
    "* __f) multiple choice:__ Which pairs of words have the highest 'similarity' scores? \n",
    "\n",
    "* __g) short response:__ Are these words 'synonyms' in the traditional sense? In what sense are their meanings 'similar'? Explain how our results are contingent on the input text. What would change if we had a much larger corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "06d178ec-c8bf-42aa-8d67-a09ba61187f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q3 Student Answers:\n",
    "> __a)__ Type your answer here!\n",
    "\n",
    "> __b)__ n factorial comparisons to make. \n",
    "\n",
    "> __d)__ 36 entries, of which 28 are zeroes\n",
    "\n",
    "> __f)__ Type your answer here! \n",
    "\n",
    "> __g)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "429c15b1-d1bd-4b04-a6e7-175e336a1907",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS:\n",
      "It was the best of times, it was the worst of times, \n",
      "it was the age of wisdom it was the age of foolishness\n",
      "VOCAB:\n",
      "['age', 'best', 'foolishness', 'times', 'wisdom', 'worst']\n"
     ]
    }
   ],
   "source": [
    "# for convenience, here are the corpus & vocab list again (RUN THIS CELL AS IS)\n",
    "print(\"CORPUS:\")\n",
    "print(corpus)\n",
    "print('VOCAB:')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "482cae9e-bee6-48ab-97e4-175f993e0834",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src='https://github.com/kyleiwaniec/w261_assets/blob/master/images/best-of-times.png?raw=true' style='width:80%'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "72c12700-cffd-4ef4-b408-f5809acc6671",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part c - USE THE TEXT ABOVE TO COMPLETE EACH STRIPE\n",
    "# Stopwords: \n",
    "#     ['it', 'was', 'the', 'of'] \n",
    "# Hint:\n",
    "#     In provided sentence, age appears in two 5 word vectors: ['was', 'the', 'age', 'of', 'wisdom'] and ['was', 'the', 'age', 'of', 'foolishness']\n",
    "#     After removing stopwords, the remaining words are 'wisdom' and 'foolishness'\n",
    "#\n",
    "#     You finish the rest of the non-stopwords below. \n",
    "\n",
    "stripes = {'age':['wisdom','foolishness'], # example\n",
    "           'best':['times'], # YOU FILL IN THE REST\n",
    "           'foolishness':['age'],\n",
    "           'times': ['best','worst'],\n",
    "           'wisdom':['age'],\n",
    "           'worst':['times']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "38f4b1a4-3e91-4976-89d3-07b4942215bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - initializing an empty co-occurrence matrix (RUN THIS CELL AS IS)\n",
    "co_matrix = pd.DataFrame({term: [0]*len(vocab) for term in vocab}, index = vocab, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bac747a0-a1e3-4795-aa2c-c010b10ab979",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>best</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>times</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foolishness</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>times</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wisdom</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             age  best  foolishness  times  wisdom  worst\n",
       "age            0     0            1      0       1      0\n",
       "best           0     0            0      1       0      0\n",
       "foolishness    1     0            0      0       0      0\n",
       "times          0     1            0      0       0      1\n",
       "wisdom         1     0            0      0       0      0\n",
       "worst          0     0            0      1       0      0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part d - this cell 1-hot encodes the co-occurrence matrix (RUN THIS CELL AS IS) \n",
    "for term, nbrs in stripes.items():\n",
    "    pass\n",
    "    for nbr in nbrs:\n",
    "        co_matrix.loc[term, nbr] = 1\n",
    "co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = A @ B / np.linalg.norm(A) * np.linalg.norm(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age            0\n",
       "best           0\n",
       "foolishness    1\n",
       "times          0\n",
       "wisdom         1\n",
       "worst          0\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_matrix['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9b6cf0e3-9003-4d88-aa65-68d0a7b693e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age-best: 0.0\n",
      "age-foolishness: 0.0\n",
      "age-times: 0.0\n",
      "age-wisdom: 0.0\n",
      "age-worst: 0.0\n",
      "best-foolishness: 0.0\n",
      "best-times: 0.0\n",
      "best-wisdom: 0.0\n",
      "best-worst: 1.0\n",
      "foolishness-times: 0.0\n",
      "foolishness-wisdom: 1.0\n",
      "foolishness-worst: 0.0\n",
      "times-wisdom: 0.0\n",
      "times-worst: 0.0\n",
      "wisdom-worst: 0.0\n"
     ]
    }
   ],
   "source": [
    "# part e - FILL IN THE MISSING LINES to compute the cosine similarity between each pair of terms\n",
    "for term1, term2 in itertools.combinations(vocab, 2):\n",
    "    # one hot-encoded vectors\n",
    "    v1 = co_matrix[term1]\n",
    "    v2 = co_matrix[term2]\n",
    "    \n",
    "    # cosine similarity\n",
    "    ############# YOUR CODE HERE #################\n",
    "    csim = v1 @ v2 / np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "    ############# (END) YOUR CODE #################    \n",
    "    \n",
    "    print(f\"{term1}-{term2}: {csim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3e757c6f-10ef-487c-bd29-4f0bd0c93aa7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 4: Pairs and Stripes at Scale\n",
    "\n",
    "As you read in the paper by [Zadeh et al](http://stanford.edu/~rezab/papers/disco.pdf), the advantage of metrics like Cosine, Dice, Overlap and Jaccard is that they are dimension independent -- that is to say, if we implement them in a smart way the computational complexity of performing these computations is independent of the number of documents we want to compare (or in our case, the number of terms that are potential synonyms). One component of a 'smart implementation' involves thinking carefully both about how you define the \"basis vocabulary\" that forms your feature set (removing stopwords, etc). Another key idea is to use a data structure that facilitates distributed calculations. The DISCO (Dimension Independent Similarity Computation) implementation further uses a sampling strategy, but that is beyond the scope of this assignment. \n",
    "\n",
    "In this question we'll take a closer look at the computational complexity of the synonym detection approach we took in question 3 and then revisit the document similarity example as a way to explore a more efficient approach to parallelizing this analysis.\n",
    "\n",
    "### Q4 Tasks:\n",
    "\n",
    "* __a) multiple choice (choose 2):__ In question 3 you calculated the cosine similarity of pairs of words using the vector representation of their co-occurrences in a corpus. In the async videos about \"Pairs and Stripes\" you were introduced to an alternative strategy. Explain two ways that using these data structures are more efficient than 1-hot encoded vectors when it comes to distributed similarity calculations [__`HINT:`__ _Consider memory constraints, amount of information being shuffled, amount of information being transfered over the network, and level of parallelization._]\n",
    "\n",
    "* __b) read provided code:__ The code below provides a streamined implementation of Document similarity analysis in Spark. Read through this code carefully. Once you are confident you understand how it works, answer the remaining questions. [__`TIP:`__ _to see the output of each transformation try commenting out the subsequent lines and adding an early `collect()` action_.]\n",
    "\n",
    "* __c) short response:__ The second mapper function, `splitWords`, emits 'postings'. The list of all 'postings' for a word is also refered to as an 'inverted index'. In your own words, define each of these terms ('postings' and 'inverted index') based on your reading of the provided code. (*DITP by Lin and Dyer also contains a chapter on the Inverted Index although in the context of Hadoop rather than Spark. You may find the illustration in Chaprter 4 helpful in answering this question*).\n",
    "\n",
    "* __d) short response:__ The third mapper, `makeCompositeKeys`, loops over the inverted index to emit 'pairs' of what? Explain what information is included in the composite key created at this stage and why it makes sense to synchronize around that information in the context of performing document similarity calculations. In addition to the information included in these new keys, what other piece of information will we need to compute Jaccard or Cosine similarity?\n",
    "\n",
    "* __e) multiple choice:__ Out of all the Spark transformations we make in this analysis, which are 'wide' transformations and which are 'narrow' transformations, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A.B = (A \\cap B)$$, and $$||A|| = \\sqrt{|A|}$$, similarly you could calculate ||B|| from |B|. Therefore, $$cosine~similartiy = \\frac{(A \\cap B)}{\\sqrt{|A|} \\cdot \\sqrt{|B|}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "244c0e4a-7043-4bc1-a64f-231db9211ea1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q4 Student Answers:\n",
    "\n",
    "> __a)__  \n",
    "Stripes are efficient because they can be stored as Python dicts, which are very efficient data structures.\n",
    "Stripes allow us to track the same information as full vectors/matrices with less memory overhead because we don't encode zeros.\n",
    "? Pairs allow us to compute counts needed for the DISCO metrics without doing an explicit vector computation.\n",
    "\n",
    "\n",
    "> __b)__ _read provided code before answering d-e_ \n",
    "\n",
    "> __c)__ An inverted index is a method of storing counts by term in dictionary form. The term is the key value, and they values for each key are the counts per document that the term appears in. These values are called postings, and act as a dictionary within a dictionary.  The keys are the documents that the term appears in, and the values for each of these keys are the number of times the term appears within that document.\n",
    "\n",
    "> __d)__ The mapper makeCompositeKeys emits pairs of possible document combinations that a word can be in. For example, one key can be for terms that appear in both document A and document B.  It makes sense to synchronize around this information since these term intersections can be used within certain similarity metrics that leverage unions of terms between documents. For Jaccard similarity, the only piece of information that is missing is the total count of documents that a term appears in. For cosine similarity, the missing piece of information is the norm of each term.\n",
    "\n",
    "> __e)__ The map() and three flatMap()s are wide because they can operate on any part of the data regardless of partitioning. The reduceByKey()s are narrow transformations, because they \"narrow\" the data by aggregating it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "02bbeeeb-2c83-40ad-b648-79b76f2603cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A small test file that we put in a folder on your private data bucket : __`sample_docs.txt`__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c4aa7016-5fff-4b11-b615-4cff0007f244",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying from <STDIN>...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "sample_doc=\"\"\"docA\tbright blue butterfly forget\n",
    "docB\tbest forget bright sky\n",
    "docC\tblue sky bright sun\n",
    "docD\tunder butterfly sky hangs\n",
    "docE\tforget blue butterfly\"\"\"\n",
    "\n",
    "sample_docs_loc = f'{HW3_FOLDER}/sample_docs.txt'\n",
    "\n",
    "!echo \"{sample_doc}\" | gsutil cp - {sample_docs_loc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       145  2022-06-16T00:43:17Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/sample_docs.txt\n",
      "       494  2022-06-15T03:25:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/systems_test.txt\n",
      "                                 gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/\n",
      "TOTAL: 2 objects, 639 bytes (639 B)\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -l {HW3_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c738aa14-92f0-4727-bd60-0c0f5326ad37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docA\tbright blue butterfly forget\n",
      "docB\tbest forget bright sky\n",
      "docC\tblue sky bright sun\n",
      "docD\tunder butterfly sky hangs\n",
      "docE\tforget blue butterfly\n"
     ]
    }
   ],
   "source": [
    "# load data - RUN THIS CELL AS IS\n",
    "!gsutil cat {sample_docs_loc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "36e04f30-d4ee-484f-a6f4-5568a1698a49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__Document Similarity Analysis in Spark:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da504044-0c75-4763-adad-2d8e2817265b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load data - RUN THIS CELL AS IS\n",
    "data = sc.textFile(sample_docs_loc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "13bca2b5-8996-4018-b59b-3db3599f14fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# helper function - RUN THIS CELL AS IS\n",
    "def splitWords(pair):\n",
    "    \"\"\"Mapper 2: tokenize each document and emit postings.\"\"\"\n",
    "    doc, text = pair\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        yield (w, [(doc,len(words))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "68200122-deb9-434b-84b1-a1a130566b68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# helper function - RUN THIS CELL AS IS\n",
    "def makeCompositeKey(inverted_index):\n",
    "    \"\"\"Mapper 3: loop over postings and yield pairs.\"\"\"\n",
    "    word, postings = inverted_index\n",
    "    # taking advantage of symmetry, output only (a,b), but not (b,a)\n",
    "    for subset in itertools.combinations(sorted(postings), 2):\n",
    "        yield (str(subset), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "34b72a6c-cab1-46ea-b50b-00ce7fa49cc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# helper function - RUN THIS CELL AS IS\n",
    "def jaccard(line):\n",
    "    \"\"\"Mapper 4: compute similarity scores\"\"\"\n",
    "    (doc1, n1), (doc2, n2) = ast.literal_eval(line[0])\n",
    "    total = int(line[1])\n",
    "    jaccard = total / float(int(n1) + int(n2) - total)\n",
    "    yield doc1+\" - \"+doc2, jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ignore latency warnings\n",
    "NOTE: do NOT be alarmed if you see some latency warnings when you run the following command. This can happen when you are accessing your cloud bucket for the first time. Spark is super EAGER! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1a706f1c-bbb2-4f45-af1c-9ebee2fbea0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"(('docA', 4), ('docB', 4))\", 1),\n",
       " (\"(('docA', 4), ('docC', 4))\", 1),\n",
       " (\"(('docB', 4), ('docC', 4))\", 1),\n",
       " (\"(('docA', 4), ('docD', 4))\", 1),\n",
       " (\"(('docA', 4), ('docE', 3))\", 1),\n",
       " (\"(('docD', 4), ('docE', 3))\", 1),\n",
       " (\"(('docB', 4), ('docC', 4))\", 1),\n",
       " (\"(('docB', 4), ('docD', 4))\", 1),\n",
       " (\"(('docC', 4), ('docD', 4))\", 1),\n",
       " (\"(('docA', 4), ('docC', 4))\", 1),\n",
       " (\"(('docA', 4), ('docE', 3))\", 1),\n",
       " (\"(('docC', 4), ('docE', 3))\", 1),\n",
       " (\"(('docA', 4), ('docB', 4))\", 1),\n",
       " (\"(('docA', 4), ('docE', 3))\", 1),\n",
       " (\"(('docB', 4), ('docE', 3))\", 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Job - RUN THIS CELL AS IS\n",
    "result = data.map(lambda line: line.split('\\t')) \\\n",
    "             .flatMap(splitWords) \\\n",
    "             .reduceByKey(lambda x,y : x+y) \\\n",
    "             .flatMap(makeCompositeKey) \\\n",
    "             .collect()\n",
    "#              .reduceByKey(lambda x,y : x+y) \\\n",
    "#              .flatMap(jaccard) \\\n",
    "#              .takeOrdered(10, key=lambda x: -x[1])\n",
    "result \n",
    "#NOTE: do NOT be alarmed if you see some latency warnings here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6a9f78ca-93fe-46ba-8d26-fa7a8adc6710",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# About the Data\n",
    "Now that you are comfortable with similarity metrics we turn to the main task in this assignment: \"Synonym\" Detection. As you saw in Question 3 the ability of our algorithm to detect words with similar meanings is highly dependent on our input text. Specifically, we need a large enough corpus of natural language that we can expose our algorithm to a realistic range of contexts in which any given word might get used. Ideally, these 'contexts' would also provide enough signal to distinguish between words with similar semantic roles but different meaning. Finding such a corpus will be easier to accomplish for some words than others.\n",
    "\n",
    "For the main task in this portion of the homework you will use data from Google's n-gram corpus. This data is particularly convenient for our task because Google has already done the first step for us: they windowed over a large subset of the web and extracted all 5-grams. If you are interested in learning more about this dataset the original source is: http://books.google.com/ngrams/, and a large subset is available [here from AWS](https://aws.amazon.com/datasets/google-books-ngrams/). \n",
    "\n",
    "For this assignment we have provided a subset of the 5-grams data consisting of 191 files of approximately 10MB each. These files are available in the 'data' folder in 'Assignments/HW3/' github. Please only use the provided data so that we can ensure consistent results from student to student.\n",
    "\n",
    "Each row in our dataset represents one of these 5 grams in the format:\n",
    "> `(ngram) \\t (count) \\t (pages_count) \\t (books_count)`\n",
    "\n",
    "__DISCLAIMER__: In real life, we would calculate the stripes cooccurrence data from the raw text by windowing over the raw text and not from the 5-gram preprocessed data.  Calculating pairs on this 5-gram is a little corrupt as we will be double counting cooccurences. Having said that this exercise can still pull out some similar terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.66 MiB  2022-05-31T04:38:39Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-0-filtered.txt.gz\n",
      "      70 B  2022-05-31T04:38:39Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-1-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:39Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-10-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:39Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-100-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:39Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-101-filtered.txt.gz\n",
      "  3.65 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-102-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-103-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-104-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-105-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-106-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-107-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-108-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-109-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-11-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-110-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:39Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-111-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:39Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-112-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:39Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-113-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:39Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-114-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:39Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-115-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-116-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-117-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-118-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-119-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-12-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-120-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-121-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-122-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-123-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-124-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-125-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-126-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-127-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-128-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-129-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-13-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-130-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-131-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-132-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-133-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-134-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-135-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-136-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-137-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-138-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-139-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-14-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-140-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-141-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-142-filtered.txt.gz\n",
      "  3.69 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-143-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-144-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-145-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-146-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-147-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-148-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-149-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-15-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-150-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-151-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-152-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-153-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-154-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-155-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-156-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-157-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-158-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-159-filtered.txt.gz\n",
      "  3.69 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-16-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-160-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-161-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-162-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:40Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-163-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-164-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-165-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-166-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-167-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-168-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-169-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-17-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-170-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-171-filtered.txt.gz\n",
      "  3.65 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-172-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-173-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-174-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-175-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-176-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-177-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-178-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-179-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-18-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-180-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-181-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-182-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-183-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-184-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-185-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-186-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-187-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-188-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-189-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-19-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-2-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-20-filtered.txt.gz\n",
      "  3.65 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-21-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-22-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-23-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-24-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-25-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-26-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-27-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-28-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-29-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-3-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-30-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-31-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-32-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-33-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-34-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-35-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-36-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-37-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-38-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-39-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-4-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-40-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-41-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-42-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-43-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-44-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-45-filtered.txt.gz\n",
      "      71 B  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-46-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-47-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-48-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:41Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-49-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-5-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-50-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-51-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-52-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-53-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-54-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-55-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-56-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-57-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-58-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-59-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-6-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-60-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-61-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-62-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-63-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-64-filtered.txt.gz\n",
      "  3.69 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-65-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-66-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-67-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-68-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-69-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-7-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-70-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-71-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-72-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-73-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-74-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-75-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-76-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-77-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-78-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-79-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-8-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-80-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-81-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-82-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-83-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-84-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-85-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-86-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-87-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-88-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-89-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-9-filtered.txt.gz\n",
      "  3.68 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-90-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-91-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-92-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-93-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-94-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-95-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-96-filtered.txt.gz\n",
      "  3.66 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-97-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-98-filtered.txt.gz\n",
      "  3.67 MiB  2022-05-31T04:38:42Z  gs://hw_data_bucket/main/Assignments/HW3/docker/student/data/googlebooks-eng-all-5gram-20090715-99-filtered.txt.gz\n",
      "TOTAL: 190 objects, 723277880 bytes (689.77 MiB)\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -lh {HW3_FOLDER}/data  #The data is preloaded into your personal data bucket (all 190 zipped files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "67ee798a-99fd-4ada-b8b1-4bfa6d032335",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set global paths to full data folder and to the first file (which we'll use for testing)\n",
    "NGRAMS  = f'{HW3_FOLDER}/data'\n",
    "F1_PATH = f'{HW3_FOLDER}/data/googlebooks-eng-all-5gram-20090715-0-filtered.txt.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d36301d1-c5fd-46c7-91a5-4745bb175741",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As you develop your code you should use the following file to systems test each of your solutions before running it on the Google data. (Note: these are the 5-grams extracted from our two line Dickens corpus in Question 3... you should find that your Spark job results match the calculations we did \"by hand\").\n",
    "\n",
    "Test file: __`systems_test.txt`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ed5ebc27-0381-4fe6-901b-fe4fc4f5a10d",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying from <STDIN>...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "systems_test= \"\"\"it was the best of\t1\t1\t1\n",
    "age of wisdom it was\t1\t1\t1\n",
    "best of times it was\t1\t1\t1\n",
    "it was the age of\t2\t1\t1\n",
    "it was the worst of\t1\t1\t1\n",
    "of times it was the\t2\t1\t1\n",
    "of wisdom it was the\t1\t1\t1\n",
    "the age of wisdom it\t1\t1\t1\n",
    "the best of times it\t1\t1\t1\n",
    "the worst of times it\t1\t1\t1\n",
    "times it was the age\t1\t1\t1\n",
    "times it was the worst\t1\t1\t1\n",
    "was the age of wisdom\t1\t1\t1\n",
    "was the best of times\t1\t1\t1\n",
    "was the age of foolishness\t1\t1\t1\n",
    "was the worst of times\t1\t1\t1\n",
    "wisdom it was the age\t1\t1\t1\n",
    "worst of times it was\t1\t1\t1\"\"\"\n",
    "\n",
    "\n",
    "systems_test_loc = f'{HW3_FOLDER}/systems_test.txt'\n",
    "\n",
    "!echo \"{systems_test}\" | gsutil cp - {systems_test_loc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ac26da79-b410-4a8c-9d5d-6c074d48769d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, we'll create a Spark RDD for each of these files so that they're easy to access throughout the rest of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e162747-4cf6-4f36-89ca-a1214512f5c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spark RDDs for each dataset\n",
    "testRDD = sc.textFile(systems_test_loc) \n",
    "f1RDD = sc.textFile(F1_PATH)\n",
    "dataRDD = sc.textFile(NGRAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "70ae8e81-27de-4975-8b4c-22bbb82c6be5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's take a peek at what each of these RDDs looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6382f9a6-0b3d-4cb5-be31-f4ac27814306",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it was the best of\\t1\\t1\\t1',\n",
       " 'age of wisdom it was\\t1\\t1\\t1',\n",
       " 'best of times it was\\t1\\t1\\t1',\n",
       " 'it was the age of\\t2\\t1\\t1',\n",
       " 'it was the worst of\\t1\\t1\\t1',\n",
       " 'of times it was the\\t2\\t1\\t1',\n",
       " 'of wisdom it was the\\t1\\t1\\t1',\n",
       " 'the age of wisdom it\\t1\\t1\\t1',\n",
       " 'the best of times it\\t1\\t1\\t1',\n",
       " 'the worst of times it\\t1\\t1\\t1']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7a94a009-6a28-4c71-9cfd-57fc3f0469e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A BILL FOR ESTABLISHING RELIGIOUS\\t59\\t59\\t54',\n",
       " 'A Biography of General George\\t92\\t90\\t74',\n",
       " 'A Case Study in Government\\t102\\t102\\t78',\n",
       " 'A Case Study of Female\\t447\\t447\\t327',\n",
       " 'A Case Study of Limited\\t55\\t55\\t43',\n",
       " \"A Child's Christmas in Wales\\t1099\\t1061\\t866\",\n",
       " 'A Circumstantial Narrative of the\\t62\\t62\\t50',\n",
       " 'A City by the Sea\\t62\\t60\\t49',\n",
       " 'A Collection of Fairy Tales\\t123\\t117\\t80',\n",
       " 'A Collection of Forms of\\t116\\t103\\t82']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1RDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3821fa08-6fbd-46dd-9233-92ae8dad4e7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/16 00:43:28 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A BILL FOR ESTABLISHING RELIGIOUS\\t59\\t59\\t54',\n",
       " 'A Biography of General George\\t92\\t90\\t74',\n",
       " 'A Case Study in Government\\t102\\t102\\t78',\n",
       " 'A Case Study of Female\\t447\\t447\\t327',\n",
       " 'A Case Study of Limited\\t55\\t55\\t43',\n",
       " \"A Child's Christmas in Wales\\t1099\\t1061\\t866\",\n",
       " 'A Circumstantial Narrative of the\\t62\\t62\\t50',\n",
       " 'A City by the Sea\\t62\\t60\\t49',\n",
       " 'A Collection of Fairy Tales\\t123\\t117\\t80',\n",
       " 'A Collection of Forms of\\t116\\t103\\t82']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.a) \n",
    "\n",
    "Write a Spark application to retrieve:\n",
    "\n",
    "The number of unique words that appear in the data. (i.e. size of the vocabulary)\n",
    "A list of the top 10 words & their counts.\n",
    "A list of the bottom 10 words & their counts.\n",
    "NOTE 1: don't forget to lower case the ngrams before extracting words.\n",
    "NOTE 2: don't forget to take in to account the number of occurances (count) of each ngram.\n",
    "NOTE 3: to make this code more reusable, the EDA1 function code base uses a parameter 'n' to specify the number of top/bottom words to print (in this case we've requested 10).\n",
    "\n",
    "In the cell below, please, paste the code you wrote between the delimiters:\n",
    "\n",
    "### Q5.b) \n",
    "What is the size of the vocabulary?\n",
    "### Q5.c) \n",
    "Given the vocabulary size you just found, how many potential synonym pairs could we form from this corpus?\n",
    "> 10 factorial\n",
    "### Q5.d)  \n",
    "If each term's stripe were 1000 words long, how many tuples would we need to shuffle in order to form the inverted indices?\n",
    "> 1000 factorial?\n",
    "### Q5.e) \n",
    "Show and briefly explain your calculations for each part of this question.  Please, use the Equation Editor to better present your calculations.\n",
    "### Q5.f) \n",
    "Looking at the most frequent words and their counts, how useful will these top words be in synonym detection?\n",
    "### Q5.g) \n",
    "Looking at the least frequent words and their counts, what is a possible explanation of their low counts?\n",
    "### Q5.h) \n",
    "How reliable should we expect the detected 'synonyms' for the least frequent words to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2bec439f-08e0-45b9-a249-6e57064f627d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 5: N-gram EDA part 1 (words)\n",
    "\n",
    "Before starting our synonym-detection, let's get a sense for this data. As you saw in questions 3 and 4 the size of the vocabulary will impact the amount of computation we have to do. Write a Spark job that will accomplish the three tasks below as efficiently as possible. (No credit will be awarded for jobs that sort or subset after calling `collect()`-- use the framework to get the minimum information requested). As you develop your code, systems test each job on the provided file with Dickens ngrams, then on a single file from the Ngram dataset before running the full analysis.\n",
    "\n",
    "\n",
    "### Q5 Tasks:\n",
    "* __a) code:__ Write a Spark application to retrieve:\n",
    "  * The number of unique words that appear in the data. (i.e. size of the vocabulary) \n",
    "  * A list of the top 10 words & their counts.\n",
    "  * A list of the bottom 10 words & their counts.  \n",
    "  \n",
    "  __`NOTE  1:`__ _don't forget to lower case the ngrams before extracting words._  \n",
    "  __`NOTE  2:`__ _don't forget to take in to account the number of occurances (count) of each ngram._  \n",
    "  __`NOTE  3:`__ _to make this code more reusable, the `EDA1` function code base uses a parameter 'n' to specify the number of top/bottom words to print (in this case we've requested 10)._\n",
    "\n",
    "* __b) numerical:__ What is the size of the vocabulary?\n",
    "\n",
    "* __c) numerical:__ Given the vocab size you just found, how many potential synonym pairs could we form from this corpus? \n",
    "\n",
    "* __d) numerical:__ If each term's stripe were 1000 words long, how many tuples would we need to shuffle in order to form the inverted indices? \n",
    "\n",
    "* __e) short response:__ Show and briefly explain your calculations for each part of this question. [__`HINT:`__ see your work from q4 for a review of these concepts.]\n",
    "\n",
    "* __f) multiple choice:__ Looking at the most frequent words and their counts, how useful will these top words be in synonym detection?\n",
    "\n",
    "* __g) multiple choice:__ Looking at the least frequent words and their counts, what is a possible explanation of their low counts?\n",
    "\n",
    "* __h) multiple choice:__ How reliable should we expect the detected 'synonyms' for the least frequent words to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "551ea292-76ba-40de-920f-e3e528c52b31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q5 Student Answers:\n",
    "\n",
    "> __b)__ Type your answer here!   \n",
    "\n",
    "> __c)__ Type your answer here!   \n",
    "\n",
    "> __d)__ Type your answer here!\n",
    "\n",
    "> __e)__ Type your answer here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "091af7db-444d-4aa7-bbc3-691f728d2840",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part a - write your spark job here \n",
    "def EDA1(rdd, n):\n",
    "    total, top_n, bottom_n = None, None, None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "    def parseDoc(doc):\n",
    "        ngram, count, pages_count, books_count = doc.split('\\t')\n",
    "        words = ngram.lower().split(' ')\n",
    "        return [[word, int(count)] for word in words]\n",
    "    \n",
    "    reduced_rdd = rdd.flatMap(lambda x: parseDoc(x)) \\\n",
    "                     .reduceByKey(lambda x, y: x+y).cache()\n",
    "    \n",
    "    sorted_rdd = reduced_rdd.sortBy(lambda x: x[1], False).collect()\n",
    "    top_n = sorted_rdd[0:n]\n",
    "    bottom_n = sorted_rdd[-n:]\n",
    "    \n",
    "    total = reduced_rdd.keys().count()\n",
    "    \n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return total, top_n, bottom_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4babf25b-06f1-4d63-86d8-a2e853f13e2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.42029595375061035 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\n",
    "import time\n",
    "start = time.time()\n",
    "vocab_size, most_frequent, least_frequent = EDA1(testRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "615f0416-8170-4d20-9eb1-42c5abfbeb64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 10\n",
      " ---- Top Words ----|--- Bottom Words ----\n",
      "     was         17 |            was  17\n",
      "      of         17 |             of  17\n",
      "     the         17 |            the  17\n",
      "      it         16 |             it  16\n",
      "   times         10 |          times  10\n",
      "     age          8 |            age   8\n",
      "   worst          5 |          worst   5\n",
      "  wisdom          5 |         wisdom   5\n",
      "    best          4 |           best   4\n",
      "foolishness          1 |    foolishness   1\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\" ---- Top Words ----|--- Bottom Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_frequent, least_frequent):\n",
    "    print(f\"{w1:>8} {c1:>10} |{w2:>15} {c2:>3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cf050ce9-3cc6-484c-8036-7d380d4ca0dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Expected output for testRDD:\n",
    "<pre>\n",
    "    Vocabulary Size: 10\n",
    " ---- Top Words ----|--- Bottom Words ----\n",
    "     was         17 |    foolishness   1\n",
    "      of         17 |           best   4\n",
    "     the         17 |          worst   5\n",
    "      it         16 |         wisdom   5\n",
    "   times         10 |            age   8\n",
    "     age          8 |          times  10\n",
    "   worst          5 |             it  16\n",
    "  wisdom          5 |            was  17\n",
    "    best          4 |             of  17\n",
    "foolishness       1 |            the  17  \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "61d1bbae-7d1d-483e-b23a-a32fef6e2264",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.157864570617676 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# part a - run a single file, ie., a small sample (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "vocab_size, most_frequent, least_frequent = EDA1(f1RDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "79a6aa9a-4c42-483e-b859-9711a8bbff9f",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 36353\n",
      " ---- Top Words ----|--- Bottom Words ----\n",
      "     the   27691943 |          burra  40\n",
      "      of   18590950 |     improvised  40\n",
      "      to   11601757 |     complacent  40\n",
      "      in    7470912 |    perfectness  40\n",
      "       a    6926743 |superimposition  40\n",
      "     and    6150529 |         woolly  40\n",
      "    that    4077421 |         escaci  40\n",
      "      is    4074864 |       huvieran  40\n",
      "      be    3720812 |    considerate  40\n",
      "     was    2492074 |         zodiac  40\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\" ---- Top Words ----|--- Bottom Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_frequent, least_frequent):\n",
    "    print(f\"{w1:>8} {c1:>10} |{w2:>15} {c2:>3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7e6bd030-552b-4cbb-aafb-955cc5f2575c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Expected output for f1RDD\n",
    "<pre>\n",
    "Vocabulary Size: 36353\n",
    " ---- Top Words ----|--- Bottom Words ----\n",
    "     the   27691943 |    stakeholder  40\n",
    "      of   18590950 |          kenny  40\n",
    "      to   11601757 |         barnes  40\n",
    "      in    7470912 |         arnall  40\n",
    "       a    6926743 |     buonaparte  40\n",
    "     and    6150529 |       puzzling  40\n",
    "    that    4077421 |             hd  40\n",
    "      is    4074864 |        corisca  40\n",
    "      be    3720812 |       cristina  40\n",
    "     was    2492074 |         durban  40\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f82284a5-a9a1-49b4-b8fa-057d6ed6f368",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 59.05569052696228 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - run full analysis (RUN THIS CELL AS IS)\n",
    "\n",
    "start = time.time()\n",
    "vocab_size, most_frequent, least_frequent = EDA1(dataRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "# Command took 4 minute minutes on N1-Std-4 (4 CPUS) as May 30, 2022\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "47c705e3-8578-48ce-82ca-9cf3274ab801",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 269339\n",
      " ---- Top Words ----|--- Bottom Words ----\n",
      "     the 5490815394 |uninhabitability  40\n",
      "      of 3698583299 |       oratores  40\n",
      "      to 2227866570 |        teubert  40\n",
      "      in 1421312776 |       icaruses  40\n",
      "       a 1361123022 |     horniman's  40\n",
      "     and 1149577477 |    wundertater  40\n",
      "    that  802921147 |         busbye  40\n",
      "      is  758328796 |   guancabelica  40\n",
      "      be  688707130 |       belfield  40\n",
      "      as  492170314 |          folha  40\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\" ---- Top Words ----|--- Bottom Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_frequent, least_frequent):\n",
    "    print(f\"{w1:>8} {c1:>10} |{w2:>15} {c2:>3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b9cd59be-634c-4cad-a6a8-209e0c777905",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Expected output for dataRDD:\n",
    "(bottom words might vary a little due to ties)\n",
    "<pre>\n",
    "Vocabulary Size: 269339\n",
    " ---- Top Words ----|--- Bottom Words ----\n",
    "     the 5490815394 |   schwetzingen  40\n",
    "      of 3698583299 |           cras  40\n",
    "      to 2227866570 |       parcival  40\n",
    "      in 1421312776 |          porti  40\n",
    "       a 1361123022 |    scribbler's  40\n",
    "     and 1149577477 |      washermen  40\n",
    "    that  802921147 |    viscerating  40\n",
    "      is  758328796 |         mildes  40\n",
    "      be  688707130 |      scholared  40\n",
    "      as  492170314 |       jaworski  40\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "81a856ad-4475-4912-9de7-fa14d6a73044",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 6: N-gram EDA part 2 (co-occurrences)\n",
    "\n",
    "The computational complexity of synonym analysis depends not only on the number of words, but also on the number of co-ocurrences each word has. In this question you'll take a closer look at that aspect of our data. As before, please test each job on small \"systems test\" (Dickens ngrams) file and on a single file from the Ngram dataset before running the full analysis.\n",
    "\n",
    "### Q6 Tasks:\n",
    "* __a) code:__ Write a spark job that computes:\n",
    "  * the number of unique neighbors (i.e. 5-gram co-occuring words) for each word in the vocabulary. \n",
    "  \n",
    " <pre>\n",
    "  HINT: consider all words within a five-gram to be co-occuring. In other words, a word in a single 5-gram will always have 4 neighbors\n",
    "  EXAMPLE:\n",
    "    the dog ate cat litter \n",
    "    the cat has clean litter \n",
    "    \n",
    "    Vocabulary:\n",
    "    the, dog, ate, litter, cat, has, clean\n",
    "    \n",
    "    Neighbors:\n",
    "    (the, dog) (the, ate) (the, cat) (the, littler), (dog, ate) (dog, cat) (dog, litter), (ate, cat) (ate, litter), (cat, litter)\n",
    "    (the, cat) (the, has) (the, clean) (the, litter), (cat, has) (cat, clean) (cat, litter), (has, clean) (has, litter) (clean, litter)\n",
    "    \n",
    "    Unique neighbors:\n",
    "    the 6\n",
    "    dog 4\n",
    "    ate 4\n",
    "    litter 6\n",
    "    cat 6\n",
    "    has 4\n",
    "    clean 4\n",
    " </pre>\n",
    "    \n",
    "    \n",
    "  * the top 10 words with the most \"neighbors\"\n",
    "  * the bottom 10 words with least \"neighbors\"\n",
    "  * a random sample of 1% of the words' neighbor counts     \n",
    "  __`NOTE:`__ for the last item, please return only the counts and not the words -- we'll go on to use these in a plotting function that expects a list of integers.\n",
    "\n",
    "* __b) short response:__ Use the provided code to plot a histogram of the sampled list from `a`. Comment on the distribution you observe. How will this distribution affect our synonym detection analysis?\n",
    "\n",
    "* __c) code + numerical:__ Write a Spark Job to compare word frequencies to number of neighbors.\n",
    "    * Of the 1000 words with most neighbors, what percent are also in the list of 1000 most frequent words?\n",
    "    * Of the 1000 words with least neighbors, what percent are also in the list of 1000 least frequent words?   \n",
    "[__`NOTE:`__ _technically these lists are short enough to compare in memory on your local machine but please design your Spark job as if we were potentially comparing much larger lists._]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c9910e4-476d-4501-998d-08d7e85444af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q6 Student Answers:\n",
    "\n",
    "> __b)__ Type your answer here!   \n",
    "\n",
    "> __c)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - spark job\n",
    "def EDA2(rdd,n):\n",
    "    top_n, bottom_n, sampled_counts = None, None, None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "\n",
    "    def create_neighbors(doc):\n",
    "        ngram, count, pages_count, books_count = doc.split('\\t')\n",
    "        words = ngram.lower().split(' ')\n",
    "        for combo in itertools.combinations(sorted(words), 2):\n",
    "            yield (combo, int(count))\n",
    "    \n",
    "    result = rdd.flatMap(lambda x: create_neighbors(x))\\\n",
    "                .reduceByKey(lambda x, y: x+y)\\\n",
    "                .cache()\n",
    "    \n",
    "    def neighbor_count(combo):\n",
    "        neighbors, count = combo[0], combo[1]\n",
    "        for word in neighbors:\n",
    "            yield (word, 1)\n",
    "    \n",
    "    result_2 = result.flatMap(lambda x: neighbor_count(x)) \\\n",
    "                 .reduceByKey(lambda x, y: x+y)\\\n",
    "                 .cache()\n",
    "    \n",
    "    sorted_rdd = result_2.sortBy(lambda x: x[1], False)\n",
    "    top_n = sorted_rdd.takeOrdered(n, key=lambda x: -x[1])\n",
    "    bottom_n = sorted_rdd.takeOrdered(n, key=lambda x: x[1])\n",
    "    \n",
    "    sampled_counts = result_2.values().sample(withReplacement=False, fraction=0.01, seed=2018).collect()\n",
    "    \n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return top_n, bottom_n, sampled_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7965afbc-ba04-4482-91c6-369579a412ed",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# part a - spark job\n",
    "def EDA2(rdd,n):\n",
    "    top_n, bottom_n, sampled_counts = None, None, None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "\n",
    "    def create_neighbors(doc):\n",
    "        ngram, count, pages_count, books_count = doc.split('\\t')\n",
    "        words = ngram.split(' ')\n",
    "        for combo in itertools.combinations(sorted(words), 2):\n",
    "            yield (combo, int(count))\n",
    "    \n",
    "    result = rdd.flatMap(lambda x: create_neighbors(x))\\\n",
    "                .reduceByKey(lambda x, y: x+y)\\\n",
    "                .cache()\n",
    "    \n",
    "    def neighbor_count(combo):\n",
    "        neighbors, count = combo[0], combo[1]\n",
    "        for word in neighbors:\n",
    "            yield (word, 1)\n",
    "    \n",
    "    result_2 = result.flatMap(lambda x: neighbor_count(x)) \\\n",
    "                 .reduceByKey(lambda x, y: x+y)\\\n",
    "                 .cache()\n",
    "    \n",
    "    sorted_rdd = result_2.sortBy(lambda x: x[1], False)\n",
    "    top_n = sorted_rdd.takeOrdered(10, key=lambda x: -x[1])\n",
    "    bottom_n = sorted_rdd.takeOrdered(10, key=lambda x: x[1])\n",
    "    sampled_counts = result_2.values().sample(False, 0.01, 2018).collect()\n",
    "    \n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return top_n, bottom_n, sampled_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "28365223-462e-4232-abd6-cfa2bc38af66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.534306526184082 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - systems test (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "most_nbrs, least_nbrs, sample_counts = EDA2(testRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "# Command took 0.83 seconds -- by kyleiwaniec@gmail.com at 12/8/2020, 1:41:59 PM on HW-S21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d387813d-7682-467e-b501-d3332ca207fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Most Co-Words ---|--- Least Co-Words ----\n",
      "          of        9 |     foolishness    4\n",
      "         was        9 |            best    5\n",
      "         the        9 |           worst    5\n",
      "          it        8 |          wisdom    5\n",
      "         age        7 |             age    7\n",
      "       times        7 |           times    7\n",
      "        best        5 |              it    8\n",
      "       worst        5 |              of    9\n",
      "      wisdom        5 |             was    9\n",
      " foolishness        4 |             the    9\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\" --- Most Co-Words ---|--- Least Co-Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_nbrs, least_nbrs):\n",
    "    print(f\"{w1:>12} {c1:>8} |{w2:>16} {c2:>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a002829e-8e36-40ec-bde8-1d7d8a39d104",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Expected output for testRDD:\n",
    "<pre>\n",
    " --- Most Co-Words ---|--- Least Co-Words ----\n",
    "         was        9 |     foolishness    4\n",
    "          of        9 |            best    5\n",
    "         the        9 |           worst    5\n",
    "          it        8 |          wisdom    5\n",
    "         age        7 |             age    7\n",
    "       times        7 |           times    7\n",
    "        best        5 |              it    8\n",
    "       worst        5 |             was    9\n",
    "      wisdom        5 |              of    9\n",
    " foolishness        4 |             the    9\n",
    " </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f23d9344-a019-4985-bb5b-f98a7bf03bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 285:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.796377182006836 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# part a - single file test (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "most_nbrs, least_nbrs, sample_counts = EDA2(f1RDD, 10) #repartiion and use all 4 cores\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "# Command took 13.80 seconds -- DCE\n",
    "\n",
    "#[Stage 40:>                                                         (0 + 1) / 1]\n",
    "#Wall time: 10.146554470062256 seconds\n",
    "\n",
    "#[Stage 50:>                                                         (0 + 4) / 4]\n",
    "# Wall time: 6.513125658035278 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9722b282-a935-48c6-8a10-54a72290f639",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Most Co-Words ---|--- Least Co-Words ----\n",
      "         the    25550 |             phe    2\n",
      "          of    22498 |           pizza    2\n",
      "         and    16491 |            hoot    2\n",
      "          to    14251 |     palpitation    2\n",
      "          in    13893 |      noncleaved    2\n",
      "           a    13047 |            twel    2\n",
      "        that     8013 |          dalles    2\n",
      "          is     7949 |        premiers    2\n",
      "        with     7554 | destabilisation    2\n",
      "          by     7402 |        enclaves    2\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\" --- Most Co-Words ---|--- Least Co-Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_nbrs, least_nbrs):\n",
    "    print(f\"{w1:>12} {c1:>8} |{w2:>16} {c2:>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "91305880-710d-4bea-8611-be8482ff1fb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Expected output for f1RDD:\n",
    "<pre>\n",
    " --- Most Co-Words ---|--- Least Co-Words ----\n",
    "         the    25548 |              vo    1\n",
    "          of    22496 |      noncleaved    2\n",
    "         and    16489 |        premiers    2\n",
    "          to    14249 |        enclaves    2\n",
    "          in    13891 |   selectiveness    2\n",
    "           a    13045 |           trill    2\n",
    "        that     8011 |           pizza    2\n",
    "          is     7947 |            hoot    2\n",
    "        with     7552 |     palpitation    2\n",
    "          by     7400 |            twel    2\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c1d0769-3bbb-4c2f-886c-a6c286d1552c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 226.9152581691742 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - full data (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "most_nbrs, least_nbrs, sample_counts = EDA2(dataRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "\n",
    "# Wall time: 796.3152396678925 seconds on n1-std-4 (with gunzipped txt files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dd5932f9-e92a-4001-a11f-7fe9ef1441ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Most Co-Words ---|--- Least Co-Words ----\n",
      "         the   164984 |          cococo    1\n",
      "          of   155710 |            inin    1\n",
      "         and   132816 |         ooooooo    1\n",
      "          in   110617 |           iiiii    1\n",
      "          to    94360 |          iiiiii    1\n",
      "           a    89199 |             cnj    1\n",
      "          by    67268 |      cococococo    1\n",
      "        with    65129 |             ctn    1\n",
      "        that    61176 |             cni    1\n",
      "          as    60654 |    realacademia    2\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\" --- Most Co-Words ---|--- Least Co-Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_nbrs, least_nbrs):\n",
    "    print(f\"{w1:>12} {c1:>8} |{w2:>16} {c2:>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5d6764e4-62a4-4fce-9761-9aa5ee03d30e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Expected output for dataRDD: \n",
    "(bottom words might vary a little due to ties)\n",
    "<pre>\n",
    " --- Most Co-Words ---|--- Least Co-Words ----\n",
    "         the   164982 |          cococo    1\n",
    "          of   155708 |            inin    1\n",
    "         and   132814 |        charuhas    1\n",
    "          in   110615 |         ooooooo    1\n",
    "          to    94358 |           iiiii    1\n",
    "           a    89197 |          iiiiii    1\n",
    "          by    67266 |             cnj    1\n",
    "        with    65127 |            choh    1\n",
    "        that    61174 |             neg    1\n",
    "          as    60652 |      cococococo    1\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "589e0ce5-0d5e-4d34-8948-1f907fb3d316",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__`NOTE:`__ _before running the plotting code below, make sure that the variable_ `sample_counts` _points to the list generated in_ `part a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "41df9fd4-f75a-45f3-86f9-46d207136f18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: we'll exclude the 0 words with more than 6000 nbrs in this 388 count sample.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAAE/CAYAAADVKysfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvJ0lEQVR4nO3de5xdZXno8d8WKF4wRVQQnqQMtaBC6CBVlGqVU7XFagy9aAFBqpHUHpHY5lRBbbG1eGlFjUe0jYKARTAHUUmLAtKPptJwsVRhA9qmgmaepIKCjZcOmGSfP941shlmz31mrZ35fT+ffDL7Xbdnr9mznvWs911rtzqdDpIkSZKkZnpE3QFIkiRJknqzaJMkSZKkBrNokyRJkqQGs2iTJEmSpAazaJMkSZKkBrNokyRJkqQGs2jTghURrYj4eETcFxE3NiCegYjoRMTuNW3/ORHxHxHxo4g4ro4Y5kpEfCkiXlt3HJLmV0S8KyLeWP18TEQMzcI694yIb0TEvjMOcBfTtDwSERdExF/VtO1GnWPMptn6W9LU1HJyqPpFxF3AfsCOruZDMnNLPRHV4rnAi4DFmfnj0RMj4g+AjwNvysy/6WofAk7KzC/NU5zz5S+BD2Xmml4zRMSJwJ8ATwV+CHwNODszvzKVDUXEVcC1mfnX1esAhoA3j9G2f2b+19TfjqSFLCKeCLwK+KXZXG9m3h8R5wNvBlZX2/oD4Dzgf7pmvSAzT5vNbfeBcfNIde7xKOAXR/JudUHtpMw8Zr6CnCfjnmMARMT+wF8BvwXsBSTwKeCvey3TYz1HA1cBj8vMHVXbR4HfH6NtR2a+btrvSrWxp21hW5aZe3X9e0jBVlePzzw6ELhrggPjvcCbI2LRPMU0K6b5uzsQuG2cdf4J8AHgnZSC/xeADwPLp7GtDcDzu14/D/jGGG3/MZWCrbqy6XFNEsAfAFdm5v9MNOM0fBI4JSL27GrbOCqnPqxgWyB5tWceqewOrJqHWGZVROw2xUXGPceIiH2AjZQi9ujMfCylyNsbePIUt/VVYDfgyK62XwO2jGp7HiX/TtoC+Mz2DX8ReoiI6ACnAW+kfD4OioiXUq4EDQC3A6/LzFuq+Z9Oubp4MHAl0AE2ZebbqunjLXsX8CHKldADgS8Ap2TmcDV9OfAXwC8C9wCvBx4LnJGZv9IV82rg1zLzuDHezwHA31KueN0LvCczPxoRK4BzgT0i4kfAOZl51hi75A7gPuCPq1hGr/8CYKjr/R4D/H1mLu56j+cCJ1MOwpcCbwEuqGK6AXh5Zt7XtdrXRMTbgRbw3sw8p1rXI4A3AadSDurXVvvz3ogYAO4EXgucBdxFOTiPjvdUytXhfYCvVMtviYj/BA4C1kfEDuDxmXl/13I/T7mC+urMvLxrleurf1QnL+8BXlFNW0fpObufh9sA/GlEPCIzd1KSyweAvxzVtqFa968Ca4BDgH8HVmXmv1TTvgRcBxxDSU6HR8RBwP8F9gc+Ue3LkffyS5TP7BHATyk9fr8/RoyS+tuLgfN7TYyIpwEfoRwLEjgzM6+opj2ecpx+PvBNSi/GMZn5XIDMHIqI+4BnA18eZxtvB5YCw8DLgD+JiP8HvI/Su7KTMqLjrMzcURUG76EUnNuAcyh5co/M3F4di3st+weUHHA9sAL4AfC/M/PzVSz7VOv7TUqh8OXMPC4i2tV7HzmW7wFsBV6YmV8b4z1NK490+RvgTRHx4cz8wah1D1By2R6Zub1q+xIlr36seo+nAjcCr6bk9ZMoueEdwJ7An2bmhV2rfUJEXEP5Xd0MvCozv12t+6mUXPErlPOMP8vMddW0Cyg9pwdSPgfLgS+Oincm5xh/QhmxclKV88jMzXQVtOPlvm6Z+dOIuJ6S92+qhu7+HCX/dbcdAmwYL1+PnMdU++WPgWsiYiXlb2U55bPx8VH74c3A6cAiSqH4vzPz2tFxama8Iq2xHAc8Czg0Io6kJL0/BB4P/B1wRTWm/+eAz1IOCvsA/w/43ZGVjLds17ZeARxLOdD/MiVRERFHARcBf0opUJ5HKUSuoBSST+tax0lVDGO5hDLE7gDg94B3RsQLMvM84HU8eGV0rIJtxJ8Bf1wlvOn4XcrVs0OAZcDnKYXbEyh/g6ePmv9/UYrg3wDOiIgXVu2nU343z6/ez32UpNDt+cDTKEn5ISLi14F3Ufb5/sC3KUUkmflk4Ds82Ps6OtEeDTwS+Mw47/OtlKR4BDAIHAW8rce8N1KS62D1+nnANcCmUW0bqv3+j8AHKZ+j9wH/WJ1UjTgZWEkp6v8b+HS17ScA/wk8p2vedwBXA48DFlMSk6Rdz+GUguthqsJkPeVYsC/wBuDiiHhKNcu5wI+BJwGnVP9Gu4MHj1fjWQ5cRsllFwMXAtspwzafTjnWj9xzeyrw0qr9GZS81W28ZaHk7m9Sjn1/DZwXESMXrT4BPBo4rHrP76/aL6Lk0RG/BWztUbDNJI+M+CrwJeD/9Jg+kWcBt1DywSer7T+Tsk9OAj4UEXt1zf9KynH/CZQh/RdX7+UxlLzzScr+OAH4cEQc1rXsicDZlNwy1m0AMznHeCFw+UjBNtokc1+3DTx4sfZ5VbxfGdV2Z2YOMXG+fhLlvO5ASm49i3Lh+cmU84uf/T1UfzOnAc+segt/k3K+pllm0bawfTYiflD9+2xX+7sy895qSMmpwN9l5g2ZuaO6enU/5Y/92cAewAcy86eZeRlwU9d6xlt2xAczc0tm3ktJoEdU7SuA8zPzmszcmcU3qiTwKaoEUx1cB4B/GP3mImIJ5erXmzNzuEpAH6Oc4E9atdzVlCuL0/F/M/O7mZnAPwM3ZOa/Ve/lM5TE2+0vMvPHmXkr5WrWCVX7HwJvzcyhatm3A783aujC26tlxxoO9ErKPr25Wv5M4OjqyuZEHg98b+TKZw+vBP4yM+/OzHsoPZNj7utq+zcAz6sS096Z+S3K/hlpO5RyBfsllGGSn8jM7Zl5CWUo5bKuVV6QmbdV8b0YuD0zL8vMn1J68LqHWP6UkogOqD4XU7ofT1Lf2JvSkzGWZ1PuIXp3Zj6Qmf9EySMnVL1dv0vpwfpJZt5OKZZG+2G1jZ+tsyun/iAiRnLdxsz8bHVyvohyjHpjday+m1I8HV/N+wpKTt1c5cV3jaw8IvabYFmAb2fmR7Pcw3QhpbDar7p36sWUXrH7qpw90kP498Bvdd0GcDK9L4TOJI90+3PgDdV9h1N1Z2Z+vHqPnwKWUHLP/Zl5NfAAD72P8R8zc0MV71ureJdQiuO7qnVtz8ybKRf8ugvlz2XmddV5yHB3ELNwjvF4Sq9VL5PJfd2+DDy3KtJ/jZJPN1I+lyNtI7/zifL1Tsrn//7qfOIVlPvX7616Az/YNe8OykXYQyNij8y8KzP/c5L7QFNg0bawHZeZe1f/jutq39z184HA6u5ERDlAHlD9y8zsdM3/7UkuO6L7ZPonlCRKNV+vP/oLgROrg9DJwLoeV/QOAO7NzO6k/W0geqx3PH8O/FFEPGkay3636+f/GeP1Xg+d/SH7/9s8uL8OBD7TtS/voBws9+ux7GgH0PX7ycwfAd9ncvvj+5QhJuMNqX7I+rtjj4i3RHma2I8i4m+r6SNXBX+NB69gfqWrbXOWISyj1zuy7u64u9/3Ad2vq89n9/Q3UYZL3hgRt0XEa8Z5T5L6132UHpKxHEA5xnT3cowcV55IuT2g+7gx1rH1sZQhiCOu78qpe2fm9WMseyDlYufWrmP531F6en4W16iYJrssdOXUzPxJ9eNelJx6bz50KP7IfFsoQ8x/NyL2phR3F4/xfkfim24e6d5mm1IknzGV5SqjcyiZOV5e7c4HP6IMYzyAsj+fNeoc5ZWUXqaHLTuGmZ5jfJ9SVI+3/p65ryun/igifoEyLHYvynDc5wH/XL3fzV1tG3qsu/tcA+CeUUVqz89lZm6i3FLzduDuiLi0GjaqWeY9bRpLdxG2mXJ15ezRM0XE88t/0eoq3H6BB4utnstOwmZ63IibmddHxAOUE/sTq39j2QLsExGP7Tqo/gLl3oUpycxvRMTllGGN3X5MGW4yYjpF3WhLKFfToMQ78oCYzcBrMvO60Qt0XeXsjJ7WZQslSY0s8xjKlb7J7I+NlHsyjqMM8xlv/SM3of8s9sx8J+UBJt02UIaP3EW5IgjlxOFjVdtIcnlI3F3r/kLX6+73vZWyD4HycJLu11kebHJqNe25wBcjYkOVeCTtOm6hDEu/aYxpW4Al8eA9tFCOK/9OubdpO2X49L9X05Y8fBU8jXKP2ERG59T7gSf0GLnwkONXFdNklx3PZko+3DtH3UdWuZAyzHJ3Ss9gr7wwkzwy2lmUe8y69+HIQzseTbmnD2aeV7vzwV6UYX9bKPvky5n5onGWnSinzuQc44vAb0fEX+TYQyTHzX2ZOfqCLxFxE6UHcf/MHDmP+Oeq7Zd5eF59WL6ujH7fI5/L7vl/JjM/CXyy6q39O8r9clMa1aSJ2dOmiXwUeF1EPCvKk/keExEviYjHUk7ktwOnR8TuEfE7lHHRk1l2IucBr46IF0TEI6J4atf0iyg3Z2/vNbyt6sL/F+BdEfHIiPhlyrDLXlcQJ/IXlBuf9+5q+xplWMk+VS/cG6e57m5/FhGProZ+vpoy/APKzc5nR8SBUB5nHeVhLZP1Sco+PaK6r/CdlKGad020YGb+N6W38dyIOK6Kb4+IeHFE/HU12yXA26q4nlDN//fjrPZfKPvyJKqirboKfE/VNpJcrgQOiYgTq8/Z71OGTj5sSGzlH4HDIuJ3qp7B0+lK+hHx8ohYXL28j5Kcdjx8NZL63JU89Im03W6gFAhvqo5lx1CGnV1aDbu7HHh7dax7KuWBWT8TEUE5+b+eKcjMrZTh9udExKIqvz25uggK5YEQp0fE4oh4HF09UZNYdqLtfp5yz9bjqvfc/bCqz1Ie5LSKkl97mXYeGSOmTZT8dnpX2z2UouekiNitGgkx1ScpjvZbEfHcKPfhv6OKdzMlhxwSESdX+2OPiHhmPPSe+fHin+k5xvsow2Uv7MrrERHvq9Y11dwHJW++sYprxFeqtv/qGrY41Xy9Djiz+uwsptwDShXzUyLi16vPwzClp9OcOgcs2jSuzPwqpVfiQ5QT3E1UDwvJzAeA36le30f5PpDLJ7PsJLY78mSo91MeLPFlHnrF6ROU7v5e4+5HnEC5520L5f6xszLzmsnEMEZMd1bbe8yoOL5O6Rm6mgcLrJn4MmVfXUt5euTVVfsayoNYro6IH1JOFp412ZVmeZLTn1HG7G+lJMLjx13oocu/j/K0q7dRCqvNlJuPP1vN8leUG8xvAW6lXEHt+aWm1dCdf6WMhW93TfpnynCfDdV836dcJVxNGU7yJuClmfm9Huv9HvBy4N3V/AdTevBGPBO4IcoTva6gPI3rzknsAkn95SLKCfujRk+o8tfLKEMBv0f5+pJXdfVOnAb8PGW44ScoJ7ndw/BPBC7sMTR/Iq+iPNnvdkpuvIwHh8l9lPKkyq9TjqGXT2HZiZxMuaf3G8DddF1krO5b+jTloWCjt0nXfDPKI2P4Sx6aU6GcN/wp5fh9GA8tQKbjk5RevXspT4l8JUDVO/YblPi3UH7X76HkpMma9jlGdc/ir1J+JzdUef1ayjnPpqnmvsqXKfmz+2L2V+jKqZUp5WvKRetvU57seTUPPffak5Jvv0fZh/vy8FFJmgWtTme8nl9pamLUI/DncDuPoiSdIzPzP+ZyW5Kk/hQR7wTuzswPzHA97wGelJkj3832deB5WR4GMmdijEfgz+G2/hw4JDNPmnBmSfPOe9rUr/4IuMmCTZLUS2ZO64p/NSTy5yi9EM+kDHt7bbXO+4Gn9l66/0R5Yu8KvA9JaiyLNvWdKF9Y3aI8FEOSpNn2WMqQyAMoozrOAT5Xa0RzJMqXZX8A+ERmbphgdkk1cXikJEmSJDWYDyKRJEmSpAazaJMkSZKkBmvEPW377LNPZ8mSsb63cvKGh4d55CMfOUsRzT/jr5fx18v461NH7Lfccsv3Op3OE+d1o31spjmynz+fYPx1M/56GX+95jv+8fJjI4q2JUuW8IUvfGFG62i32yxdunSWIpp/xl8v46+X8denjtgPOOCAb8/rBvtUq9VaBiwbGBiYUY7s588nGH/djL9exl+v+Y5/vPzo8EhJkhqo0+ms73Q6KxctWlR3KJKkmlm0SZIkSVKDWbRJkiRJUoNZtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNZtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNZtEmSJElSg+1edwCSJGluHb3m5p7TNq46ch4jkSRNxy5VtJmUJEmSJO1qHB4pSVIDtVqtZa1Wa+22bdvqDkWSVDOLNkmSGqjT6azvdDorFy1aVHcokqSaWbRJkiRJUoNNeE9bRDwS2ADsWc1/WWaeFRH7AJ8CBoC7gFdk5n3VMmcCK4AdwOmZedWcRC9JkiRJu7jJ9LTdD/x6Zg4CRwDHRsSzgTOAazPzYODa6jURcShwPHAYcCzw4YjYbQ5ilyRJkqRd3oRFW2Z2MvNH1cs9qn8dYDlwYdV+IXBc9fNy4NLMvD8z7wQ2AUfNZtCSJEmStFBM6p62iNgtIr4G3A1ck5k3APtl5laA6v99R2YHNnctPlS1SZIkSZKmaFLf05aZO4AjImJv4DMRsXSc2VtjtHVGN0TESmAlQKfTod1uTyaUnoaHh1l9+M6e02e6/rk2PDzc+BjHY/z1Mv569XP8/Ry7JEkLxZS+XDszfxARX6Lcq/bdiNg/M7dGxP6UXjgoPWtLuhZbDGwZY11rgbUAg4ODnaVLx6sDJ9Zutznnugd6Tt+4ambrn2vtdpuZ7oM6GX+9jL9e/Rx/P8cuSdJCMeHwyIh4YtXDRkQ8Cngh8A3gCuCUarZTgM9VP18BHB8Re0bEQcDBwI2zHLckSZIkLQiT6WnbH7iwegLkI4B1mfkPEbERWBcRK4DvAC8HyMzbImIdcDuwHXh9NbxSkiRJkjRFExZtmXkL8PQx2r8PvKDHMmcDZ884OkmSJEla4Cb19EhJkiRJUj0s2iRJkiSpwSzaJEmSJKnBLNokSZIkqcEs2iRJkiSpwSzaJElqoFartazVaq3dtm1b3aFIkmpm0SZJUgN1Op31nU5n5aJFi+oORZJUM4s2SZIkSWowizZJkiRJajCLNkmSJElqMIs2SZIkSWowizZJkiRJajCLNkmSJElqMIs2SZIkSWowizZJkiRJajCLNkmSJElqMIs2SZIkSWowizZJkiRJajCLNkmSJElqMIs2SZIkSWowizZJkiRJajCLNkmSJElqMIs2SZIkSWowizZJkiRJajCLNkmSJElqMIs2SZIkSWowizZJkiRJajCLNkmSJElqMIs2SZIkSWowizZJkiRJajCLNkmSJElqsN3rDkCSpIUiIo4DXgLsC5ybmVfXG5EkqR9YtEmSNAMRcT7wUuDuzFza1X4ssAbYDfhYZr47Mz8LfDYiHge8F7BokyRNaMKiLSKWABcBTwJ2Amszc01EvB04FbinmvUtmXlltcyZwApgB3B6Zl41B7FLktQEFwAfouRKACJiN+Bc4EXAEHBTRFyRmbdXs7ytmi5J0oQm09O2HVidmTdHxGOBf42Ia6pp78/M93bPHBGHAscDhwEHAF+MiEMyc8dsBi5JUhNk5oaIGBjVfBSwKTO/BRARlwLLI+IO4N3A5zPz5vmNVJLUryYs2jJzK7C1+vmHVcKJcRZZDlyamfcDd0bEJkry2jgL8UqS1A8C2Nz1egh4FvAG4IXAz0fEL2Xm3465cMRKYCVAp9Oh3W5PO5Dh4WFWH76z5/SZrHs+DA8PNz7G8Rh/vYy/XsY/e6Z0T1t1JfHpwA3Ac4DTIuJVwFcpvXH3URLV9V2LDTF+kSdJ0q6mNUZbJzM/CHxwooUzcy2wFmBwcLCzdOnSCZbord1uc851D/ScvnHV9Nc9H9rtNjN5/3Uz/noZf72Mf/ZMumiLiL2ATwNvzMxtEfER4B1Ap/r/HOA19EhUY6xv1q4iglcS62b89TL+evVz/P0ce8MNAUu6Xi8GttQUiySpz02qaIuIPSgF28WZeTlAZn63a/pHgX+oXk4qUc3mVUTwSmLdjL9exl+vfo6/n2NvuJuAgyPiICAp93qfWG9IkqR+NeGXa0dECzgPuCMz39fVvn/XbL8NjFyqvQI4PiL2rJLVwcCNsxeyJEnNERGXUO7bfkpEDEXEiszcDpwGXAXcAazLzNvqjFOS1L8m09P2HOBk4NaI+FrV9hbghIg4gjL08S7gDwEy87aIWAfcTnny5Ot9cqQkaVeVmSf0aL8SuHK66221WsuAZQMDA9NdhSRpFzGZp0d+hbHvU+uZiDLzbODsGcQlSdKC1ul01gPrBwcHT607FklSvSYcHilJkiRJqo9FmyRJkiQ1mEWbJEmSJDWYRZskSQ3UarWWtVqttdu2bas7FElSzSzaJElqoE6ns77T6axctGhR3aFIkmpm0SZJkiRJDWbRJkmSJEkNZtEmSZIkSQ1m0SZJUgP5IBJJ0giLNkmSGsgHkUiSRli0SZIkSVKDWbRJkiRJUoNZtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNZtEmS1EA+8l+SNMKiTZKkBvKR/5KkERZtkiRJktRgFm2SJEmS1GAWbZIkSZLUYBZtkiRJktRgFm2SJEmS1GAWbZIkSZLUYBZtkiQ1kN/TJkkaYdEmSVID+T1tkqQRFm2SJEmS1GAWbZIkSZLUYBZtkiRJktRgFm2SJEmS1GAWbZIkSZLUYBZtkiRJktRgFm2SJEmS1GAWbZIkSZLUYLtPNENELAEuAp4E7ATWZuaaiNgH+BQwANwFvCIz76uWORNYAewATs/Mq+YkekmSdlGtVmsZsGxgYKDuUCRJNZtMT9t2YHVmPg14NvD6iDgUOAO4NjMPBq6tXlNNOx44DDgW+HBE7DYXwUuStKvqdDrrO53OykWLFtUdiiSpZhMWbZm5NTNvrn7+IXAHEMBy4MJqtguB46qflwOXZub9mXknsAk4apbjliRJkqQFYUr3tEXEAPB04AZgv8zcCqWwA/YdmQ3Y3LXYUNUmSZIkSZqiCe9pGxERewGfBt6YmdsietZhrTHaOmOsbyWwEqDT6dButycbypiGh4dZffjOntNnuv65Njw83PgYx2P89TL+evVz/P0cuyRJC8WkiraI2INSsF2cmZdXzd+NiP0zc2tE7A/cXbUPAUu6Fl8MbBm9zsxcC6wFGBwc7CxdunSab6Fot9ucc90DPadvXDWz9c+1drvNTPdBnYy/XsZfr36Ov59jlyRpoZhweGREtIDzgDsy831dk64ATql+PgX4XFf78RGxZ0QcBBwM3Dh7IUuSJEnSwjGZnrbnACcDt0bE16q2twDvBtZFxArgO8DLATLztohYB9xOefLk6zNzx2wHLkmSJEkLwYRFW2Z+hbHvUwN4QY9lzgbOnkFckiRJkiSm+PRISZIkSdL8smiTJEmSpAazaJMkSZKkBpv097RJkqT502q1lgHLBgYG5nQ7R6+5uee0jauOnNNtS5Imx542SZIaqNPprO90OisXLVpUdyiSpJpZtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNZtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNZtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNZtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNZtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNZtEmS1ECtVmtZq9Vau23btrpDkSTVzKJNkqQG6nQ66zudzspFixbVHYokqWYWbZIkSZLUYBZtkiRJktRgFm2SJEmS1GAWbZIkSZLUYBZtkiRJktRgu9cdgCRJaqaj19zcc9rGVUfOYySStLDZ0yZJkiRJDWbRJkmSJEkNZtEmSZIkSQ1m0SZJkiRJDWbRJkmSJEkNNuHTIyPifOClwN2ZubRqeztwKnBPNdtbMvPKatqZwApgB3B6Zl41B3FLkiRJ0oIwmUf+XwB8CLhoVPv7M/O93Q0RcShwPHAYcADwxYg4JDN3zEKskiRJkrTgTDg8MjM3APdOcn3LgUsz8/7MvBPYBBw1g/gkSZIkaUGbyT1tp0XELRFxfkQ8rmoLYHPXPENVmyRJkiRpGiYzPHIsHwHeAXSq/88BXgO0xpi3M9YKImIlsBKg0+nQbrenGUoxPDzM6sN39pw+0/XPteHh4cbHOB7jr5fx16uf4+/n2CVJWiimVbRl5ndHfo6IjwL/UL0cApZ0zboY2NJjHWuBtQCDg4OdpUuXTieUn2m325xz3QM9p29cNbP1z7V2u81M90GdjL9exl+vfo6/n2OXJGmhmNbwyIjYv+vlbwMjl2mvAI6PiD0j4iDgYODGmYUoSZIkSQvXZB75fwlwDPCEiBgCzgKOiYgjKEMf7wL+ECAzb4uIdcDtwHbg9T45UpIkSZKmb8KiLTNPGKP5vHHmPxs4eyZBSZIkSZKKmTw9UpIkSZI0xyzaJEmSJKnBLNokSZIkqcEs2iRJkiSpwab75dqSJGmKIuIXgbcCP5+Zv1d3PJKk/mBPmyRJMxAR50fE3RHRHtV+bER8MyI2RcQZAJn5rcxcUU+kkqR+ZdEmSdLMXAAc290QEbsB5wIvBg4FToiIQ+c/NEnSrsCiTZKkGcjMDcC9o5qPAjZVPWsPAJcCy+c9OEnSLsF72iRJmn0BbO56PQQ8KyIeD5wNPD0izszMd425cMRKYCVAp9Oh3W6PNdukDA8Ps/rwndNevpeZxDQVw8PD87atuWD89TL+ehn/7LFokyRp9rXGaOtk5veB1020cGauBdYCDA4OdpYuXTrtQNrtNudc98C0l+9l46rpxzQV7Xabmbz/uhl/vYy/XsY/exweKUnS7BsClnS9XgxsqSkWSVKfs6dNkqTZdxNwcEQcBCRwPHBivSFJkvqVPW2SJM1ARFwCbASeEhFDEbEiM7cDpwFXAXcA6zLztjrjlCT1L3vaJEmagcw8oUf7lcCV011vq9VaBiwbGBiY7iokSbsIe9okSWqgTqezvtPprFy0aFHdoUiSambRJkmSJEkNZtEmSZIkSQ1m0SZJkiRJDeaDSCRJaqCmP4jk6DU395y2cdWR8xiJJO367GmTJKmBfBCJJGmERZskSZIkNZhFmyRJkiQ1mEWbJEmSJDWYRZskSZIkNZhFmyRJDdRqtZa1Wq2127ZtqzsUSVLNLNokSWognx4pSRph0SZJkiRJDWbRJkmSJEkNZtEmSZIkSQ1m0SZJkiRJDbZ73QFIkqSHa7Vay4BlAwMDdYcyZUevubnntI2rjpzHSCRp12BPmyRJDeTTIyVJIybsaYuI84GXAndn5tKqbR/gU8AAcBfwisy8r5p2JrAC2AGcnplXzUnkkiRJkrQATKan7QLg2FFtZwDXZubBwLXVayLiUOB44LBqmQ9HxG6zFq0kSZIkLTATFm2ZuQG4d1TzcuDC6ucLgeO62i/NzPsz805gE3DU7IQqSZIkSQvPdO9p2y8ztwJU/+9btQewuWu+oapNkiRJkjQNs/30yNYYbZ2xZoyIlcBKgE6nQ7vdntGGh4eHWX34zp7TZ7r+uTY8PNz4GMdj/PUy/nr1c/z9HLskSQvFdIu270bE/pm5NSL2B+6u2oeAJV3zLQa2jLWCzFwLrAUYHBzsLF26dJqhFO12m3Oue6Dn9I2rZrb+udZut5npPqiT8dfL+OvVz/H3c+ySJC0U0x0eeQVwSvXzKcDnutqPj4g9I+Ig4GDgxpmFKEnSwtNqtZa1Wq2127ZtqzsUSVLNJizaIuISYCPwlIgYiogVwLuBF0XEfwAvql6TmbcB64DbgS8Ar8/MHXMVvCRJuyq/p02SNGLC4ZGZeUKPSS/oMf/ZwNkzCUqSJEmSVEx3eKQkSZIkaR5YtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNZtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNN+D1tkiRp/rVarWXAsoGBgbpDmVVHr7m557SNq46cx0gkqX/Y0yZJUgN1Op31nU5n5aJFi+oORZJUM4s2SZIkSWowizZJkiRJajCLNkmSJElqMIs2SZIkSWowizZJkiRJajCLNkmSJElqMIs2SZIkSWowizZJkiRJajCLNkmSJElqMIs2SZIkSWowizZJkhqo1Wota7Vaa7dt21Z3KJKkmlm0SZLUQJ1OZ32n01m5aNGiukORJNXMok2SJEmSGsyiTZIkSZIazKJNkiRJkhrMok2SJEmSGsyiTZIkSZIazKJNkiRJkhrMok2SJEmSGsyiTZIkSZIazKJNkiRJkhrMok2SJEmSGsyiTZIkSZIazKJNkiRJkhps95ksHBF3AT8EdgDbM/MZEbEP8ClgALgLeEVm3jezMCVJkiRpYZqNnrb/lZlHZOYzqtdnANdm5sHAtdVrSZIkSdI0zMXwyOXAhdXPFwLHzcE2JEmSJGlBmGnR1gGujoh/jYiVVdt+mbkVoPp/3xluQ5IkSZIWrBnd0wY8JzO3RMS+wDUR8Y3JLlgVeSsBOp0O7XZ7RoEMDw+z+vCdPafPdP1zbXh4uPExjsf462X89ern+Ps59l1dq9VaBiwbGBioO5S+dfSam3tO27jqyHmMZPrm4j3sCvtFWmhmVLRl5pbq/7sj4jPAUcB3I2L/zNwaEfsDd/dYdi2wFmBwcLCzdOnSmYRCu93mnOse6Dl946qZrX+utdttZroP6mT89TL+evVz/P0c+66u0+msB9YPDg6eWncskqR6TXt4ZEQ8JiIeO/Iz8BtAG7gCOKWa7RTgczMNUpIkSZIWqpn0tO0HfCYiRtbzycz8QkTcBKyLiBXAd4CXzzxMSZIkSVqYpl20Zea3gMEx2r8PvGAmQUmSJEmSirl45L8kSZIkaZZYtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNZtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNZtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNZtEmSJElSg1m0SZIkSVKD7V53APPl6DU3j9m+cdWR8xyJJEmSJE2ePW2SJEmS1GAWbZIkSZLUYBZtkiRJktRgFm2SJEmS1GAWbZIkSZLUYBZtkiRJktRgFm2SJEmS1GAWbZIkSZLUYAvmy7UlSapbRDwG+DDwAPClzLy45pAkSX3AnjZJkmYgIs6PiLsjoj2q/diI+GZEbIqIM6rm3wEuy8xTgZfNe7CSpL604Hvajl5zc89pG1cdOY+RSJL61AXAh4CLRhoiYjfgXOBFwBBwU0RcASwGbq1m2zG/YUqS+tWCL9okSZqJzNwQEQOjmo8CNmXmtwAi4lJgOaWAWwx8jXFGu0TESmAlQKfTod1u95p1QsPDw6w+fOe0l59Pl/3TjQ9r2+9RY7dPxurDp7atEU/d99HT2t5YhoeHJ/z9fePun/ScNt57mO7nYvXh2ye9zsnE32QLJf7xPkOz+Xmeql1t/9e5ny3aJEmafQFs7no9BDwL+CDwoYh4CbC+18KZuRZYCzA4ONhZunTptANpt9ucc90D016+bqsP3845t87v6crGVdPf36O1220m+v2dOs6on/FMN87xtjd6nZOJv8kWSvxT+Z3Op11t/9e5ny3aJEmafa0x2jqZ+WPg1fMdjCSpv/kgEkmSZt8QsKTr9WJgS02xSJL6nD1tkiTNvpuAgyPiICCB44ET6w1JktSv7GmTJGkGIuISYCPwlIgYiogVmbkdOA24CrgDWJeZt01lva1Wa1mr1Vq7bdu22Q9aktRX7Gkbh18HIEmaSGae0KP9SuDK6a630+msB9YPDg6eOt11SJJ2Dfa0SZIkSVKD2dM2TfbCSZIkSZoPc1a0RcSxwBpgN+BjmfnuudpW01jQSZIkSZotczI8MiJ2A84FXgwcCpwQEYfOxbYkSdoV+SASSdKIueppOwrYlJnfAoiIS4HlwO1ztL0Frd979uYi/n7fJ5Lkg0gkSSPmqmgLYHPX6yHgWXO0rb7Sq5hYffh2Th2n0Jjt7cH8Fi/jxSFNVlM+z7u6XvvZfSxJUj1anU5n1lcaES8HfjMzX1u9Phk4KjPf0DXPSmAlwJYtW54CfHMm23zEIx7xhJ07d35vJuuok/HXy/jrZfz1qSn2AzudzhPneZt9q9Vq3QN8e7rL9/PnE4y/bsZfL+OvVw3x98yPc9XTNgQs6Xq9GNjSPUNmrgXWztYGI+KrmfmM2VrffDP+ehl/vYy/Pv0c+0Ix0wK333/Hxl8v46+X8derSfHPVdF2E3BwRBwEJHA8cOIcbUuSJEmSdllz8vTIzNwOnAZcBdwBrMvM2+ZiW5IkSZK0K5uz72nLzCuBK+dq/WOYtaGWNTH+ehl/vYy/Pv0cuyan33/Hxl8v46+X8derMfHPyYNIJEmSJEmzY06GR0qSJEmSZsecDY+cLxFxLLAG2A34WGa+u+aQHiYilgAXAU8CdgJrM3NNROwDfAoYAO4CXpGZ91XLnAmsAHYAp2fmVTWE/hARsRvwVSAz86X9FH9E7A18DFgKdIDXUL5mol/i/2PgtZTYbwVeDTyahsYfEecDLwXuzsylVduUPy8R8SvABcCjKMOtV2XmnA8P6BH/3wDLgAeA/wRenZk/6Jf4u6b9H+BvgCdm5veaGL9mjzlyfpgf69Nv+bGKoW9zpPmxvvj7uqetOkieC7wYOBQ4ISIOrTeqMW0HVmfm04BnA6+v4jwDuDYzDwaurV5TTTseOAw4Fvhw9V7rtoryYJkR/RT/GuALmflUYJDyPvoi/ogI4HTgGdUBZrcqvibHf0G17W7TifcjlO9zPLj6N3qdc+WCMbZ1DbA0M38Z+HfgTOir+EdOjl8EfKerrYnxaxaYI+eV+bEGfZofob9z5AVjbMf8OA/x93XRBhwFbMrMb2XmA8ClwPKaY3qYzNyamTdXP/+QckAMSqwXVrNdCBxX/bwcuDQz78/MO4FNlPdam4hYDLyEcjVuRF/EHxGLgOcB5wFk5gPVFaC+iL+yO/CoiNidcgVxCw2OPzM3APeOap5SvBGxP7AoMzdWV68u6lpmTo0Vf2ZeXT0ZF+B6yvdP9k38lfcDb6JckR7RuPg1a8yR88D8aH6cqn7OkebH+uLv96ItgM1dr4eqtsaKiAHg6cANwH6ZuRVK0gL2HZmN5r2vD1A+zDu72vol/l8E7gE+HhH/FhEfi4jH0CfxZ2YC76Vc/dkK/HdmXk2fxN9lqvFG9fPo9iZ4DfD56ue+iD8iXkb5OH199CT6IH5NS1OPBT31aY78AObHWuxC+RF2nRxpfpwj/V60tcZoa+z9FhGxF/Bp4I2ZuW2cWRv1viJiZOzvv05ykUbFT7kKdyTwkcx8OvBjqmEHPTQq/oh4HOVqz0HAAcBjIuKkcRZpVPyT0CveRr6PiHgrZTjXxVVT4+OPiEcDbwX+fIzJjY9f09ZXv8N+zJHmR8D8ONf65hhtfpxb/V60DQFLul4vpnSLN05E7EFJRhdn5uVV83erLlaq/++u2pv2vp4DvCwi7qIMr/n1iPh7+if+IWAoM2+oXl9GSVL9Ev8LgTsz857M/ClwOfCr9E/8I6Ya7xAPDrHobq9NRJxCuYH5lV03HPdD/E+mnNR8vfo7XgzcHBFPoj/i1/Q09VjwMH2cI82P5sfZ0tc50vw49/H3e9F2E3BwRBwUET9HuVnwippjepiIaFHGi9+Rme/rmnQFcEr18ynA57raj4+IPSPiIMoNjjfOV7yjZeaZmbk4Mwco+/ifMvMk+if+/wI2R8RTqqYXALfTJ/FThn08OyIeXX2WXkC556Nf4h8xpXir4SE/jIhnV+/7VV3LzLsoT+F7M/CyzPxJ16TGx5+Zt2bmvpk5UP0dDwFHVn8bjY9f02aOnGPmx9rzy66SH6GPc6T5cX7i7+tH/mfm9og4DbiK8sSg8zPztprDGstzgJOBWyPia1XbW4B3A+siYgXlwPNygMy8LSLWUQ6c24HXZ+aOeY96Yv0U/xuAi6sTl29RHgn8CPog/sy8ISIuA26u4vk3YC2wFw2NPyIuAY4BnhARQ8BZTO/z8kc8+Ejdz/PgOPk64j8T2BO4JiIArs/M1/VL/Jl53ljzNjF+zQ5zZK36KXbz4zzr5xxpfqwv/lan0/ShvJIkSZK0cPX78EhJkiRJ2qVZtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNZtEmSJElSg1m0SZIkSVKDWbRJkiRJUoNZtEmSJElSg/1/9JYMwsPydHIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# part b - plot histogram (RUN THIS CELL AS IS - feel free to modify format)\n",
    "\n",
    "# removing extreme upper tail for a better visual\n",
    "counts = np.array(sample_counts)[np.array(sample_counts) < 6000]\n",
    "t = sum(np.array(sample_counts) > 6000)\n",
    "n = len(counts)\n",
    "print(\"NOTE: we'll exclude the %s words with more than 6000 nbrs in this %s count sample.\" % (t,n))\n",
    "\n",
    "# set up figure\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (15,5))\n",
    "\n",
    "# plot regular hist\n",
    "ax1.hist(counts, bins=50)\n",
    "ax1.set_title('Freqency of Number of Co-Words', color='0.1')\n",
    "ax1.set_facecolor('0.9')\n",
    "ax1.tick_params(axis='both', colors='0.1')\n",
    "ax1.grid(True)\n",
    "\n",
    "# plot log scale hist\n",
    "ax2.hist(counts, bins=50)\n",
    "ax2.set_title('(log)Freqency of Number of Co-Words', color='0.1')\n",
    "ax2.set_facecolor('0.9')\n",
    "ax2.tick_params(axis='both', colors='0.1')\n",
    "ax2.grid(True)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "15310f55-9345-4dd1-907b-a719f475e693",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part c - spark job\n",
    "def compareRankings(rdd1, rdd2):\n",
    "    percent_overlap = None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "\n",
    "    inner_join_count = rdd1.join(rdd2).keys().distinct().count()\n",
    "    total_count = rdd1.keys().distinct().count()\n",
    "    percent_overlap = round((inner_join_count / total_count) * 100, 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return percent_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "daa4ad30-6c4d-4622-8bd6-1a5c6f94287e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 1000 words with most neighbors, 100.0 percent are also in the list of 1000 most frequent words.\n",
      "Of the 1000 words with least neighbors, 100.0 percent are also in the list of 1000 least frequent words.\n"
     ]
    }
   ],
   "source": [
    "# part c - get lists for comparison (RUN THIS CELL AS IS...)\n",
    "# (... then change 'testRDD' to 'f1RDD'/'dataRDD' when ready)\n",
    "total, topWords, bottomWords = EDA1(f1RDD, 1000)\n",
    "topNbrs, bottomNbrs, sample_counts = EDA2(f1RDD, 1000)\n",
    "twRDD = sc.parallelize(topWords)\n",
    "bwRDD = sc.parallelize(bottomWords)\n",
    "tnRDD = sc.parallelize(topNbrs)\n",
    "bnRDD = sc.parallelize(bottomNbrs)\n",
    "top_overlap = compareRankings(tnRDD, twRDD)\n",
    "bottom_overlap = compareRankings(bnRDD,bwRDD)\n",
    "print(f\"Of the 1000 words with most neighbors, {top_overlap} percent are also in the list of 1000 most frequent words.\")\n",
    "print(f\"Of the 1000 words with least neighbors, {bottom_overlap} percent are also in the list of 1000 least frequent words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "811671f1-5c10-439c-b452-8b9d22d5441e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 1000 words with most neighbors, 100.0 percent are also in the list of 1000 most frequent words.\n",
      "Of the 1000 words with least neighbors, 0.0 percent are also in the list of 1000 least frequent words.\n"
     ]
    }
   ],
   "source": [
    "#<--- SOLUTION --->\n",
    "# part c - get lists for comparison (RUN THIS CELL AS IS...)\n",
    "# (... then change 'testRDD' to 'f1RDD'/'dataRDD' when ready)\n",
    "total, topWords, bottomWords = EDA1(dataRDD, 1000)\n",
    "topNbrs, bottomNbrs, sample_counts = EDA2(dataRDD, 1000)\n",
    "twRDD = sc.parallelize(topWords)\n",
    "bwRDD = sc.parallelize(bottomWords)\n",
    "tnRDD = sc.parallelize(topNbrs)\n",
    "bnRDD = sc.parallelize(bottomNbrs)\n",
    "top_overlap = compareRankings(tnRDD, twRDD)\n",
    "bottom_overlap = compareRankings(bnRDD,bwRDD)\n",
    "print(f\"Of the 1000 words with most neighbors, {top_overlap} percent are also in the list of 1000 most frequent words.\")\n",
    "print(f\"Of the 1000 words with least neighbors, {bottom_overlap} percent are also in the list of 1000 least frequent words.\")\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8bda2949-b3f0-4446-8010-4f3d53bfd605",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 7: Basis Vocabulary & Stripes\n",
    "\n",
    "Every word that appears in our data is a potential feature for our synonym detection analysis. However as we've discussed, some are likely to be more useful than others. In this question, you'll choose a judicious subset of these words to form our 'basis vocabulary'. Practically speaking, this means that when we build our stripes, we are only going to keep track of when a term co-occurs with one of these basis words. \n",
    "\n",
    "\n",
    "### Q7 Tasks:\n",
    "* __a1) multiple choice:__ Suppose we were deciding between two different basis vocabularies: the 1000 most frequent words or the 1000 least frequent words. How would this choice impact the quality of the synonyms we are able to detect? How does this choice relate to the ideas of 'overfitting' or 'underfitting' a training set?\n",
    "<pre>  \n",
    "   MULTIPLE CHOICE:\n",
    "   A. 1000 most frequent words would overfit, while 1000 least frequent words would underfit\n",
    "   B. 1000 most frequent words would underfit, while 1000 least frequent words would overfit\n",
    "\n",
    "</pre>\n",
    "\n",
    "* __a2) short response:__Explain your answer above\n",
    "* __b) short response:__ If we had a much larger dataset, computing the full ordered list of words would be extremely expensive. If we need to none-the-less get an estimate of word frequency in order to decide on a basis vocabulary, what alternative strategy could we take?\n",
    "\n",
    "* __c) multiple choice:__ Run the provided spark job that does the following:\n",
    "  * tokenizes, removes stopwords and computes a word count on the ngram data\n",
    "  * subsets the top 10,000 words (these are the terms we'll consider as potential synonyms)\n",
    "  * subsets words 9,000-9,999 (this will be our 1,000 word basis vocabulary)    \n",
    "  (to put it another way - of the top 10,000 words, the bottom 1,000 form the basis vocabulary)\n",
    "  * saves the full 10K word list and the 1K basis vocabulary to file for use in `d`.  \n",
    "<pre>\n",
    "  What is another way to describe the Basis Vocabulary in machine learning terms?\n",
    "  A. Stop-words\n",
    "  **B. Features**\n",
    "  C. Postings\n",
    "  D. 1000-grams\n",
    "</pre>\n",
    "\n",
    "* __d) code:__ Write a spark job that builds co-occurrence stripes for the top 10K words in the ngram data using the basis vocabulary you developed in `part c`. This job/function, unlike others so far, should return an RDD (which we will then use in q8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b533d39-a950-40cc-b529-05216c6872d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q7 Student Answers:\n",
    "> __a1)__ Type your answer here!   \n",
    "> __a2)__ Type your answer here!   \n",
    "> __b)__ Type your answer here!   \n",
    "> __c)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b853bc36-399b-4f21-af49-697c2019f6a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part c - provided stopwords (RUN THIS CELL AS IS)\n",
    "STOPWORDS =  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 'should', 'can', 'now', 'will', 'just', \n",
    "              'would', 'could', 'may', 'must', 'one', 'much', \"it's\",\n",
    "              \"can't\", \"won't\", \"don't\", \"shouldn't\", \"hasn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6f3bad8f-3d76-401d-8687-e16db841de68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part c - get the vocabulary and basis (RUN THIS CELL AS IS)\n",
    "# \n",
    "def get_vocab(rdd, n_total, n_basis):\n",
    "    vocab, basis = None, None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "    \n",
    "    def create_tokens(doc):\n",
    "        ngram, count, pages_count, books_count = doc.split('\\t')\n",
    "        for word in ngram.lower().split(' '):\n",
    "            if word not in STOPWORDS:\n",
    "                yield (word, int(count))\n",
    "        \n",
    "    result = rdd.flatMap(lambda x: create_tokens(x))\\\n",
    "                .reduceByKey(lambda x, y: x+y)\\\n",
    "                .cache()\n",
    "    \n",
    "    sorted_rdd = result.sortBy(lambda x: x[1], False)\n",
    "    \n",
    "    vocab = sorted_rdd.takeOrdered(n_total, key=lambda x: -x[1])\n",
    "    basis = vocab[-(n_basis+1):-1]\n",
    "    \n",
    "#     bottom_n = sorted_rdd.takeOrdered(n, key=lambda x: x[1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return vocab, basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "817d0400-d9a5-4f5f-9fc8-106fc2614c75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 94.79371786117554 seconds\n"
     ]
    }
   ],
   "source": [
    "# part c - run your job (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "VOCAB, BASIS = get_vocab(dataRDD, 10000, 1000)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "# 268.0176115036011 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "320fa840-3dd9-40ce-9584-4e7861d5b8f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part c - save to file (RUN THIS CELL AS IS)\n",
    "#with open(\"vocabulary.txt\", \"w\") as file:\n",
    "#    file.write(str(VOCAB))\n",
    "#with open(\"basis.txt\", \"w\") as file:\n",
    "#    file.write(str(BASIS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying from <STDIN>...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n",
      "Copying from <STDIN>...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "# part c - save to file (RUN THIS CELL AS IS)\n",
    "!echo \"{','.join(VOCAB)}\" | gsutil cp - {HW3_FOLDER}/output/vocabulary.txt\n",
    "!echo \"{','.join(BASIS)}\" | gsutil cp - {HW3_FOLDER}/output/basis.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e26c9657-5646-4835-b065-489c874ad30b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - spark job\n",
    "def buildStripes(rdd, vocab, basis):\n",
    "    stripesRDD = None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return stripesRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2a70b273-fb86-4f86-8111-a3480ecf18a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - run your systems test (RUN THIS CELL AS IS)\n",
    "VOCAB, BASIS = get_vocab(testRDD, 10, 10)\n",
    "testStripesRDD = buildStripes(testRDD, VOCAB, BASIS)\n",
    "start = time.time()\n",
    "print(testStripesRDD.collect())\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "# Wall time: 0.1581110954284668 seconds\n",
    "# Expected results\n",
    "'''\n",
    "[('worst', {'times'}), ('best', {'times'}), ('foolishness', {'age'}), ('age', {'wisdom', 'foolishness', 'times'}), ('wisdom', {'age'}), ('times', {'age', 'best', 'worst'})]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2d6a2071-70d0-482b-9290-4e91d9f543d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - run your single file test (RUN THIS CELL AS IS)\n",
    "VOCAB, BASIS = get_vocab(f1RDD, 10000, 1000)\n",
    "f1StripesRDD = buildStripes(f1RDD, VOCAB, BASIS).cache()\n",
    "start = time.time()\n",
    "print(f1StripesRDD.top(5))\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "# Wall time: 1.55739426612854 seconds\n",
    "# Expected results\n",
    "'''\n",
    "[('zippor', {'balak'}), ('zedong', {'mao'}), ('zeal', {'infallibility'}), ('youth', {'mould', 'constrained'}), ('younger', {'careers'})]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "470096b5-c854-4a3a-b8b7-50e45cd23c72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - run the full analysis and take a look at a few stripes (RUN THIS CELL AS IS)\n",
    "#VOCAB = ast.literal_eval(open(\"vocabulary.txt\", \"r\").read())\n",
    "#BASIS = ast.literal_eval(open(\"basis.txt\", \"r\").read())\n",
    "wordzz= !gsutil cat {HW3_FOLDER}/output/vocabulary.txt\n",
    "VOCAB = wordzz[0].split(\",\")\n",
    "wordzz= !gsutil cat {HW3_FOLDER}/output/basis.txt\n",
    "BASIS = wordzz[0].split(\",\")\n",
    "\n",
    "stripesRDD = buildStripes(dataRDD, VOCAB, BASIS).cache()\n",
    "\n",
    "start = time.time()\n",
    "for wrd, stripe in stripesRDD.top(3):\n",
    "    print(wrd)\n",
    "    print(list(stripe))\n",
    "    print('-------')\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "# Wall time: 214.13801431655884 seconds\n",
    "# Expected results:\n",
    "'''\n",
    "zones\n",
    "['remotest', 'adhesion', 'residential', 'subdivided', 'environments', 'gaza', 'saturation', 'localities', 'uppermost', 'warmer', 'buffer', 'parks']\n",
    "-------\n",
    "zone\n",
    "['tribal', 'narrower', 'fibrous', 'saturation', 'originate', 'auxiliary', 'ie', 'buffer', 'transitional', 'turbulent', 'vomiting', 'americas', 'articular', 'poorly', 'intervening', 'officially', 'accumulate', 'assisting', 'flexor', 'traversed', 'unusually', 'uppermost', 'cartilage', 'inorganic', 'illuminated', 'glowing', 'contamination', 'trigger', 'masculine', 'defines', 'avoidance', 'residential', 'southeastern', 'penis', 'cracks', 'atlas', 'excitation', 'persia', 'diffuse', 'subdivided', 'alaska', 'guides', 'au', 'sandy', 'penetrating', 'parked']\n",
    "-------\n",
    "zinc\n",
    "['ammonium', 'coating', 'pancreas', 'insoluble', \"alzheimer's\", 'diamond', 'radioactive', 'metallic', 'weighing', 'dysfunction', 'wasting', 'phosphorus', 'transcription', 'dipped', 'hydroxide', 'burns', 'leukemia', 'dietary']\n",
    "-------\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoint the stripes (Just RUN all cells in this section AS IS and review outputs)\n",
    "Let's save your full stripes to disk. Then we can reload later if needed. We repartition our data first and then save (as otherwise, we will end up with 190 partitions; I wonder why!).\n",
    "\n",
    "```python \n",
    "!gsutil -m rm -r {HW3_FOLDER}/stripes 2> /dev/null   ##remove old results\n",
    "stripesRDD.repartition(4).saveAsTextFile(f'{HW3_FOLDER}/stripes')  #repartition and write partitions to Google Cloud Bucket\n",
    "!gsutil ls -lh {HW3_FOLDER}/stripes \n",
    "```\n",
    "\n",
    "The above produces the following output directory: \n",
    "```\n",
    "       0 B  2022-05-31T05:35:02Z  gs://w261-jgs/main/Assignments/HW3/docker/student/stripes/\n",
    "       0 B  2022-05-31T05:35:03Z  gs://w261-jgs/main/Assignments/HW3/docker/student/stripes/_SUCCESS\n",
    "  1.74 MiB  2022-05-31T05:35:02Z  gs://w261-jgs/main/Assignments/HW3/docker/student/stripes/part-00000\n",
    "   1.6 MiB  2022-05-31T05:35:01Z  gs://w261-jgs/main/Assignments/HW3/docker/student/stripes/part-00001\n",
    "  1.58 MiB  2022-05-31T05:35:02Z  gs://w261-jgs/main/Assignments/HW3/docker/student/stripes/part-00002\n",
    "  1.52 MiB  2022-05-31T05:35:01Z  gs://w261-jgs/main/Assignments/HW3/docker/student/stripes/part-00003\n",
    "TOTAL: 6 objects, 6745577 bytes (6.43 MiB)\n",
    "```\n",
    "\n",
    "The following code displays the stripe of cooccurence words for the term `sea`:\n",
    "\n",
    "```python\n",
    "!gsutil cat {HW3_FOLDER}/stripes/part-00000|head -n 1\n",
    "```\n",
    "```python\n",
    "('sea', {'sweeping', 'twisted', 'athenians', 'fog', 'tumult', 'repression', 'morphology', 'jane', 'secreted', 'tents', 'barred', 'sadness', 'hamlet', 'turbulent', 'rains', 'robe', 'imagery', 'myths', 'orient', 'intervening', 'victories', 'accumulate', 'sinners', 'constancy', 'strained', 'sermons', 'shoe', 'trembled', 'merged', 'eastward', 'avoidance', 'sensibility', 'informing', 'silently', 'dip', 'surround', 'blocked', 'voyages', 'bursting', 'vastly', 'southeastern', 'cracks', 'tore', 'temperament', \"ship's\", 'odor', 'atlas', 'matthew', 'ether', 'colonization', 'irresistible', 'shells', 'alaska', 'gaza', 'distributions', 'farthest', 'silly', 'flush', 'ugly', 'transparent', 'arabian', 'sandy', 'steering', 'penetrating', 'burns', 'norway', 'thames', 'moonlight', 'plunge', 'beset', 'yielding', 'tuesday', 'impacts', 'cheese', 'convex', 'armistice', 'polished', 'freshness', 'belgium', 'saturation', 'dumb', 'spoil', 'shines', 'sunset', 'softly', 'laden', 'realms', 'alexandria', 'parallels', 'weep', 'ushered', 'violently', 'expanse', 'travellers', 'insoluble', 'downs', 'roofs', 'filtered', 'ashore', 'graces', 'obscured', 'establishments', 'traversed', 'crystalline', 'warmer', 'skins', 'viewing', 'fascination', 'liverpool', 'contamination', 'sails', 'masculine', 'usages', 'bucket', 'dipped', 'dew', 'fare', 'overlooking', 'necks', 'sticks', 'weighing', 'danube', 'mast', 'phosphorus', 'mate', 'attested', 'anonymous', 'wax', 'finishing', 'parked', 'flocks', 'humidity', 'endurance', 'terrors', 'carpet', 'misfortunes', 'hydroxide', 'crazy', 'priesthood', 'hungary', 'nova', 'believeth', 'remotest', 'occupants', 'complexion', 'floors', 'stationary', 'provoked', 'osmotic', 'spoils', 'clearance', 'hangs', 'openings', 'halfway', 'inorganic', 'nursery', 'vigilance', 'conqueror', 'ft', 'feathers', 'roses', 'emblem', 'lawn', 'damp', 'switzerland', 'drinks', 'contradictory', 'drained', 'ordinances', 'captains', 'barren', 'steamer', 'pursuits', 'storms', 'wasting', 'frankly', 'sequences', 'pitched', 'aggravated', 'viceroy', 'leaped', 'cunning', 'simon', 'marching', 'lends', 'sherman', 'centered', 'genome', 'iran', 'sued', 'imputed', 'perilous', 'desperately', 'southward', 'maiden', 'unusually', 'crosses', 'revealing', 'uppermost', 'remission', 'inherit', 'sunny', 'ink', 'restless', 'lighting', 'serpent', 'scarlet', 'hebrews', 'flourish', 'terminology', 'bidding', 'autobiography', 'despise', 'signification', 'preparatory', 'radioactive', 'drying', 'persia', 'unfamiliar', 'twist', 'fiery', 'boon', 'delights', 'commonest', 'bounty', 'traders', 'whoever'})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4c83537a-cc0c-4629-9aa1-eaefeb9bfa46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - save your full stripes to file for ease of retrival later... Please run code as is \n",
    "!gsutil -m rm -r {HW3_FOLDER}/stripes 2> /dev/null   ##remove old results\n",
    "stripesRDD.repartition(4).saveAsTextFile(f'{HW3_FOLDER}/stripes')  #repartition and write partitions to Google Cloud Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - list all partitions in the saved output folder... (RUN THIS CELL AS IS)\n",
    "!gsutil ls -lh {HW3_FOLDER}/stripes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - display word stripes for a couple of terms... (RUN THIS CELL AS IS)\n",
    "!gsutil cat {HW3_FOLDER}/stripes/part-00000|head -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "755130f1-1af3-472e-bd74-3435b7e2939f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 8: Synonym Detection\n",
    "\n",
    "We're now ready to perform the main synonym detection analysis. In the tasks below you will compute cosine, jaccard, dice and overlap similarity measurements for each pair of words in our vocabulary and then sort your results to find the most similar pairs of words in this dataset. __`IMPORTANT:`__ When you get to the sorting step please __sort on cosine similarity__ only, so that we can ensure consistent results from student to student. \n",
    "\n",
    "Remember to test each step of your work with the small files before running your code on the full dataset. This is a computationally intense task: well designed code can be the difference between a 20min job and a 2hr job. __`NOTE:`__ _as you are designing your code you may want to review questions 3 and 4 where we modeled some of the key pieces of this analysis._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "755130f1-1af3-472e-bd74-3435b7e2939f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#![image.png](attachment:63d84d3a-fb1d-40c9-a105-34bce2edfb75.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "755130f1-1af3-472e-bd74-3435b7e2939f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q8 Tasks:\n",
    "\n",
    "* __a1) multiple choice on Canvas:__ In question 7 you wrote a function that would create word stripes for each `term` in our vocabulary. These word stripes are essentially an 'embedded representation' of the `term`'s meaning. What is the 'feature space' for this representation?\n",
    "\n",
    "* __a2) multiple choice on Canvas:__ In question 7 you wrote a function that would create word stripes for each `term` in our vocabulary. These word stripes are essentially an 'embedded representation' of the `term`'s meaning.  What is the maximum length of a stripe?\n",
    "\n",
    "* __b) multiple choice on Canvas:__ Remember that we are going to treat these stripes as 'documents' and perform similarity analysis on them. The first step is to emit postings which then get collected to form an 'inverted index.'  How many rows will there be in our inverted index?\n",
    "\n",
    "* __c) short response:__ In the demo from question 2, we were able to compute the cosine similarity directly from the stripes (we did this using their vector form, but could have used the list instead). So why do we need the inverted index?\n",
    "\n",
    "* __d) code:__ Write a spark job that does the following:\n",
    "  * loops over the stripes from Q7 and emits postings for the `term` _(key:term, value:posting)_   \n",
    "  * aggregates the postings to create an inverted index _(key:term, value:list of postings)_\n",
    "  * loops over all pairs of `term`s that appear in the same postings list and emits co-occurrence counts\n",
    "  * aggregates co-occurrences _(key:word pair, value:count + other payload)_\n",
    "  * uses the counts (along with the accompanying information) to compute the cosine, jacard, dice and overlap similarity metrics for each pair of words in the vocabulary \n",
    "  * retrieve the top 20 and bottom 20 most/least similar pairs of words\n",
    "  * also return the cached sorted RDD for use in the next question  \n",
    "  __`NOTE 1`:__ _Don't forget to include the stripe length when you are creating the postings & co-occurrence pairs. A composite key is the way to go here._  \n",
    "  __`NOTE 2`:__ _Please make sure that your final results are sorted according to cosine similarity otherwise your results may not match the expected result & you will be marked wrong._\n",
    "  \n",
    "* __e) code:__ Comment on the quality of the \"synonyms\" your analysis comes up with. Do you notice anything odd about these pairs of words? Discuss at least one idea for how you might go about improving on the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c04f1c10-c494-46fa-9c84-7eefb4036894",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q8 Student Answers:\n",
    "> __a)__ Type your answer here!\n",
    "\n",
    "> __b)__ Type your answer here!\n",
    "\n",
    "> __c)__ Type your answer here!\n",
    "\n",
    "> __e)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b56bac93-522f-4c29-8895-3b4844720edb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# helper function for pretty printing (RUN THIS CELL AS IS)\n",
    "def displayOutput(lines):\n",
    "    template = \"{:25}|{:6}, {:7}, {:7}, {:5}\"\n",
    "    print(template.format(\"Pair\", \"Cosine\", \"Jaccard\", \"Overlap\", \"Dice\"))\n",
    "    for pair, scores in lines:\n",
    "        scores = [round(s,4) for s in scores]\n",
    "        print(template.format(pair, *scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "97a29461-b203-49cb-8bc1-16127644446c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__`TIP:`__ Feel free to define helper functions within the main function to help you organize your code. Readability is important! Eg:\n",
    "```\n",
    "def similarityAnlysis(stripesRDD):\n",
    "    \"\"\"main docstring\"\"\"\n",
    "    \n",
    "    simScoresRDD, top_n, bottom_n = None, None, None\n",
    "    \n",
    "    ############ YOUR CODE HERE ###########\n",
    "    def helper1():\n",
    "        \"\"\"helper docstring\"\"\"\n",
    "        return x\n",
    "        \n",
    "    def helper2():\n",
    "        \"\"\"helper docstring\"\"\"\n",
    "        return x\n",
    "        \n",
    "    # main spark job starts here\n",
    "    \n",
    "        ...etc\n",
    "    ############ (END) YOUR CODE ###########\n",
    "    return simScoresRDD, top_n, bottom_n\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c654dc13-824b-4905-920f-d11443f3d074",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - write your spark job in the space provided\n",
    "def similarityAnalysis(stripesRDD, n):\n",
    "    \"\"\"\n",
    "    This function defines a Spark DAG to compute cosine, jaccard, \n",
    "    overlap and dice scores for each pair of words in the stripes\n",
    "    provided. \n",
    "    \n",
    "    Output: an RDD, a list of top n, a list of bottom n\n",
    "    \"\"\"\n",
    "    simScoresRDD, top_n, bottom_n = None, None, None\n",
    "    \n",
    "    ############### YOUR CODE HERE ################\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############### (END) YOUR CODE ##############\n",
    "    return result, top_n, bottom_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d0eb2822-463a-462a-b71f-f5f84b340b45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\n",
    "start = time.time()\n",
    "testResult, top_n, bottom_n = similarityAnalysis(testStripesRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "# Wall time: 1.4768586158752441 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "89790631-03c5-4fad-a582-ae8102ef6f9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\n",
    "start = time.time()\n",
    "f1Result, top_n, bottom_n = similarityAnalysis(f1StripesRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "# Wall time: 1.9845571517944336 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a770823e-2656-4db3-acf3-cdfc5901de42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "displayOutput(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d7c73eab-154b-49db-9e52-83c6c6a8a8ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "displayOutput(bottom_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "98d0be65-e606-44ab-ac0b-2df0cd9dce3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\n",
    "start = time.time()\n",
    "result, top_n, bottom_n = similarityAnalysis(stripesRDD, 20)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "# Command took 14 minutes -- May 30, 2022 on GC n1-std-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e276d49-96f1-4f6a-8858-00d804989777",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "displayOutput(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c0d9efbd-b518-4bda-98a2-5bb5f1117f2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "displayOutput(bottom_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "09c78c64-6d5e-4ae0-b098-b7ac7d02830a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__Expected output f1RDD:__  \n",
    "<table>\n",
    "<th>MOST SIMILAR:</th>\n",
    "<th>LEAST SIMILAR:</th>\n",
    "<tr><td><pre>\n",
    "Pair                     |Cosine, Jaccard, Overlap, Dice \n",
    "commentary - lady        |   1.0,     1.0,     1.0,   1.0\n",
    "commentary - toes        |   1.0,     1.0,     1.0,   1.0\n",
    "commentary - reply       |   1.0,     1.0,     1.0,   1.0\n",
    "curious - tone           |   1.0,     1.0,     1.0,   1.0\n",
    "curious - lady           |   1.0,     1.0,     1.0,   1.0\n",
    "curious - owe            |   1.0,     1.0,     1.0,   1.0\n",
    "lady - tone              |   1.0,     1.0,     1.0,   1.0\n",
    "reply - tone             |   1.0,     1.0,     1.0,   1.0\n",
    "lady - toes              |   1.0,     1.0,     1.0,   1.0\n",
    "lady - reply             |   1.0,     1.0,     1.0,   1.0\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "\n",
    "Pair                     |Cosine, Jaccard, Overlap, Dice \n",
    "part - time              |0.0294,  0.0149,  0.0303, 0.0294\n",
    "time - upon              |0.0314,  0.0159,  0.0345, 0.0312\n",
    "time - two               |0.0314,  0.0159,  0.0345, 0.0312\n",
    "made - time              |0.0325,  0.0164,   0.037, 0.0323\n",
    "first - time             |0.0338,  0.0169,    0.04, 0.0333\n",
    "new - time               |0.0352,  0.0175,  0.0435, 0.0345\n",
    "part - us                |0.0355,  0.0179,  0.0417, 0.0351\n",
    "little - part            |0.0355,  0.0179,  0.0417, 0.0351\n",
    "made - two               |0.0357,  0.0182,   0.037, 0.0357\n",
    "made - upon              |0.0357,  0.0182,   0.037, 0.0357\n",
    "</pre></td></tr>\n",
    "</table>\n",
    "\n",
    "__Expected output dataRDD:__  \n",
    "<table>\n",
    "<th>Most Similar</th>\n",
    "<th>Least Similar</th>\n",
    "<tr><td><pre>\n",
    "Pair                     |Cosine, Jaccard, Overlap, Dice \n",
    "first - time             |  0.89,  0.8012,  0.9149, 0.8897\n",
    "time - well              |0.8895,   0.801,   0.892, 0.8895\n",
    "great - time             | 0.875,  0.7757,   0.925, 0.8737\n",
    "part - well              | 0.874,  0.7755,  0.9018, 0.8735\n",
    "first - well             |0.8717,  0.7722,  0.8936, 0.8715\n",
    "part - time              |0.8715,  0.7715,  0.9018, 0.871\n",
    "time - upon              |0.8668,   0.763,  0.9152, 0.8656\n",
    "made - time              | 0.866,  0.7619,  0.9109, 0.8649\n",
    "made - well              |0.8601,  0.7531,  0.9022, 0.8592\n",
    "time - way               |0.8587,  0.7487,  0.9259, 0.8563\n",
    "great - well             |0.8526,  0.7412,  0.8988, 0.8514\n",
    "time - two               |0.8517,  0.7389,  0.9094, 0.8498\n",
    "first - great            |0.8497,  0.7381,  0.8738, 0.8493\n",
    "first - part             |0.8471,  0.7348,  0.8527, 0.8471\n",
    "great - upon             |0.8464,  0.7338,  0.8475, 0.8464\n",
    "upon - well              |0.8444,   0.729,   0.889, 0.8433\n",
    "new - time               |0.8426,   0.724,  0.9133, 0.8399\n",
    "first - two              |0.8411,  0.7249,  0.8737, 0.8405\n",
    "way - well               |0.8357,  0.7146,  0.8986, 0.8335\n",
    "time - us                |0.8357,  0.7105,  0.9318, 0.8308\n",
    "\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "Pair                     |Cosine, Jaccard, Overlap, Dice \n",
    "region - write           |0.0067,  0.0032,  0.0085, 0.0065\n",
    "relation - snow          |0.0067,  0.0026,  0.0141, 0.0052\n",
    "cardiac - took           |0.0074,  0.0023,  0.0217, 0.0045\n",
    "ever - tumor             |0.0076,   0.002,  0.0263, 0.004\n",
    "came - tumor             |0.0076,   0.002,  0.0263, 0.004\n",
    "let - therapy            |0.0076,   0.003,  0.0161, 0.0059\n",
    "related - stay           |0.0078,  0.0036,  0.0116, 0.0072\n",
    "factors - hear           |0.0078,  0.0039,  0.0094, 0.0077\n",
    "implications - round     |0.0078,  0.0033,  0.0145, 0.0066\n",
    "came - proteins          |0.0079,   0.002,  0.0286, 0.0041\n",
    "population - window      |0.0079,  0.0039,    0.01, 0.0077\n",
    "love - proportional      | 0.008,  0.0029,  0.0185, 0.0058\n",
    "got - multiple           | 0.008,  0.0034,  0.0149, 0.0067\n",
    "changes - fort           |0.0081,  0.0032,  0.0161, 0.0065\n",
    "layer - wife             |0.0081,  0.0038,  0.0119, 0.0075\n",
    "five - sympathy          |0.0081,  0.0034,  0.0149, 0.0068\n",
    "arrival - essential      |0.0081,   0.004,  0.0093, 0.008\n",
    "desert - function        |0.0081,  0.0031,  0.0175, 0.0062\n",
    "fundamental - stood      |0.0081,  0.0038,  0.0115, 0.0077\n",
    "patients - plain         |0.0081,   0.004,  0.0103, 0.0079\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "63c40693-e756-4efd-985a-7db35c58af83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Congratulations, you have completed HW3! Please refer to the readme for submission instructions.\n",
    "\n",
    "If you would like to provide feedback regarding this homework, please use the survey at: https://docs.google.com/forms/d/e/1FAIpQLSce9feiQeSkdP43A0ZYui1tMGIBfLfzb0rmgToQeZD9bXXX8Q/viewform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0f095b6f-0866-4e3e-91ec-7ae65e5fa9d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "hw3_Workbook_revF20",
   "notebookOrigID": 2162291293013809,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
