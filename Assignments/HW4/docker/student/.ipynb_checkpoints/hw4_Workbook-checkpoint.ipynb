{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2f5235b9-a2b4-4fd3-a148-7e8fc65676c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# HW 4 - Supervised Learning at Scale.\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__\n",
    "\n",
    "In the first three homeworks you became familiar with the Map-Reduce programming paradigm as manifested in the Hadoop Streaming and Spark frameworks. We explored how different data structures and design patterns can help us manage the computational complexity of an algorithm. As part of this process you implemented both a supervised learning alogorithm (Naive Bayes) and an unsupervised learning algorithm (synonym detection via cosine similarity). In both of these tasks parallelization helped us manage calculations involving a large number of features. However a large feature space isn't the only situation that might prompt us to want to parallelize a machine learning algorithm. In the final two assignments we'll look at cases where the iterative nature of an algorithm is the main driver of its computational complexity (and the reason we might want to parallelize it).\n",
    "\n",
    "In this week's assignment we'll perform 3 kinds of linear regression: OLS, Ridge and Lasso. As in previous assignments you will implement the core calculations using Spark RDDs... though we've provided more of a code base than before since the focus of the latter half of the course is more on general machine learning concepts. By the end of this homework you should be able to:  \n",
    "* ... __define__ the loss functions for OLS, Ridge and Lasso regression.\n",
    "* ... __calculate__ the gradient for each of these loss functions.\n",
    "* ... __identify__ which parts of the gradient descent algorithm can be parallelized.\n",
    "* ... __implement__ parallelized gradient descent with cross-validation and regularization.\n",
    "* ... __compare/contrast__ how L1 and L2 regularization impact model parameters & performance.\n",
    "\n",
    "Additional Reference: [Spark 3.0.0 Documentation - RDD programming guide](https://spark.apache.org/docs/3.0.0/rdd-programming-guide.html)\n",
    "\n",
    "__Please refer to the `README` for homework submission instructions and additional resources.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9799fd7a-8875-4ee2-aa5a-1de4b1f99173",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Notebook Set-Up\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "336dd213-f583-4fbe-9c90-cb86ba100fd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "336dd213-f583-4fbe-9c90-cb86ba100fd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "336dd213-f583-4fbe-9c90-cb86ba100fd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw4_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d866dc44-c7b2-4913-9858-8cb2e14a7f7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://docker.w261:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>hw4_notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0538cc6810>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b7b227ac-5c1e-4399-9e8b-e0106dcc9c75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 1: Opimization Theory \n",
    "\n",
    "As you know from w207, Gradient Descent is an iterative process that seeks to find the optimal parameters for a model given a particular training data set. It does this by using the vector of partial derivatives of a loss function to strategically update parameters in a way that will reduce the loss. In live session 6 you discussed some of the theory behnid why gradient descent works and looked at a small example of gradient descent in the context of linear regression.\n",
    "\n",
    "### Q1 Tasks:\n",
    "\n",
    "* __a) short response:__ What are the first and second order conditions for convexity and why do we care about them when performing Gradient Descent?\n",
    "\n",
    "* __b) short response:__ Explain the relationship between problem domain space and model parameter space in the context of Gradient Descent. In practice, why can't we find the optimal model by simply looking at the error surface in model parameter space?\n",
    "\n",
    "* __c) short response:__ In the context of Gradient Descent, what is the 'learning rate' and what are the tradeoffs associated with setting this hyperparameter?\n",
    "\n",
    "* __d) BONUS:__ In the context of OLS, what do we mean by a 'closed form solution' and why is it not scalable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6472d5e7-29a1-420f-ba74-bd855e049de2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q1 Student Answers:\n",
    "> __a)__ Type your answer here!\n",
    "\n",
    "> __b)__ Type your answer here!\n",
    "\n",
    "> __c)__ Type your answer here!  \n",
    "\n",
    "> __d)__ Type your (optional bonus) answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "53c7548b-a503-49cc-826d-6519e1408b10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# About the Data\n",
    "\n",
    "For the main task in this portion of the homework you will use data about red and white Portuguese wines. [This data](http://archive.ics.uci.edu/ml/datasets/Wine+Quality) was made available to the UC Irvine public repository of Machine Learning datasets by researchers at the University of Minho in association with [this paper](https://www.sciencedirect.com/science/article/pii/S0167923609001377?via%3Dihub):\n",
    "> P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n",
    "Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n",
    "\n",
    "The dataset includes 12 fields:\n",
    ">`fixed acidity`  \n",
    "`volatile acidity`  \n",
    "`citric acid`  \n",
    "`residual sugar`  \n",
    "`chlorides`  \n",
    "`free sulfur dioxide`  \n",
    "`total sulfur dioxide`  \n",
    "`density`  \n",
    "`pH`  \n",
    "`sulphates`  \n",
    "`alcohol`  \n",
    "`quality`   -- (_a score between 0 and 10_)\n",
    "\n",
    "__`IMPORTANT NOTE:`__ The outcome variable in our data is a human assigned score ranging from 0 to 10. Since the scores are integers this is actually an ordinal and not numerical outcome varaible. However for the purposes of this assignment we'll treat it as a numerical quantity.\n",
    "\n",
    "The data are in two files: one containing red wines and another containing white wines.  Use the following cells to download the data, add a field for red/white, and split it into a test and train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0bd4f45e-b0ab-4a11-8b75-5ed7ecfda13f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# make a data folder & download red & white wine files\n",
    "!mkdir data\n",
    "!wget -q -O data/reds.csv http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n",
    "!wget -q -O data/whites.csv http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a01b6070-6862-4152-a7c9-8da61b79d84c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# extract header fields - RUN THIS CELL AS IS\n",
    "header = !head -n 1 data/reds.csv\n",
    "header = header[0]\n",
    "FIELDS = ['color'] + re.sub('\"', '', header).split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "70cc1772-2983-40bf-a64c-77140a164817",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the raw data into an RDD - RUN THIS CELL AS IS\n",
    "redsRDD = sc.textFile('data/reds.csv')\\\n",
    "            .filter(lambda x: x != header)\\\n",
    "            .map(lambda x: '1;' + x) # set first field 1 to indicate red wine\n",
    "whitesRDD = sc.textFile('data/whites.csv')\\\n",
    "              .filter(lambda x: x != header)\\\n",
    "              .map(lambda x: '0;' + x) # set first field 0 to indicate white wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a4275958-4cf3-4806-a9f3-d4a7b859c20b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1;7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5',\n",
       " '1;7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5',\n",
       " '1;7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;9.8;5',\n",
       " '1;11.2;0.28;0.56;1.9;0.075;17;60;0.998;3.16;0.58;9.8;6',\n",
       " '1;7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eaf4de86-27f1-405c-8388-90b1c9e69b30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0;7;0.27;0.36;20.7;0.045;45;170;1.001;3;0.45;8.8;6',\n",
       " '0;6.3;0.3;0.34;1.6;0.049;14;132;0.994;3.3;0.49;9.5;6',\n",
       " '0;8.1;0.28;0.4;6.9;0.05;30;97;0.9951;3.26;0.44;10.1;6',\n",
       " '0;7.2;0.23;0.32;8.5;0.058;47;186;0.9956;3.19;0.4;9.9;6',\n",
       " '0;7.2;0.23;0.32;8.5;0.058;47;186;0.9956;3.19;0.4;9.9;6']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitesRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4ec5e621-0fc0-4e8b-8cf9-ce6476201e25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... held out 1316 records for evaluation and assigned 5181 for training.\n"
     ]
    }
   ],
   "source": [
    "# Generate 80/20 (pseudo)random train/test split - RUN THIS CELL AS IS\n",
    "trainRDD, heldOutRDD = redsRDD.union(whitesRDD).randomSplit([0.8,0.2], seed = 1)\n",
    "print(f\"... held out {heldOutRDD.count()} records for evaluation and assigned {trainRDD.count()} for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6c0da7af-2c2e-4c6d-97f3-10c4720079bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# helper function - RUN THIS CELL AS IS\n",
    "def parse(line):\n",
    "    \"\"\"\n",
    "    Map record_csv_string --> (tuple,of,fields)\n",
    "    \"\"\"\n",
    "    fields = np.array(line.split(';'), dtype = 'float')\n",
    "    features,quality = fields[:-1], fields[-1]\n",
    "    return(features, quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d9445952-22a9-4759-a10d-48b07345b55c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# cache the training set - RUN THIS CELL AS IS \n",
    "trainRDDCached = trainRDD.map(parse).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fc3912d0-f86d-4229-be9d-b384c121fbe9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 2: EDA\n",
    "\n",
    "A statistician's approach to Linear Regression typically involves a series of EDA steps to examine each feature in the data and then a series of steps to test assumptions about their potential contribution to a multi-feature linear model. In particular, we'd want to look for a set of features that exhibit a likely linear relationship with the outcome variable and that are _not_ highy correlated with each other. In the context of machine learning, these considerations remain important techniques for improving model generalizability despite the common practice to use model evaluation techniques (and large data sets) to get the final word on feature selection. \n",
    "\n",
    "In this question we'll briefly look at the features in our data set. To mimic an 'at scale' analysis we'll start by sampling from our Spark RDD training set so that we have a manageable amount of data to work with in our visuals.\n",
    "\n",
    "### Q2 Tasks:\n",
    "* __a) short response:__ Run the provided code to sample 1000 points and visualize histograms of each feature. Comment on the distributions you observe (eg. _Which features appear normaly distributed, which don't? Which features vary most/least?_) How is the varaible `color` different than the other features & what does that mean about how we interpret its regression coefficient?\n",
    "\n",
    "* __b) short response:__ Run the provided code to create boxplots of each feature. Which, if any, appear to have a positive linear relationship with `quality`? Which if any appear to have a negative linear relationship with `quality`?\n",
    "\n",
    "\n",
    "* __c) short response:__ Run the provided code to plot the correlations matrix. Which pairs of features are most _strongly_ (postively or negatively) associated with each other? What implications would that have for our feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e91f5427-d635-406a-a3ab-bcdca13d16bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q2 Student Answers:\n",
    "> __a)__ Type your answer here!  \n",
    "\n",
    "> __b)__ Type your answer here!  \n",
    "\n",
    "> __c)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8c048484-8fd0-499f-98a0-7e631e2d544b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part a - take a 1000 point sample for EDA (RUN THIS CELL AS IS)\n",
    "sample = np.array(trainRDDCached.map(lambda x: np.append(x[0], [x[1]]))\n",
    "                                .takeSample(False, 1000))\n",
    "sample_df = pd.DataFrame(np.array(sample), columns = FIELDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be0343d0-70f9-46ca-8a06-dc9e76ed155c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part a - take a look at histograms for each feature (RUN THIS CELL AS IS)\n",
    "sample_df[FIELDS[:-1]].hist(figsize=(15,15), bins=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e38cc08f-ac61-4783-9fe5-4149f5678e73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part b -  plot boxplots of each feature vs. the outcome (RUN THIS CELL AS IS)\n",
    "fig, ax_grid = plt.subplots(4, 3, figsize=(15,15))\n",
    "y = sample_df['quality']\n",
    "for idx, feature in enumerate(FIELDS[:-1]):\n",
    "    x = sample_df[feature]\n",
    "    sns.boxplot(x, y, ax=ax_grid[idx//3][idx%3], orient='h', linewidth=.5)\n",
    "    ax_grid[idx//3][idx%3].invert_yaxis()\n",
    "fig.suptitle(\"Individual Features vs. Outcome (qualilty)\", fontsize=15, y=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "10d7c4c2-2f27-425e-9a20-4f1cd73303d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot heatmap for correlations matrix - RUN THIS CELL AS IS\n",
    "corr = sample_df[FIELDS[:-1]].corr()\n",
    "fig, ax = plt.subplots(figsize=(11, 9))\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "cmap = sns.diverging_palette(240, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=.5)\n",
    "plt.title(\"Correlations between features.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da631eaf-d3db-4f74-b94e-580d2cf4817a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 3: OLS Loss\n",
    "\n",
    "For a parametric model, the key factor that will impact how easy it is to optimize is your choice of how to define the loss function. In Ordinary Least Squares (OLS) Regression our loss function is just about as convenient as you will get: not only is it convex, its also very easy to interpret. \n",
    "\n",
    "When doing supervised learning, a simple sanity check consists of comparing one’s estimator against simple rules of thumb. It is useful as a simple baseline to compare with other (real) regressors. Examples of regression baselines include:\n",
    "* \"mean\": always predicts the mean of the training set\n",
    "* \"median\": always predicts the median of the training set\n",
    "* \"quantile\": always predicts a specified quantile of the training set,provided with the quantile parameter.\n",
    "* \"constant\": always predicts a constant value that is provided by the user.\n",
    "\n",
    "In this question you'll \"choose\" a baseline model and then write a function to compute the loss of a linear model in Spark. You'll reuse this function in Q4 when you implement gradient descent.\n",
    "\n",
    "#### Baseline example illustrated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f5942c8d-87c5-4fe4-a9bd-035e7caaf02c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# points from our mini example from the demo 6 notebook (RUN THIS CELL AS IS)\n",
    "points = np.array([[1,2],[3,4],[5,5],[4,3],[2,3]])\n",
    "x = points[:,0]\n",
    "y = points[:,1]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y,'o', label='data points')\n",
    "plt.axhline(np.mean(y),c='r', label='\"mean\" model')\n",
    "plt.title('Example of \"mean\" baseline model')\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "27d79c61-c4eb-4ddf-838c-59f3e10ff840",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q3 Tasks:\n",
    "* __a) code:__ Fill in the code below to compute the mean and variance of your outcome variable. [__`HINT:`__ _use `trainRDDCached` as the input & feel free to use Spark built-in functions._]\n",
    "\n",
    "\n",
    "* __b) short response:__ Write the formula for the OLS loss function and explain how to interpret it graphically.\n",
    "\n",
    "\n",
    "* __c) short response:__ In the context of linear models & vector computations what does it mean to 'augment' a data point and why do we do this?\n",
    "\n",
    "\n",
    "* __d) code + short response:__ Fill in the missing code to complete the`OLSLoss` function. Is computing the loss \"embarassingly parallel'? Explain. [__`TIP:`__ Use `augmentedData` as your input when computing the loss.]\n",
    "\n",
    "* __e) code + short response:__ Fill in the missing code to define a baseline model for this data set that has a bias term equal to the mean of your outcome variable and `0.0` for all coefficients. Note that in the docstring for `OLSLoss` we specified that the model should be a numpy array with the bias in the first position. Once you've defined your model, run the provided cells to check that your model has the correct dimensions and then compute the loss for your baseline model. Compare your results to the result you got in `part a` and explain what you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1d514045-6cc7-4dfd-a3ff-ab581277cddb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q3 Student Answers:\n",
    "> __b)__ Type your answer here!  \n",
    "\n",
    "> __c)__ Type your answer here!  \n",
    "\n",
    "> __d)__ Type your answer here!  \n",
    "\n",
    "> __e)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9617c461-d2e9-44f5-8637-2d06880e3de6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: None\n",
      "Variance: None\n"
     ]
    }
   ],
   "source": [
    "# part a - mean and variance of the outcome variable \n",
    "meanQuality = None # FILL IN YOUR CODE HERE\n",
    "varQuality = None # FILL IN YOUR CODE HERE\n",
    "print(f\"Mean: {meanQuality}\")\n",
    "print(f\"Variance: {varQuality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "94d09af5-bb09-4cdd-bcbe-0c0f0a621a39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - write function to compute loss (FILL IN MISSING CODE BELOW)\n",
    "def OLSLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute mean squared error.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    ################## YOUR CODE HERE ##################\n",
    "    loss = None\n",
    "    ################## (END) YOUR CODE ##################\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "14982bbd-8436-4a3a-971c-32a1f7287cd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part e - define your baseline model here\n",
    "BASELINE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4d315379-a192-4d6a-99fa-230a554e9081",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part e - compute the loss for your baseline model (RUN THIS CELL AS IS)\n",
    "assert len(BASELINE) == len(trainRDDCached.take(1)[0][0]) + 1, \"Double check model dimensions\"\n",
    "print(f\"Baseline model loss: {OLSLoss(trainRDDCached, BASELINE)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "345a836b-ae3f-4270-a302-53a9d63dcc0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 4: Vanilla Gradient Descent\n",
    "\n",
    "Performing Gradient Descent technically only requires two steps: 1) _use the current model to calculate the gradient_; 2) _use the gradient to update the current model parameters_. In practice though, we'll want to add a third step which is to compute the loss for our new model so that we can see if its working. In this question you'll implement gradient descent for OLS regression and take a look at a few update steps.\n",
    "\n",
    "### Q4 Tasks:\n",
    "* __a) short response:__ Jimi describes the main part of the gradient calculation for OLS Regression using a short mantra: _'the mean of the data weighted by the errors'_. . Write the formula for the gradient and explain how it reflects this phrase. \n",
    "\n",
    "* __b) short response:__ Looking at the formula you wrote in `part a`, what parts of this calculation can be parallelized and what has to happen after reducing?\n",
    "\n",
    "\n",
    "* __c) code:__ Fill in the missing lines in `GDUpdate` to compute the gradient and perform a single update of the model parameters.   \n",
    "    * __`TIP 1:`__ _remember that the gradient is a vector of partial derivatives, `grad` should be a numpy array_    \n",
    "    * __`TIP 2:`__ _Spark's built in `mean()` function may help you here_  \n",
    "\n",
    "\n",
    "* __d) short response:__ Run the provided code to perform 5 steps of Gradient Descent on our data. What is wrong with these results?\n",
    "\n",
    "\n",
    "* __e) code + short response:__ Fill in the missing code in `normalize` so that this function scales each feature and centers it at 0. Then use the provide code block to rerun your same gradient descent code on the scaled data. Use these results to explain what the problem was in 'd'.\n",
    "    * __`TIP:`__ _You may find [this brief illustration](https://www.coursera.org/lecture/machine-learning/gradient-descent-in-practice-i-feature-scaling-xx3Da) from Andrew Ng's Coursera helpful._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b6644459-75c7-4b20-bd8c-c043d8286532",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q4 Student Answers:\n",
    "> __a)__ Type your answer here!  \n",
    "\n",
    "> __b)__ Type your answer here! \n",
    "\n",
    "> __c)__ _complete the coding portions of this question before answering d & e_ \n",
    "\n",
    "> __d)__ Type your answer here!  \n",
    "\n",
    "> __e)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c7f11a8-c529-4fe1-9b94-9138cdff9ad0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part b - function to perform a single GD step\n",
    "def GDUpdate(dataRDD, W, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one OLS gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    ################## YOUR CODE HERE ################# \n",
    "    grad = None\n",
    "    new_model = None\n",
    "    ################## (END) YOUR CODE ################# \n",
    "   \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b76eba8f-cfe0-4d77-9d78-ba64357b69cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # part c - take a look at a few Gradient Descent steps (RUN THIS CELL AS IS)\n",
    "\n",
    "nSteps = 5\n",
    "model = BASELINE\n",
    "# print(f\"BASELINE:  Loss = {OLSLoss(trainRDDCached,model)}\")\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    model = GDUpdate(trainRDDCached, model)\n",
    "#     loss = OLSLoss(trainRDDCached, model)\n",
    "#     print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(w,3) for w in model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bebf2cfd-ecbe-4d74-bba5-23eaf95f5ded",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - helper function to normalize the data (FILL IN THE MISSING CODE BELOW)\n",
    "def normalize(dataRDD):\n",
    "    \"\"\"\n",
    "    Scale and center data round mean of each feature.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "    Returns:\n",
    "        normedRDD - records are tuples of (features_array, y)\n",
    "    \"\"\"\n",
    "    featureMeans = dataRDD.map(lambda x: x[0]).mean()\n",
    "    featureStdev = np.sqrt(dataRDD.map(lambda x: x[0]).variance())\n",
    "    \n",
    "    ################ YOUR CODE HERE #############\n",
    "    normedRDD = None\n",
    "    ################ FILL IN YOUR CODE HERE #############\n",
    "    \n",
    "    return normedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4cb99ab5-511f-4cd3-9803-5c37871d89ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - cache normalized data (RUN THIS CELL AS IS)\n",
    "normedRDD = normalize(trainRDDCached).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba2a42bc-46b4-4a63-8ccd-9637e6f3bf24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# part e - take a look at a few GD steps w/ normalized data  (RUN THIS CELL AS IS)\n",
    "nSteps = 5\n",
    "model = BASELINE\n",
    "print(f\"BASELINE:  Loss = {OLSLoss(trainRDDCached,model)}\")\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    model = GDUpdate(normedRDD, model)\n",
    "    loss = OLSLoss(normedRDD, model) \n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(w,3) for w in model]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f65a4eb5-0a92-4154-9e7f-f1ba436319df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 5: Assessing the performance of your model.\n",
    "\n",
    "Printing out the loss as we perform each gradient descent step allows us to confirm that our Gradient Descent code appears to be working, but this number doesn't accurately reflect \"how good\" our model is. In this question you'll plot error curves for a test and training set in order to discuss model performance. Note that although we split out a test & train set when we first loaded the data... in the spirit of keeping that 20% truly 'held out' until then end of the assignment, we'll make an additional split for the purposes of this question dividing the existing training set into two smaller RDDs.\n",
    "\n",
    "### Q5 Tasks:\n",
    "* __a) short response:__ Why doesn't the loss that we printed in Question 4 accurately reflect \"how good\" our model is? \n",
    "\n",
    "\n",
    "* __b) code:__ Since we're going to be running Gradient Descent a number of times let's package it into a function for convenience. Fill in the missing code in `GradientDescent()`, note that the missing code is going to look a lot like the provided code blocks in Q5 -- feel free to use those as a starting point.\n",
    "\n",
    "\n",
    "* __c) short response:__ Use the provided code to split the normalized data into a test and train set, then run 50 iterations of gradient descent and plot the MSE curves for each. Describe what you see and speculate about why this might be happening.\n",
    "\n",
    "\n",
    "* __d) short response:__ Note that passing the optional parameter `seed` to the Spark method `randomSplit` allows us to pseudo randomize our test/train split in a way that is replicable. Re-run the code for part 'c but this time in the line where we perform the `normedRDD.randomSplit` change the seed to _`seed = 5`_. What changes in the plot? Repeat for _`seed = 4`_. How does this change your interpret the results you saw in 'c'. What is the more likely explanation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "997f7ee1-7eb3-42ba-84e9-0e13cdf9b119",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q5 Student Answers:\n",
    "> __a)__ Type your answer here!  \n",
    "\n",
    "> __c)__ Type your answer here! \n",
    "\n",
    "> __d)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "96daef89-bfa6-459e-8251-6687f607718a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part b - OLS gradient descent function\n",
    "def GradientDescent(trainRDD, testRDD, wInit, nSteps = 20, \n",
    "                    learningRate = 0.1, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of OLS gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = wInit\n",
    "    for idx in range(nSteps): \n",
    "        \n",
    "        ############## YOUR CODE HERE #############\n",
    "        model = None\n",
    "        training_loss = None\n",
    "        test_loss = None\n",
    "        ############## (END) YOUR CODE #############\n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(training_loss)\n",
    "        test_history.append(test_loss)\n",
    "        model_history.append(model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {training_loss}\")\n",
    "            print(f\"test loss: {test_loss}\")\n",
    "            print(f\"Model: {[round(w,3) for w in model]}\")\n",
    "    return train_history, test_history, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9c1e18b5-4d75-45ef-a6ea-470d82cacd38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot error curves - RUN THIS CELL AS IS\n",
    "def plotErrorCurves(trainLoss, testLoss, title = None):\n",
    "    \"\"\"\n",
    "    Helper function for plotting.\n",
    "    Args: trainLoss (list of MSE) , testLoss (list of MSE)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize = (16,8))\n",
    "    x = list(range(len(trainLoss)))[1:]\n",
    "    ax.plot(x, trainLoss[1:], 'k--', label='Training Loss')\n",
    "    ax.plot(x, testLoss[1:], 'r--', label='Test Loss')\n",
    "    ax.legend(loc='upper right', fontsize='x-large')\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "778968ef-8ca1-4e5a-9d85-53ea8bc29ed7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run 50 iterations (RUN THIS CELL AS IS)\n",
    "wInit = BASELINE\n",
    "trainRDD, testRDD = normedRDD.randomSplit([0.8,0.2], seed = 2018)\n",
    "start = time.time()\n",
    "MSEtrain, MSEtest, models = GradientDescent(trainRDD, testRDD, wInit, nSteps = 50)\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4bbda5f8-d621-4604-9497-b90c16790eeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# take a look (RUN THIS CELL AS IS)\n",
    "plotErrorCurves(MSEtrain, MSEtest, title = 'Ordinary Least Squares Regression' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "989054d9-0152-409a-9da4-928105ced8a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save the models & their performance for comparison later (RUN THIS CELL AS IS)\n",
    "np.savetxt(PWD + '/data/OLSmodels.csv', np.array(models), delimiter=',')\n",
    "np.savetxt(PWD + '/data/OLSloss.csv', np.array([MSEtrain, MSEtest]), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "082ce7a4-c938-44b8-98cd-1c036f7e8890",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run 50 iterations (RUN THIS CELL AS IS)\n",
    "wInit = BASELINE\n",
    "trainRDD, testRDD = normedRDD.randomSplit([0.8,0.2], seed = 5)\n",
    "start = time.time()\n",
    "MSEtrain, MSEtest, models = GradientDescent(trainRDD, testRDD, wInit, nSteps = 50)\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "86962dd2-4586-4dac-9978-966dcb584eb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# take a look (RUN THIS CELL AS IS)\n",
    "plotErrorCurves(MSEtrain, MSEtest, title = 'Ordinary Least Squares Regression' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c6acd2a1-2338-439b-8c5f-4ecd259669e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run 50 iterations (RUN THIS CELL AS IS)\n",
    "wInit = BASELINE\n",
    "trainRDD, testRDD = normedRDD.randomSplit([0.8,0.2], seed = 4)\n",
    "start = time.time()\n",
    "MSEtrain, MSEtest, models = GradientDescent(trainRDD, testRDD, wInit, nSteps = 50)\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f84e617f-a02b-4d3d-a3e1-1215794e08bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# take a look (RUN THIS CELL AS IS)\n",
    "plotErrorCurves(MSEtrain, MSEtest, title = 'Ordinary Least Squares Regression' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ee3ca19-5dcb-425f-8654-89f66a58585b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 6: Cross Validation\n",
    "\n",
    "In question 5 we mentioned that computing the loss after each iteration is not strictly a part of Gradient Descent, its just convenient for visualizing our progress. This \"third step\" however comes with a tradeoff: it requires an extra pass through the data. Normally this would cause us to cringe except for the fact that both the loss computation and the gradient computation are very easy to parallelize - lots of the work can be done in place no shuffle needed for the aggregation. \n",
    "\n",
    "[Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)), sometimes called rotation estimation, or out-of-sample testing, is a model validation technique for assessing how well the model will generalize to an independent data set. The goal of cross-validation is to test the model's ability to predict new data. \n",
    "\n",
    "Cross validation, which will solve the problem of the unreliable test-loss that we saw in question 5, presents a bit more of a scalability challenge. To avoid over-dependence on a particulary good or bad test/train split we divide the data into `k` roughly equal size parts and train `k` models. The `k-th` model is trained on all the data _except_ the `k-th` split which is used as a test set for that model. Finally we compute the loss by averaging together the test/train loss for each model. In this question we've provided a code base to perform gradient descent and cross validation in parallel. You'll fill in some of the key details based on your understanding from questions 1-5.\n",
    "\n",
    "#### From ISLR Chapter 5.1 - Cross Validation\n",
    "<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/CV-ISLRp181.png?raw=true\">\n",
    "\n",
    "\n",
    "### Q6 Tasks:\n",
    "* __a) short response:__ A naive approach to training an OLS Regression model with cross validation might be to simply perform Gradient Descent on each of the 5 models in sequence. In this naive approach, how many total passes would be made over the data? [__`HINT:`__ _it will depend on factors that you should be able to name._]\n",
    "\n",
    "\n",
    "* __b) short response:__ Read through the provided helper function `kResiduals()` and note where it gets used in the subsequent function `CVLoss()`. For each record in the original dataset, how many tuples does `kResiduals()` emit? What are the keys of these newly emitted records? How will these keys help us compute cross validated loss?\n",
    "\n",
    "\n",
    "* __c) code:__ Complete the missing Spark code in `CVLoss()` so that this function returns the test/train cross validated error for a given set of data splits and their corresponding models. [__`TIP:`__ _your goal is to start from `partialLossRDD` and compute the test & train loss for each model so that the provided code can take the final average_].\n",
    "\n",
    "\n",
    "* __d) code:__ Read through the provided functions `partialGradients()` and `CVUpdate()`. These should have a familiar feel. Fill in the missing line in `CVUpdate()` to update each model and add the (new) array of coefficients to the `new_models` list. \n",
    "\n",
    "\n",
    "* __e) short response:__ Read `GradientDescent_withCV()` and then run the provided code to perform 50 iterations and plot the error curves. What can you conclude from this graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aa355ad6-eec8-40f1-8c76-bfe320aa7657",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q6 Student Answers:\n",
    "> __a)__ Type your answer here!  \n",
    "\n",
    "> __b)__ Type your answer here!  \n",
    "\n",
    "> __e)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a54f785c-aae5-47bf-8346-1ae48f874f4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part b - helper function to emit residuals (RUN THIS CELL AS IS)\n",
    "def kResiduals(dataPoint, models, splitNum):\n",
    "    \"\"\"\n",
    "    Compute the (squared) residuals for a data point given k different models.\n",
    "    Note that points from the k-th split are part of the test set for model number k\n",
    "    and part of the training set for all other models. We'll emit a key to track this.\n",
    "    Args:\n",
    "        dataPoint - tuple of (features_array, y)\n",
    "        models    - list of arrays representing model weights (bias at index 0)\n",
    "    Returns:\n",
    "        (stringFormattedKey, squared_error)\n",
    "    \"\"\"\n",
    "    # augment the data point with a bias term at index 0\n",
    "    X = np.append([1.0], dataPoint[0])\n",
    "    y = dataPoint[1]\n",
    "    # emit squared residuals for each model\n",
    "    for modelNum, W in enumerate(models):\n",
    "        if modelNum == splitNum:\n",
    "            yield(f\"{modelNum}-test\", (W.dot(X) - y)**2)\n",
    "        else:\n",
    "            yield(f\"{modelNum}-train\", (W.dot(X) - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6765b7f2-a243-4d96-bc33-067ac20d6b82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part c - fill in the missing code below\n",
    "def CVLoss(dataSplits, models):\n",
    "    \"\"\"\n",
    "    Compute the k-fold cross-validated test and train loss.\n",
    "    Args:\n",
    "        dataSplits - list of RDDs corresponding to the k test splits.\n",
    "        models     - list of k arrays representing model weights (bias at index 0)\n",
    "    Returns: \n",
    "        tuple of floats: (training_loss, test_loss)\n",
    "    \"\"\"\n",
    "    # compute k residuals for each dataPoint (one for each model)\n",
    "    partialLossRDD = sc.parallelize([])\n",
    "    for splitNum, splitRDD in enumerate(dataSplits):\n",
    "        residuals = splitRDD.flatMap(lambda x: kResiduals(x, models, splitNum))\n",
    "        partialLossRDD = sc.union([partialLossRDD, residuals])\n",
    "    \n",
    "    ################ YOUR CODE HERE #################        \n",
    "    loss = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ################ (END) YOUR CODE ################# \n",
    "    \n",
    "    test_loss = np.mean([x[1] for x in loss if x[0].split('-')[1] == 'test'])\n",
    "    training_loss = np.mean([x[1] for x in loss if x[0].split('-')[1] == 'train'])\n",
    "    return training_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f517e997-3621-473f-8d04-371c66ae5624",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - helper function RUN THIS CELL AS IS\n",
    "def partialGradients(splitNum, dataPoint, models):\n",
    "    \"\"\"\n",
    "    Emit partial gradient for this data point for each model.\n",
    "    NOTE: a data point from split-number k is in the test set for \n",
    "    model-k so we don't compute a partial gradient for that model.\n",
    "    \"\"\"\n",
    "    # augment the data point\n",
    "    X = np.append([1.0], dataPoint[0])\n",
    "    y = dataPoint[1]\n",
    "    # emit partial gradients for each model with a counter for averaging later\n",
    "    for modelNum, W in enumerate(models):\n",
    "        if modelNum != splitNum:\n",
    "            yield (modelNum, [(W.dot(X) - y)*X, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9f8dd6ca-1bf8-44ad-8dcd-1eced5d5dae8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - perform GD updates for all k models (FILL IN MISSING CODE BELOW)\n",
    "def CVUpdate(dataSplits, models, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Compute gradients for k models given k corresponding dataSplits.\n",
    "    NOTE: the training set for model-k is all records EXCEPT those in the k-th split.\n",
    "    \"\"\"\n",
    "    # compute partial gradient k-1 times for each fold\n",
    "    partialsRDD = sc.parallelize([])\n",
    "    for splitNum, splitRDD in enumerate(dataSplits):\n",
    "        thisFoldPartialGrads = splitRDD.flatMap(lambda x: partialGradients(splitNum, x, models))\n",
    "        partialsRDD = sc.union([partialsRDD, thisFoldPartialGrads])\n",
    "\n",
    "    # compute gradients by taking the average partialGrad for each fold\n",
    "    gradients = partialsRDD.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n",
    "                           .mapValues(lambda x: x[0]/x[1])\\\n",
    "                           .map(lambda x: x[1])\\\n",
    "                           .collect()\n",
    "    \n",
    "    # update all k models & return them in a list\n",
    "    new_models = []\n",
    "    for W, grad in zip(models, gradients):\n",
    "        ############# YOUR CODE HERE ############\n",
    "        pass\n",
    "        ############# (END) YOUR CODE ###########\n",
    "    return new_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "edc26814-f025-42da-a0a9-be9d6f22977b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part e - RUN THIS CELL AS IS\n",
    "def GradientDescent_withCV(dataSplits, wInit, learningRate=0.1, nSteps = 5, verbose = False):\n",
    "    \"\"\"\n",
    "    Train k models in parallel and track cross validated test/train loss.\n",
    "    Returns:\n",
    "        train_hist - (list) of floats\n",
    "        test_hist - (list) of floats\n",
    "        model_hist - (list) of arrays representing model coefficients (bias at index 0)\n",
    "    \"\"\"\n",
    "    # broadcast initial models (one for each fold)\n",
    "    bModels = sc.broadcast([wInit] * len(dataSplits))\n",
    "    \n",
    "    \n",
    "    # initialize lists to track performance\n",
    "    train_loss_0, test_loss_0 = CVLoss(dataSplits, bModels.value)\n",
    "    train_hist, test_hist, model_hist = [train_loss_0], [test_loss_0], [wInit]\n",
    "    \n",
    "    # perform k gradient updates at a time (one for each fold)\n",
    "    start = time.time()\n",
    "    for step in range(nSteps):\n",
    "        new_models = CVUpdate(dataSplits, bModels.value, learningRate)\n",
    "           \n",
    "        bModels = sc.broadcast(new_models)\n",
    "\n",
    "        # log progress\n",
    "        train_loss, test_loss = CVLoss(dataSplits, bModels.value)\n",
    "        train_hist.append(train_loss)\n",
    "        test_hist.append(test_loss)\n",
    "        model_hist.append(new_models[0])\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"-------------------\")\n",
    "            print(f\"STEP {step}: \")\n",
    "            print(f\"model 1: {[round(w,4) for w in new_models[0]]}\")\n",
    "            print(f\" train loss: {round(train_loss,4)}\")\n",
    "            print(f\" test loss: {round(test_loss,4)}\")\n",
    "            \n",
    "    print(f\"\\n... trained {nSteps} iterations in {time.time() - start} seconds\")\n",
    "    return train_hist, test_hist, model_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "842c3024-ed16-4e24-922b-4df751b135b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d -  run 50 iterations (RUN THIS CELL AS IS)\n",
    "dataSplits = normedRDD.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2], seed = 2018) \n",
    "wInit = BASELINE\n",
    "trainLoss, testLoss, models = GradientDescent_withCV(dataSplits, wInit, learningRate=0.1, nSteps = 50, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3c3e6f5-379f-4251-a660-30b1676ff9de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - take a look (RUN THIS CELL AS IS)\n",
    "plotErrorCurves(trainLoss, testLoss, title = '5-fold Cross Validated Loss' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c9a0c0c-7bc5-4054-8e63-0d72dfbdd84a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 7: Regularization.\n",
    "\n",
    "Our goal, as always, is to build a linear model that will extend well to unseen data. Chosing the right combination of features to optimize generalizability can be extremely computationally costly given that there are \\\\(2^{p}\\\\) potential models that can be built from \\\\(p\\\\) features. Traditional methods like forward selection would involve iteratively testing these options to asses which combinations of features achieve a statistically significant prediction.\n",
    "\n",
    "Ridge Regression and Lasso Regression are two popular alternatives to OLS, which enable us to train generalizable models without the trouble of forward selection and/or manual feature selection.  Both methods take advantage of the bias-variance tradeoff by _shrinking_ the model coefficients towards 0 which reduces the variance of our model with little increase in bias. In practice this 'shrinkage' is achieved by adding a penalty (a.k.a. 'regularization') term to the means squared error loss function. In this question you will implement Gradient Descent with ridge and lasso regularization.\n",
    "\n",
    "__`IMPORTANT NOTE:`__ When performing regularization _do not_ include the bias in your regularization term calcultion (Recall, that throughout this assignment we've included the bias at index 0 in the vector of weights that is your model).\n",
    "\n",
    "### Q7 Tasks:\n",
    "* __a) short response:__ The regularization term for ridge regression is the square of the \\\\(L2\\\\) norm of the weights vector (i.e. the sum of squares of the coefficients) times the regularization parameter, \\\\(\\lambda\\\\). Write the formulas for both the loss function and the gradient for Ridge Regularization and explain what extra step this will add to our gradient descent algorithm.\n",
    "\n",
    "\n",
    "* __b) short response:__ The regularization term for lasso regression is the \\\\(L1\\\\) norm of the weights vector (i.e. the sum of the absolute values of the coefficients) times the regularization parameter, \\\\(\\lambda\\\\). Write the formulas for both the loss function and the gradient for Lasso Regularization and explain how the gradient descent update in Lasso will be different than it was in Ridge.\n",
    "\n",
    "\n",
    "* __c) code:__ Fill in the first two missing code blocks in `GDUpdate_wReg()` so that this function will perform a single parameter update using \\\\(L2\\\\) regularization if the parameter `regType` is set to `ridge`, \\\\(L1\\\\) regularization if set to `lasso` and unregularized OLS otherwise.\n",
    "\n",
    "\n",
    "* __d) code + short response:__ Use the provided code to train 50 iterations of ridge and lasso regression and plot the test/train error. Comment on the curves you see. Does this match your expectation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "02ffd797-a25b-438f-a12e-16900ad3100f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q7 Student Answers:\n",
    "> __a)__ Type your answer here!  \n",
    "\n",
    "> __b)__ Type your answer here!  \n",
    "\n",
    "> __d)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aa766d8c-00d8-4b42-879b-506462670492",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part c - gradient descent with regularization\n",
    "def GDUpdate_wReg(dataRDD, W, learningRate = 0.1, regType = None, regParam = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step/update with ridge or lasso regularization.\n",
    "    Args:\n",
    "        dataRDD - tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        learningRate - (float) defaults to 0.1\n",
    "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
    "        regParam - (float) regularization term coefficient\n",
    "    Returns:\n",
    "        model   - (array) updated coefficients, bias still at index 0\n",
    "    \"\"\"\n",
    "    # augmented data\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    \n",
    "    new_model = None\n",
    "    #################### YOUR CODE HERE ###################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ################## (END) YOUR CODE ####################\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e89b9309-8a4a-4c60-b87b-a3447428e204",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. -4.  0.  8.]\n",
      "[ 1 -2  0  4]\n",
      "[ 0 -1  0  1]\n",
      "[ 1 -2  0  4]\n",
      "[ 1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "W=np.array([1,-2,0,4])\n",
    "print(2*np.append([0.0], W[1:]))\n",
    "print(W)\n",
    "\n",
    "wreg=np.sign(W)\n",
    "wreg[0]=0\n",
    "print(wreg)\n",
    "print(W)\n",
    "\n",
    "wReg = W * 1\n",
    "wReg[-1] = 0\n",
    "wReg=(wReg>0).astype(int) * 2-1\n",
    "print(wReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a37b7b07-8b70-4eba-8c9e-61c78f7444cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - ridge/lasso gradient descent function\n",
    "def GradientDescent_wReg(trainRDD, testRDD, wInit, nSteps = 20, learningRate = 0.1,\n",
    "                         regType = None, regParam = 0.1, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of regularized gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = wInit\n",
    "    for idx in range(nSteps):  \n",
    "        # update the model\n",
    "        model = GDUpdate_wReg(trainRDD, model, learningRate, regType, regParam)\n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(OLSLoss(trainRDD, model))\n",
    "        test_history.append(OLSLoss(testRDD, model))\n",
    "        model_history.append(model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {training_loss}\")\n",
    "            print(f\"test loss: {test_loss}\")\n",
    "            print(f\"Model: {[round(w,3) for w in model]}\")\n",
    "    return train_history, test_history, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8e5748b0-545b-4f48-abde-2b2fe3821a53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run 50 iterations of ridge (RUN THIS CELL AS IS)\n",
    "wInit = BASELINE\n",
    "trainRDD, testRDD = normedRDD.randomSplit([0.8,0.2], seed = 5)\n",
    "start = time.time()\n",
    "ridge_results = GradientDescent_wReg(trainRDD, testRDD, wInit, nSteps = 50, \n",
    "                                     regType='ridge', regParam = 0.05 )\n",
    "print(f\"\\n... trained {len(ridge_results[2])} iterations in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "faa89abc-d75c-4093-9fcf-c89f14072710",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - save and display ridge results (RUN THIS CELL AS IS)\n",
    "trainLoss, testLoss, models = ridge_results\n",
    "np.savetxt(PWD + '/data/ridge_models.csv', np.array(models), delimiter=',')\n",
    "np.savetxt(PWD + '/data/ridge_loss.csv', np.array([trainLoss, testLoss]), delimiter=',')\n",
    "plotErrorCurves(trainLoss, testLoss, title = 'Ridge Regression Error Curves' )\n",
    "plotErrorCurves(trainLoss, testLoss, title = 'Ridge Regression Error Curves' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f8ec4c04-46b6-4ec8-87b6-208a65eb0234",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run 50 iterations of lasso (RUN THIS CELL AS IS)\n",
    "wInit = BASELINE\n",
    "trainRDD, testRDD = normedRDD.randomSplit([0.8,0.2], seed = 5)\n",
    "start = time.time()\n",
    "lasso_results = GradientDescent_wReg(trainRDD, testRDD, wInit, nSteps = 50,\n",
    "                                     regType='lasso', regParam = 0.05)\n",
    "print(f\"\\n... trained {len(lasso_results[2])} iterations in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "83ae8d15-4c71-4738-b7c5-6d8b51291b7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - save and display lasso results (RUN THIS CELL AS IS)\n",
    "trainLoss, testLoss, models = lasso_results\n",
    "np.savetxt(PWD + '/data/lasso_models.csv', np.array(models), delimiter=',')\n",
    "np.savetxt(PWD + '/data/lasso_loss.csv', np.array([trainLoss, testLoss]), delimiter=',')\n",
    "plotErrorCurves(trainLoss, testLoss, title = 'Lasso Regression Error Curves' )\n",
    "plotErrorCurves(trainLoss, testLoss, title = 'Lasso Regression Error Curves' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9d3678ea-b10d-411a-9b87-a122d12fb578",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Question 8: Results\n",
    "\n",
    "In this final question we'll use a few different plots to help us compare the OLS, Ridge and Lasso models that we have trained. Use the provided code to load the training history from file and retrieve the best (i.e. last) model from each method.\n",
    "\n",
    "### Q8 Tasks:\n",
    "* __a) code:__ Use the provided code to load the training history from file and retrieve the best (i.e. last) model from each method. Then create a new RDD called `validationRDD` by computing the mean squared error on the held out dataset for each of the three models. [__`TIP:`__ _the held out data is in it's raw form, don't forget to parse and normalize before applying your calculations, you should also be careful to normalize using the same scaling parameters that you used for the training data._]\n",
    "\n",
    "* __b) short response:__ Which model performed best? Discuss how you interpret these results and what you would want to try next.\n",
    "\n",
    "\n",
    "* __c) short response:__ Use the provided code to plot side by side boxplots of the residuals vs. the outcome (i.e. `quality`). What can you observe about our model performance? [__`TIP:`__ _note that the heldout data set is plenty small enough to fit in memory so no need to sample. Feel free to do your plotting in pandas or any other comfortable python package._]\n",
    "\n",
    "\n",
    "* __d) short response:__ Run the provided code to visualize the model coefficients for the first 50 iterations of training. What do you observe about how the OLS, ridge and lasso coefficients change over the course of the training process. Please be sure to discuss all three in your response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "398e36af-a2cc-4869-81e2-39b41acf25cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Q8 Student Answers:\n",
    "> __b)__ Type your answer here!  \n",
    "\n",
    "> __c)__ Type your answer here!  \n",
    "\n",
    "> __d)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a43791fb-9f8f-4d27-a86f-e1ad451178c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part a - load the models from file (RUN THIS CELL AS IS)\n",
    "ols_models = np.loadtxt(PWD + '/data/OLSmodels.csv', dtype=float, delimiter=',')\n",
    "ridge_models = np.loadtxt(PWD + '/data/ridge_models.csv', dtype=float, delimiter=',')\n",
    "lasso_models = np.loadtxt(PWD + '/data/lasso_models.csv', dtype=float, delimiter=',')\n",
    "best_ols = ols_models[-1,:]\n",
    "best_ridge = ridge_models[-1,:]\n",
    "best_lasso = lasso_models[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2a3a81d4-a3ff-4ec5-8c1b-525a2f9cc147",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Mean Squared Error: None\n",
      "Ridge Mean Squared Error: None\n",
      "Lasso Mean Squared Error: None\n"
     ]
    }
   ],
   "source": [
    "# part a - compute MSE on the held out data for all three 'best' models\n",
    "olsMSE, ridgeMSE, lassoMSE = None, None, None\n",
    "validationRDD = None\n",
    "############### YOUR CODE HERE #################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############### YOUR CODE HERE #################\n",
    "\n",
    "print(f\"OLS Mean Squared Error: {olsMSE}\")\n",
    "print(f\"Ridge Mean Squared Error: {ridgeMSE}\")\n",
    "print(f\"Lasso Mean Squared Error: {lassoMSE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f332b280-3fe7-4352-8a08-36ca69f078f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part c - helper function (RUN THIS CELL AS IS)\n",
    "def get_residuals(dataRDD, model):\n",
    "    \"\"\"\n",
    "    Return a collected list of tuples (residual, quality_score)\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    residuals = augmentedData.map(lambda x: (x[1] - model.dot(x[0]), x[1]))\n",
    "    return residuals.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "89363889-24de-45ce-b471-db773935ef9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part c - compute residuals for all three models (RUN THIS CELL AS IS)\n",
    "ols_resid = np.array(get_residuals(validationRDD, best_ols))\n",
    "ridge_resid = np.array(get_residuals(validationRDD, best_ridge))\n",
    "lasso_resid = np.array(get_residuals(validationRDD, best_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "09eb6bdc-e7a9-48ec-9893-58e7072e144d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # part c - boxplots of residuals for all three models (RUN THIS CELL AS IS)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "stuff_to_plot = zip(axes, [\"OLS\", \"Ridge\", \"Lasso\"], [ols_resid, ridge_resid, lasso_resid])\n",
    "for ax, title, data in stuff_to_plot:\n",
    "    ax.set_title(title)\n",
    "    y = data[:, 0]\n",
    "    x = data[:, 1]\n",
    "    sns.boxplot(x, y, ax=ax)\n",
    "fig.suptitle(\"Prediction Error vs. Quality Score\", fontsize=15, y=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2dd62903-3a50-4593-b7f4-7a9f598f9f88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part d - plotting function (RUN THIS CELL AS IS)\n",
    "def plotCoeffs(models, featureNames, title):\n",
    "    \"\"\"\n",
    "    Helper Function to show how coefficients change as we train.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize = (15,8))\n",
    "    X = list(range(len(models)))\n",
    "    for data, name in zip(models.T, featureNames):\n",
    "        if name == \"Bias\":\n",
    "            continue\n",
    "        ax.plot(X, data, label=name)\n",
    "    ax.plot(X,[0]*len(X), 'k--')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "96fb842d-4f12-4ade-8a7b-7e34ef9a0889",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# take a look (RUN THIS CELL AS IS)\n",
    "plotCoeffs(ols_models, ['Bias'] + FIELDS, \"OLS Coefficients over 50 GD steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4b31d9b6-fee9-447f-b047-985dbc812b60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# take a look (RUN THIS CELL AS IS)\n",
    "plotCoeffs(ridge_models, ['Bias'] + FIELDS, \"Ridge Coefficients over 50 GD steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "10c132d3-5c13-4218-b3b5-2d7b0d3d7b6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# take a look (RUN THIS CELL AS IS)\n",
    "plotCoeffs(lasso_models, ['Bias'] + FIELDS, \"Lasso Coefficients over 50 GD steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a8abfadd-3842-4961-abe0-38d1a3af3b41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Congratulations, you have completed HW4! Please refer to the readme for submission instructions.\n",
    "\n",
    "If you would like to provide feedback regarding this homework, please use the survey at: https://docs.google.com/forms/d/e/1FAIpQLSce9feiQeSkdP43A0ZYui1tMGIBfLfzb0rmgToQeZD9bXXX8Q/viewform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d08dae5d-a659-4518-89cf-6336dfe309f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "hw4_Workbook",
   "notebookOrigID": 3159780226953918,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
